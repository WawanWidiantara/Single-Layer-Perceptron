{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data Training Hasil Encode** (Belum dinormalisasi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Embarked\n",
       "0            1         0       3    1  22.0      1      0   7.2500         2\n",
       "1            2         1       1    0  38.0      1      0  71.2833         0\n",
       "2            3         1       3    0  26.0      0      0   7.9250         2\n",
       "3            4         1       1    0  35.0      1      0  53.1000         2\n",
       "4            5         0       3    1  35.0      0      0   8.0500         2"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_excel('D:/Code/py_code/Artificial-Neural-Network/Single-Layer-Perceptron/Data Training.xlsx', sheet_name='Hasil Encode', index_col=0)\n",
    "x_train = train.iloc[:, 2:].values\n",
    "y_train = train.iloc[:, 1].values\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data Training Hasil Normalisasi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.271174</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.472229</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.139136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.321438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.434531</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103644</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.434531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass  Sex       Age  SibSp  Parch      Fare  Embarked   \n",
       "0            1     1.0    1  0.271174    0.2    0.0  0.014151       1.0  \\\n",
       "1            2     0.0    0  0.472229    0.2    0.0  0.139136       0.0   \n",
       "2            3     1.0    0  0.321438    0.0    0.0  0.015469       1.0   \n",
       "3            4     0.0    0  0.434531    0.2    0.0  0.103644       1.0   \n",
       "4            5     1.0    1  0.434531    0.0    0.0  0.015713       1.0   \n",
       "\n",
       "   Survived  \n",
       "0         0  \n",
       "1         1  \n",
       "2         1  \n",
       "3         1  \n",
       "4         0  "
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_normalize = pd.read_excel('D:/Code/py_code/Artificial-Neural-Network/Single-Layer-Perceptron/Data Training.xlsx', sheet_name='Normalisasi', index_col=0)\n",
    "x_train_normalize = train_normalize.iloc[:, 1:-1].values\n",
    "y_train_normalize = train_normalize.iloc[:, -1].values\n",
    "train_normalize.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data Testing Hasil Encode** (Belum dinormalisasi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass  Sex   Age  SibSp  Parch     Fare  Embarked\n",
       "0          892       3    1  34.5      0      0   7.8292         1\n",
       "1          893       3    0  47.0      1      0   7.0000         2\n",
       "2          894       2    1  62.0      0      0   9.6875         1\n",
       "3          895       3    1  27.0      0      0   8.6625         2\n",
       "4          896       3    0  22.0      1      1  12.2875         2"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_excel('D:/Code/py_code/Artificial-Neural-Network/Single-Layer-Perceptron/Data Testing.xlsx', sheet_name='Hasil Encode', index_col=0)\n",
    "\n",
    "label = pd.read_csv(\"D:/Code/py_code/Artificial-Neural-Network/Single-Layer-Perceptron/data/test_data_GroundTruth_cl.csv\")\n",
    "\n",
    "x_test = test.iloc[:, 1:].values\n",
    "y_test = label['Survived'].values\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data Training Hasil Normalisasi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.452723</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015282</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.617566</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013663</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.815377</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018909</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.353818</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016908</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.287881</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.023984</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass  Sex       Age  SibSp     Parch      Fare  Embarked   \n",
       "0          892     1.0    1  0.452723  0.000  0.000000  0.015282       0.5  \\\n",
       "1          893     1.0    0  0.617566  0.125  0.000000  0.013663       1.0   \n",
       "2          894     0.5    1  0.815377  0.000  0.000000  0.018909       0.5   \n",
       "3          895     1.0    1  0.353818  0.000  0.000000  0.016908       1.0   \n",
       "4          896     1.0    0  0.287881  0.125  0.166667  0.023984       1.0   \n",
       "\n",
       "   Survived  \n",
       "0         0  \n",
       "1         1  \n",
       "2         0  \n",
       "3         0  \n",
       "4         1  "
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_normalize = pd.read_excel('D:/Code/py_code/Artificial-Neural-Network/Single-Layer-Perceptron/Data Testing.xlsx', sheet_name='Normalisasi', index_col=0)\n",
    "x_test_normalize = test_normalize.iloc[:, 1:-1].values\n",
    "y_test_normalize = test_normalize.iloc[:, -1].values\n",
    "test_normalize.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisasi Plot Confussion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(matrix, title): \n",
    "    ax = plt.subplot()\n",
    "    sns.heatmap(matrix, annot=True, fmt='g', ax=ax, cmap=sns.cubehelix_palette(as_cmap=True))\n",
    "\n",
    "    ax.set_title(f'Confusion Matrix {title}', pad=10);\n",
    "    ax.xaxis.set_ticklabels(['Not Survived', 'Survived']);ax.yaxis.set_ticklabels(['Not Survived', 'Survived'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pembuatan Model Single Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_31 (Dense)            (None, 1)                 8         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8 (32.00 Byte)\n",
      "Trainable params: 8 (32.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.01)\n",
    "keras.utils.set_random_seed(42)\n",
    "model = keras.Sequential([keras.layers.Dense(units=1, input_shape=[len(x_train[0])], activation='sigmoid', kernel_initializer='random_uniform')])\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.03581891],\n",
       "        [ 0.01626286],\n",
       "        [-0.01412892],\n",
       "        [ 0.04499895],\n",
       "        [-0.04219389],\n",
       "        [ 0.01523323],\n",
       "        [ 0.04704999]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.get_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 932us/step - loss: 0.2544 - accuracy: 0.3778\n",
      "Epoch 2/1000\n",
      "23/23 [==============================] - 0s 867us/step - loss: 0.2496 - accuracy: 0.5225\n",
      "Epoch 3/1000\n",
      "23/23 [==============================] - 0s 819us/step - loss: 0.2457 - accuracy: 0.5941\n",
      "Epoch 4/1000\n",
      "23/23 [==============================] - 0s 806us/step - loss: 0.2423 - accuracy: 0.5955\n",
      "Epoch 5/1000\n",
      "23/23 [==============================] - 0s 961us/step - loss: 0.2390 - accuracy: 0.5955\n",
      "Epoch 6/1000\n",
      "23/23 [==============================] - 0s 870us/step - loss: 0.2363 - accuracy: 0.5955\n",
      "Epoch 7/1000\n",
      "23/23 [==============================] - 0s 842us/step - loss: 0.2339 - accuracy: 0.5955\n",
      "Epoch 8/1000\n",
      "23/23 [==============================] - 0s 921us/step - loss: 0.2319 - accuracy: 0.5955\n",
      "Epoch 9/1000\n",
      "23/23 [==============================] - 0s 887us/step - loss: 0.2299 - accuracy: 0.5955\n",
      "Epoch 10/1000\n",
      "23/23 [==============================] - 0s 823us/step - loss: 0.2283 - accuracy: 0.5955\n",
      "Epoch 11/1000\n",
      "23/23 [==============================] - 0s 891us/step - loss: 0.2268 - accuracy: 0.5955\n",
      "Epoch 12/1000\n",
      "23/23 [==============================] - 0s 826us/step - loss: 0.2255 - accuracy: 0.5955\n",
      "Epoch 13/1000\n",
      "23/23 [==============================] - 0s 932us/step - loss: 0.2243 - accuracy: 0.5955\n",
      "Epoch 14/1000\n",
      "23/23 [==============================] - 0s 790us/step - loss: 0.2232 - accuracy: 0.5955\n",
      "Epoch 15/1000\n",
      "23/23 [==============================] - 0s 843us/step - loss: 0.2221 - accuracy: 0.5955\n",
      "Epoch 16/1000\n",
      "23/23 [==============================] - 0s 853us/step - loss: 0.2212 - accuracy: 0.5955\n",
      "Epoch 17/1000\n",
      "23/23 [==============================] - 0s 850us/step - loss: 0.2203 - accuracy: 0.5955\n",
      "Epoch 18/1000\n",
      "23/23 [==============================] - 0s 854us/step - loss: 0.2194 - accuracy: 0.5955\n",
      "Epoch 19/1000\n",
      "23/23 [==============================] - 0s 882us/step - loss: 0.2187 - accuracy: 0.5955\n",
      "Epoch 20/1000\n",
      "23/23 [==============================] - 0s 857us/step - loss: 0.2179 - accuracy: 0.5955\n",
      "Epoch 21/1000\n",
      "23/23 [==============================] - 0s 829us/step - loss: 0.2173 - accuracy: 0.5955\n",
      "Epoch 22/1000\n",
      "23/23 [==============================] - 0s 822us/step - loss: 0.2166 - accuracy: 0.5955\n",
      "Epoch 23/1000\n",
      "23/23 [==============================] - 0s 816us/step - loss: 0.2160 - accuracy: 0.5955\n",
      "Epoch 24/1000\n",
      "23/23 [==============================] - 0s 817us/step - loss: 0.2154 - accuracy: 0.5955\n",
      "Epoch 25/1000\n",
      "23/23 [==============================] - 0s 918us/step - loss: 0.2148 - accuracy: 0.5955\n",
      "Epoch 26/1000\n",
      "23/23 [==============================] - 0s 833us/step - loss: 0.2142 - accuracy: 0.5955\n",
      "Epoch 27/1000\n",
      "23/23 [==============================] - 0s 878us/step - loss: 0.2137 - accuracy: 0.5955\n",
      "Epoch 28/1000\n",
      "23/23 [==============================] - 0s 837us/step - loss: 0.2131 - accuracy: 0.5955\n",
      "Epoch 29/1000\n",
      "23/23 [==============================] - 0s 829us/step - loss: 0.2126 - accuracy: 0.5955\n",
      "Epoch 30/1000\n",
      "23/23 [==============================] - 0s 805us/step - loss: 0.2121 - accuracy: 0.5955\n",
      "Epoch 31/1000\n",
      "23/23 [==============================] - 0s 870us/step - loss: 0.2116 - accuracy: 0.5955\n",
      "Epoch 32/1000\n",
      "23/23 [==============================] - 0s 807us/step - loss: 0.2111 - accuracy: 0.5969\n",
      "Epoch 33/1000\n",
      "23/23 [==============================] - 0s 832us/step - loss: 0.2106 - accuracy: 0.5969\n",
      "Epoch 34/1000\n",
      "23/23 [==============================] - 0s 798us/step - loss: 0.2102 - accuracy: 0.5997\n",
      "Epoch 35/1000\n",
      "23/23 [==============================] - 0s 792us/step - loss: 0.2097 - accuracy: 0.6011\n",
      "Epoch 36/1000\n",
      "23/23 [==============================] - 0s 821us/step - loss: 0.2093 - accuracy: 0.6011\n",
      "Epoch 37/1000\n",
      "23/23 [==============================] - 0s 808us/step - loss: 0.2088 - accuracy: 0.6025\n",
      "Epoch 38/1000\n",
      "23/23 [==============================] - 0s 822us/step - loss: 0.2084 - accuracy: 0.6067\n",
      "Epoch 39/1000\n",
      "23/23 [==============================] - 0s 842us/step - loss: 0.2079 - accuracy: 0.6110\n",
      "Epoch 40/1000\n",
      "23/23 [==============================] - 0s 831us/step - loss: 0.2075 - accuracy: 0.6208\n",
      "Epoch 41/1000\n",
      "23/23 [==============================] - 0s 950us/step - loss: 0.2071 - accuracy: 0.6320\n",
      "Epoch 42/1000\n",
      "23/23 [==============================] - 0s 855us/step - loss: 0.2066 - accuracy: 0.6419\n",
      "Epoch 43/1000\n",
      "23/23 [==============================] - 0s 820us/step - loss: 0.2062 - accuracy: 0.6447\n",
      "Epoch 44/1000\n",
      "23/23 [==============================] - 0s 788us/step - loss: 0.2058 - accuracy: 0.6461\n",
      "Epoch 45/1000\n",
      "23/23 [==============================] - 0s 840us/step - loss: 0.2054 - accuracy: 0.6461\n",
      "Epoch 46/1000\n",
      "23/23 [==============================] - 0s 852us/step - loss: 0.2050 - accuracy: 0.6503\n",
      "Epoch 47/1000\n",
      "23/23 [==============================] - 0s 807us/step - loss: 0.2045 - accuracy: 0.6503\n",
      "Epoch 48/1000\n",
      "23/23 [==============================] - 0s 837us/step - loss: 0.2041 - accuracy: 0.6503\n",
      "Epoch 49/1000\n",
      "23/23 [==============================] - 0s 761us/step - loss: 0.2037 - accuracy: 0.6517\n",
      "Epoch 50/1000\n",
      "23/23 [==============================] - 0s 815us/step - loss: 0.2033 - accuracy: 0.6489\n",
      "Epoch 51/1000\n",
      "23/23 [==============================] - 0s 814us/step - loss: 0.2029 - accuracy: 0.6531\n",
      "Epoch 52/1000\n",
      "23/23 [==============================] - 0s 874us/step - loss: 0.2025 - accuracy: 0.6573\n",
      "Epoch 53/1000\n",
      "23/23 [==============================] - 0s 847us/step - loss: 0.2021 - accuracy: 0.6784\n",
      "Epoch 54/1000\n",
      "23/23 [==============================] - 0s 761us/step - loss: 0.2018 - accuracy: 0.6952\n",
      "Epoch 55/1000\n",
      "23/23 [==============================] - 0s 782us/step - loss: 0.2014 - accuracy: 0.6994\n",
      "Epoch 56/1000\n",
      "23/23 [==============================] - 0s 821us/step - loss: 0.2010 - accuracy: 0.7022\n",
      "Epoch 57/1000\n",
      "23/23 [==============================] - 0s 839us/step - loss: 0.2006 - accuracy: 0.7037\n",
      "Epoch 58/1000\n",
      "23/23 [==============================] - 0s 831us/step - loss: 0.2002 - accuracy: 0.7037\n",
      "Epoch 59/1000\n",
      "23/23 [==============================] - 0s 840us/step - loss: 0.1999 - accuracy: 0.7037\n",
      "Epoch 60/1000\n",
      "23/23 [==============================] - 0s 867us/step - loss: 0.1995 - accuracy: 0.7037\n",
      "Epoch 61/1000\n",
      "23/23 [==============================] - 0s 865us/step - loss: 0.1992 - accuracy: 0.7037\n",
      "Epoch 62/1000\n",
      "23/23 [==============================] - 0s 793us/step - loss: 0.1988 - accuracy: 0.7037\n",
      "Epoch 63/1000\n",
      "23/23 [==============================] - 0s 843us/step - loss: 0.1984 - accuracy: 0.7037\n",
      "Epoch 64/1000\n",
      "23/23 [==============================] - 0s 921us/step - loss: 0.1981 - accuracy: 0.7037\n",
      "Epoch 65/1000\n",
      "23/23 [==============================] - 0s 823us/step - loss: 0.1978 - accuracy: 0.7037\n",
      "Epoch 66/1000\n",
      "23/23 [==============================] - 0s 890us/step - loss: 0.1974 - accuracy: 0.7037\n",
      "Epoch 67/1000\n",
      "23/23 [==============================] - 0s 846us/step - loss: 0.1971 - accuracy: 0.7037\n",
      "Epoch 68/1000\n",
      "23/23 [==============================] - 0s 811us/step - loss: 0.1967 - accuracy: 0.7037\n",
      "Epoch 69/1000\n",
      "23/23 [==============================] - 0s 845us/step - loss: 0.1964 - accuracy: 0.7037\n",
      "Epoch 70/1000\n",
      "23/23 [==============================] - 0s 811us/step - loss: 0.1961 - accuracy: 0.7037\n",
      "Epoch 71/1000\n",
      "23/23 [==============================] - 0s 793us/step - loss: 0.1957 - accuracy: 0.7051\n",
      "Epoch 72/1000\n",
      "23/23 [==============================] - 0s 820us/step - loss: 0.1954 - accuracy: 0.7051\n",
      "Epoch 73/1000\n",
      "23/23 [==============================] - 0s 845us/step - loss: 0.1951 - accuracy: 0.7065\n",
      "Epoch 74/1000\n",
      "23/23 [==============================] - 0s 803us/step - loss: 0.1947 - accuracy: 0.7107\n",
      "Epoch 75/1000\n",
      "23/23 [==============================] - 0s 812us/step - loss: 0.1944 - accuracy: 0.7107\n",
      "Epoch 76/1000\n",
      "23/23 [==============================] - 0s 835us/step - loss: 0.1941 - accuracy: 0.7107\n",
      "Epoch 77/1000\n",
      "23/23 [==============================] - 0s 827us/step - loss: 0.1938 - accuracy: 0.7135\n",
      "Epoch 78/1000\n",
      "23/23 [==============================] - 0s 828us/step - loss: 0.1935 - accuracy: 0.7135\n",
      "Epoch 79/1000\n",
      "23/23 [==============================] - 0s 811us/step - loss: 0.1932 - accuracy: 0.7135\n",
      "Epoch 80/1000\n",
      "23/23 [==============================] - 0s 865us/step - loss: 0.1929 - accuracy: 0.7135\n",
      "Epoch 81/1000\n",
      "23/23 [==============================] - 0s 826us/step - loss: 0.1926 - accuracy: 0.7177\n",
      "Epoch 82/1000\n",
      "23/23 [==============================] - 0s 880us/step - loss: 0.1923 - accuracy: 0.7219\n",
      "Epoch 83/1000\n",
      "23/23 [==============================] - 0s 869us/step - loss: 0.1920 - accuracy: 0.7388\n",
      "Epoch 84/1000\n",
      "23/23 [==============================] - 0s 814us/step - loss: 0.1917 - accuracy: 0.7416\n",
      "Epoch 85/1000\n",
      "23/23 [==============================] - 0s 945us/step - loss: 0.1914 - accuracy: 0.7570\n",
      "Epoch 86/1000\n",
      "23/23 [==============================] - 0s 889us/step - loss: 0.1911 - accuracy: 0.7584\n",
      "Epoch 87/1000\n",
      "23/23 [==============================] - 0s 759us/step - loss: 0.1909 - accuracy: 0.7697\n",
      "Epoch 88/1000\n",
      "23/23 [==============================] - 0s 810us/step - loss: 0.1906 - accuracy: 0.7907\n",
      "Epoch 89/1000\n",
      "23/23 [==============================] - 0s 889us/step - loss: 0.1903 - accuracy: 0.7907\n",
      "Epoch 90/1000\n",
      "23/23 [==============================] - 0s 849us/step - loss: 0.1900 - accuracy: 0.7907\n",
      "Epoch 91/1000\n",
      "23/23 [==============================] - 0s 893us/step - loss: 0.1897 - accuracy: 0.7907\n",
      "Epoch 92/1000\n",
      "23/23 [==============================] - 0s 821us/step - loss: 0.1895 - accuracy: 0.7907\n",
      "Epoch 93/1000\n",
      "23/23 [==============================] - 0s 854us/step - loss: 0.1892 - accuracy: 0.7907\n",
      "Epoch 94/1000\n",
      "23/23 [==============================] - 0s 854us/step - loss: 0.1889 - accuracy: 0.7907\n",
      "Epoch 95/1000\n",
      "23/23 [==============================] - 0s 854us/step - loss: 0.1887 - accuracy: 0.7907\n",
      "Epoch 96/1000\n",
      "23/23 [==============================] - 0s 857us/step - loss: 0.1884 - accuracy: 0.7907\n",
      "Epoch 97/1000\n",
      "23/23 [==============================] - 0s 883us/step - loss: 0.1882 - accuracy: 0.7907\n",
      "Epoch 98/1000\n",
      "23/23 [==============================] - 0s 878us/step - loss: 0.1879 - accuracy: 0.7907\n",
      "Epoch 99/1000\n",
      "23/23 [==============================] - 0s 833us/step - loss: 0.1876 - accuracy: 0.7907\n",
      "Epoch 100/1000\n",
      "23/23 [==============================] - 0s 841us/step - loss: 0.1874 - accuracy: 0.7907\n",
      "Epoch 101/1000\n",
      "23/23 [==============================] - 0s 858us/step - loss: 0.1871 - accuracy: 0.7907\n",
      "Epoch 102/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1869 - accuracy: 0.7907\n",
      "Epoch 103/1000\n",
      "23/23 [==============================] - 0s 787us/step - loss: 0.1866 - accuracy: 0.7907\n",
      "Epoch 104/1000\n",
      "23/23 [==============================] - 0s 824us/step - loss: 0.1864 - accuracy: 0.7907\n",
      "Epoch 105/1000\n",
      "23/23 [==============================] - 0s 827us/step - loss: 0.1862 - accuracy: 0.7907\n",
      "Epoch 106/1000\n",
      "23/23 [==============================] - 0s 835us/step - loss: 0.1859 - accuracy: 0.7907\n",
      "Epoch 107/1000\n",
      "23/23 [==============================] - 0s 768us/step - loss: 0.1857 - accuracy: 0.7907\n",
      "Epoch 108/1000\n",
      "23/23 [==============================] - 0s 825us/step - loss: 0.1855 - accuracy: 0.7907\n",
      "Epoch 109/1000\n",
      "23/23 [==============================] - 0s 852us/step - loss: 0.1852 - accuracy: 0.7907\n",
      "Epoch 110/1000\n",
      "23/23 [==============================] - 0s 868us/step - loss: 0.1850 - accuracy: 0.7907\n",
      "Epoch 111/1000\n",
      "23/23 [==============================] - 0s 788us/step - loss: 0.1848 - accuracy: 0.7907\n",
      "Epoch 112/1000\n",
      "23/23 [==============================] - 0s 854us/step - loss: 0.1846 - accuracy: 0.7907\n",
      "Epoch 113/1000\n",
      "23/23 [==============================] - 0s 845us/step - loss: 0.1843 - accuracy: 0.7907\n",
      "Epoch 114/1000\n",
      "23/23 [==============================] - 0s 775us/step - loss: 0.1841 - accuracy: 0.7907\n",
      "Epoch 115/1000\n",
      "23/23 [==============================] - 0s 800us/step - loss: 0.1839 - accuracy: 0.7907\n",
      "Epoch 116/1000\n",
      "23/23 [==============================] - 0s 819us/step - loss: 0.1837 - accuracy: 0.7907\n",
      "Epoch 117/1000\n",
      "23/23 [==============================] - 0s 800us/step - loss: 0.1835 - accuracy: 0.7907\n",
      "Epoch 118/1000\n",
      "23/23 [==============================] - 0s 792us/step - loss: 0.1833 - accuracy: 0.7907\n",
      "Epoch 119/1000\n",
      "23/23 [==============================] - 0s 811us/step - loss: 0.1830 - accuracy: 0.7907\n",
      "Epoch 120/1000\n",
      "23/23 [==============================] - 0s 804us/step - loss: 0.1828 - accuracy: 0.7907\n",
      "Epoch 121/1000\n",
      "23/23 [==============================] - 0s 848us/step - loss: 0.1826 - accuracy: 0.7907\n",
      "Epoch 122/1000\n",
      "23/23 [==============================] - 0s 813us/step - loss: 0.1824 - accuracy: 0.7907\n",
      "Epoch 123/1000\n",
      "23/23 [==============================] - 0s 762us/step - loss: 0.1822 - accuracy: 0.7907\n",
      "Epoch 124/1000\n",
      "23/23 [==============================] - 0s 996us/step - loss: 0.1820 - accuracy: 0.7907\n",
      "Epoch 125/1000\n",
      "23/23 [==============================] - 0s 828us/step - loss: 0.1818 - accuracy: 0.7907\n",
      "Epoch 126/1000\n",
      "23/23 [==============================] - 0s 843us/step - loss: 0.1816 - accuracy: 0.7907\n",
      "Epoch 127/1000\n",
      "23/23 [==============================] - 0s 883us/step - loss: 0.1814 - accuracy: 0.7907\n",
      "Epoch 128/1000\n",
      "23/23 [==============================] - 0s 877us/step - loss: 0.1812 - accuracy: 0.7907\n",
      "Epoch 129/1000\n",
      "23/23 [==============================] - 0s 788us/step - loss: 0.1810 - accuracy: 0.7907\n",
      "Epoch 130/1000\n",
      "23/23 [==============================] - 0s 795us/step - loss: 0.1808 - accuracy: 0.7907\n",
      "Epoch 131/1000\n",
      "23/23 [==============================] - 0s 815us/step - loss: 0.1806 - accuracy: 0.7907\n",
      "Epoch 132/1000\n",
      "23/23 [==============================] - 0s 838us/step - loss: 0.1804 - accuracy: 0.7907\n",
      "Epoch 133/1000\n",
      "23/23 [==============================] - 0s 843us/step - loss: 0.1802 - accuracy: 0.7907\n",
      "Epoch 134/1000\n",
      "23/23 [==============================] - 0s 815us/step - loss: 0.1801 - accuracy: 0.7907\n",
      "Epoch 135/1000\n",
      "23/23 [==============================] - 0s 810us/step - loss: 0.1799 - accuracy: 0.7907\n",
      "Epoch 136/1000\n",
      "23/23 [==============================] - 0s 870us/step - loss: 0.1797 - accuracy: 0.7893\n",
      "Epoch 137/1000\n",
      "23/23 [==============================] - 0s 814us/step - loss: 0.1795 - accuracy: 0.7893\n",
      "Epoch 138/1000\n",
      "23/23 [==============================] - 0s 929us/step - loss: 0.1794 - accuracy: 0.7879\n",
      "Epoch 139/1000\n",
      "23/23 [==============================] - 0s 920us/step - loss: 0.1792 - accuracy: 0.7865\n",
      "Epoch 140/1000\n",
      "23/23 [==============================] - 0s 857us/step - loss: 0.1790 - accuracy: 0.7879\n",
      "Epoch 141/1000\n",
      "23/23 [==============================] - 0s 878us/step - loss: 0.1788 - accuracy: 0.7865\n",
      "Epoch 142/1000\n",
      "23/23 [==============================] - 0s 820us/step - loss: 0.1786 - accuracy: 0.7851\n",
      "Epoch 143/1000\n",
      "23/23 [==============================] - 0s 779us/step - loss: 0.1784 - accuracy: 0.7837\n",
      "Epoch 144/1000\n",
      "23/23 [==============================] - 0s 853us/step - loss: 0.1783 - accuracy: 0.7809\n",
      "Epoch 145/1000\n",
      "23/23 [==============================] - 0s 828us/step - loss: 0.1781 - accuracy: 0.7795\n",
      "Epoch 146/1000\n",
      "23/23 [==============================] - 0s 784us/step - loss: 0.1780 - accuracy: 0.7781\n",
      "Epoch 147/1000\n",
      "23/23 [==============================] - 0s 883us/step - loss: 0.1778 - accuracy: 0.7781\n",
      "Epoch 148/1000\n",
      "23/23 [==============================] - 0s 868us/step - loss: 0.1776 - accuracy: 0.7781\n",
      "Epoch 149/1000\n",
      "23/23 [==============================] - 0s 807us/step - loss: 0.1775 - accuracy: 0.7781\n",
      "Epoch 150/1000\n",
      "23/23 [==============================] - 0s 838us/step - loss: 0.1773 - accuracy: 0.7795\n",
      "Epoch 151/1000\n",
      "23/23 [==============================] - 0s 823us/step - loss: 0.1771 - accuracy: 0.7767\n",
      "Epoch 152/1000\n",
      "23/23 [==============================] - 0s 790us/step - loss: 0.1770 - accuracy: 0.7739\n",
      "Epoch 153/1000\n",
      "23/23 [==============================] - 0s 798us/step - loss: 0.1768 - accuracy: 0.7767\n",
      "Epoch 154/1000\n",
      "23/23 [==============================] - 0s 825us/step - loss: 0.1767 - accuracy: 0.7725\n",
      "Epoch 155/1000\n",
      "23/23 [==============================] - 0s 869us/step - loss: 0.1765 - accuracy: 0.7725\n",
      "Epoch 156/1000\n",
      "23/23 [==============================] - 0s 917us/step - loss: 0.1764 - accuracy: 0.7711\n",
      "Epoch 157/1000\n",
      "23/23 [==============================] - 0s 862us/step - loss: 0.1762 - accuracy: 0.7725\n",
      "Epoch 158/1000\n",
      "23/23 [==============================] - 0s 847us/step - loss: 0.1761 - accuracy: 0.7711\n",
      "Epoch 159/1000\n",
      "23/23 [==============================] - 0s 765us/step - loss: 0.1759 - accuracy: 0.7697\n",
      "Epoch 160/1000\n",
      "23/23 [==============================] - 0s 845us/step - loss: 0.1757 - accuracy: 0.7725\n",
      "Epoch 161/1000\n",
      "23/23 [==============================] - 0s 917us/step - loss: 0.1756 - accuracy: 0.7697\n",
      "Epoch 162/1000\n",
      "23/23 [==============================] - 0s 906us/step - loss: 0.1754 - accuracy: 0.7725\n",
      "Epoch 163/1000\n",
      "23/23 [==============================] - 0s 898us/step - loss: 0.1753 - accuracy: 0.7683\n",
      "Epoch 164/1000\n",
      "23/23 [==============================] - 0s 831us/step - loss: 0.1752 - accuracy: 0.7697\n",
      "Epoch 165/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1750 - accuracy: 0.7697\n",
      "Epoch 166/1000\n",
      "23/23 [==============================] - 0s 880us/step - loss: 0.1749 - accuracy: 0.7697\n",
      "Epoch 167/1000\n",
      "23/23 [==============================] - 0s 870us/step - loss: 0.1748 - accuracy: 0.7683\n",
      "Epoch 168/1000\n",
      "23/23 [==============================] - 0s 853us/step - loss: 0.1746 - accuracy: 0.7683\n",
      "Epoch 169/1000\n",
      "23/23 [==============================] - 0s 830us/step - loss: 0.1745 - accuracy: 0.7711\n",
      "Epoch 170/1000\n",
      "23/23 [==============================] - 0s 860us/step - loss: 0.1743 - accuracy: 0.7739\n",
      "Epoch 171/1000\n",
      "23/23 [==============================] - 0s 850us/step - loss: 0.1742 - accuracy: 0.7725\n",
      "Epoch 172/1000\n",
      "23/23 [==============================] - 0s 843us/step - loss: 0.1741 - accuracy: 0.7739\n",
      "Epoch 173/1000\n",
      "23/23 [==============================] - 0s 750us/step - loss: 0.1740 - accuracy: 0.7725\n",
      "Epoch 174/1000\n",
      "23/23 [==============================] - 0s 813us/step - loss: 0.1738 - accuracy: 0.7711\n",
      "Epoch 175/1000\n",
      "23/23 [==============================] - 0s 835us/step - loss: 0.1737 - accuracy: 0.7711\n",
      "Epoch 176/1000\n",
      "23/23 [==============================] - 0s 863us/step - loss: 0.1735 - accuracy: 0.7697\n",
      "Epoch 177/1000\n",
      "23/23 [==============================] - 0s 880us/step - loss: 0.1734 - accuracy: 0.7739\n",
      "Epoch 178/1000\n",
      "23/23 [==============================] - 0s 831us/step - loss: 0.1733 - accuracy: 0.7725\n",
      "Epoch 179/1000\n",
      "23/23 [==============================] - 0s 857us/step - loss: 0.1732 - accuracy: 0.7669\n",
      "Epoch 180/1000\n",
      "23/23 [==============================] - 0s 887us/step - loss: 0.1730 - accuracy: 0.7711\n",
      "Epoch 181/1000\n",
      "23/23 [==============================] - 0s 855us/step - loss: 0.1729 - accuracy: 0.7683\n",
      "Epoch 182/1000\n",
      "23/23 [==============================] - 0s 861us/step - loss: 0.1728 - accuracy: 0.7725\n",
      "Epoch 183/1000\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.1727 - accuracy: 0.7739\n",
      "Epoch 184/1000\n",
      "23/23 [==============================] - 0s 851us/step - loss: 0.1725 - accuracy: 0.7725\n",
      "Epoch 185/1000\n",
      "23/23 [==============================] - 0s 838us/step - loss: 0.1724 - accuracy: 0.7711\n",
      "Epoch 186/1000\n",
      "23/23 [==============================] - 0s 836us/step - loss: 0.1723 - accuracy: 0.7753\n",
      "Epoch 187/1000\n",
      "23/23 [==============================] - 0s 809us/step - loss: 0.1722 - accuracy: 0.7753\n",
      "Epoch 188/1000\n",
      "23/23 [==============================] - 0s 832us/step - loss: 0.1720 - accuracy: 0.7767\n",
      "Epoch 189/1000\n",
      "23/23 [==============================] - 0s 875us/step - loss: 0.1719 - accuracy: 0.7767\n",
      "Epoch 190/1000\n",
      "23/23 [==============================] - 0s 821us/step - loss: 0.1718 - accuracy: 0.7767\n",
      "Epoch 191/1000\n",
      "23/23 [==============================] - 0s 889us/step - loss: 0.1717 - accuracy: 0.7781\n",
      "Epoch 192/1000\n",
      "23/23 [==============================] - 0s 852us/step - loss: 0.1716 - accuracy: 0.7795\n",
      "Epoch 193/1000\n",
      "23/23 [==============================] - 0s 880us/step - loss: 0.1715 - accuracy: 0.7795\n",
      "Epoch 194/1000\n",
      "23/23 [==============================] - 0s 882us/step - loss: 0.1714 - accuracy: 0.7795\n",
      "Epoch 195/1000\n",
      "23/23 [==============================] - 0s 797us/step - loss: 0.1713 - accuracy: 0.7795\n",
      "Epoch 196/1000\n",
      "23/23 [==============================] - 0s 809us/step - loss: 0.1711 - accuracy: 0.7795\n",
      "Epoch 197/1000\n",
      "23/23 [==============================] - 0s 861us/step - loss: 0.1710 - accuracy: 0.7795\n",
      "Epoch 198/1000\n",
      "23/23 [==============================] - 0s 871us/step - loss: 0.1709 - accuracy: 0.7795\n",
      "Epoch 199/1000\n",
      "23/23 [==============================] - 0s 855us/step - loss: 0.1708 - accuracy: 0.7795\n",
      "Epoch 200/1000\n",
      "23/23 [==============================] - 0s 815us/step - loss: 0.1707 - accuracy: 0.7795\n",
      "Epoch 201/1000\n",
      "23/23 [==============================] - 0s 798us/step - loss: 0.1706 - accuracy: 0.7795\n",
      "Epoch 202/1000\n",
      "23/23 [==============================] - 0s 832us/step - loss: 0.1705 - accuracy: 0.7795\n",
      "Epoch 203/1000\n",
      "23/23 [==============================] - 0s 881us/step - loss: 0.1704 - accuracy: 0.7795\n",
      "Epoch 204/1000\n",
      "23/23 [==============================] - 0s 865us/step - loss: 0.1703 - accuracy: 0.7795\n",
      "Epoch 205/1000\n",
      "23/23 [==============================] - 0s 845us/step - loss: 0.1702 - accuracy: 0.7795\n",
      "Epoch 206/1000\n",
      "23/23 [==============================] - 0s 836us/step - loss: 0.1701 - accuracy: 0.7795\n",
      "Epoch 207/1000\n",
      "23/23 [==============================] - 0s 945us/step - loss: 0.1700 - accuracy: 0.7795\n",
      "Epoch 208/1000\n",
      "23/23 [==============================] - 0s 858us/step - loss: 0.1699 - accuracy: 0.7795\n",
      "Epoch 209/1000\n",
      "23/23 [==============================] - 0s 825us/step - loss: 0.1698 - accuracy: 0.7795\n",
      "Epoch 210/1000\n",
      "23/23 [==============================] - 0s 817us/step - loss: 0.1697 - accuracy: 0.7795\n",
      "Epoch 211/1000\n",
      "23/23 [==============================] - 0s 892us/step - loss: 0.1696 - accuracy: 0.7795\n",
      "Epoch 212/1000\n",
      "23/23 [==============================] - 0s 837us/step - loss: 0.1695 - accuracy: 0.7795\n",
      "Epoch 213/1000\n",
      "23/23 [==============================] - 0s 857us/step - loss: 0.1694 - accuracy: 0.7795\n",
      "Epoch 214/1000\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.1693 - accuracy: 0.7795\n",
      "Epoch 215/1000\n",
      "23/23 [==============================] - 0s 823us/step - loss: 0.1692 - accuracy: 0.7795\n",
      "Epoch 216/1000\n",
      "23/23 [==============================] - 0s 882us/step - loss: 0.1691 - accuracy: 0.7795\n",
      "Epoch 217/1000\n",
      "23/23 [==============================] - 0s 840us/step - loss: 0.1690 - accuracy: 0.7795\n",
      "Epoch 218/1000\n",
      "23/23 [==============================] - 0s 789us/step - loss: 0.1689 - accuracy: 0.7795\n",
      "Epoch 219/1000\n",
      "23/23 [==============================] - 0s 887us/step - loss: 0.1688 - accuracy: 0.7795\n",
      "Epoch 220/1000\n",
      "23/23 [==============================] - 0s 854us/step - loss: 0.1687 - accuracy: 0.7795\n",
      "Epoch 221/1000\n",
      "23/23 [==============================] - 0s 817us/step - loss: 0.1686 - accuracy: 0.7795\n",
      "Epoch 222/1000\n",
      "23/23 [==============================] - 0s 831us/step - loss: 0.1685 - accuracy: 0.7795\n",
      "Epoch 223/1000\n",
      "23/23 [==============================] - 0s 819us/step - loss: 0.1684 - accuracy: 0.7795\n",
      "Epoch 224/1000\n",
      "23/23 [==============================] - 0s 814us/step - loss: 0.1684 - accuracy: 0.7795\n",
      "Epoch 225/1000\n",
      "23/23 [==============================] - 0s 858us/step - loss: 0.1683 - accuracy: 0.7795\n",
      "Epoch 226/1000\n",
      "23/23 [==============================] - 0s 805us/step - loss: 0.1682 - accuracy: 0.7795\n",
      "Epoch 227/1000\n",
      "23/23 [==============================] - 0s 853us/step - loss: 0.1681 - accuracy: 0.7795\n",
      "Epoch 228/1000\n",
      "23/23 [==============================] - 0s 809us/step - loss: 0.1680 - accuracy: 0.7795\n",
      "Epoch 229/1000\n",
      "23/23 [==============================] - 0s 845us/step - loss: 0.1679 - accuracy: 0.7795\n",
      "Epoch 230/1000\n",
      "23/23 [==============================] - 0s 852us/step - loss: 0.1678 - accuracy: 0.7795\n",
      "Epoch 231/1000\n",
      "23/23 [==============================] - 0s 868us/step - loss: 0.1677 - accuracy: 0.7795\n",
      "Epoch 232/1000\n",
      "23/23 [==============================] - 0s 845us/step - loss: 0.1677 - accuracy: 0.7795\n",
      "Epoch 233/1000\n",
      "23/23 [==============================] - 0s 860us/step - loss: 0.1676 - accuracy: 0.7795\n",
      "Epoch 234/1000\n",
      "23/23 [==============================] - 0s 791us/step - loss: 0.1675 - accuracy: 0.7795\n",
      "Epoch 235/1000\n",
      "23/23 [==============================] - 0s 830us/step - loss: 0.1674 - accuracy: 0.7795\n",
      "Epoch 236/1000\n",
      "23/23 [==============================] - 0s 795us/step - loss: 0.1673 - accuracy: 0.7795\n",
      "Epoch 237/1000\n",
      "23/23 [==============================] - 0s 799us/step - loss: 0.1673 - accuracy: 0.7795\n",
      "Epoch 238/1000\n",
      "23/23 [==============================] - 0s 832us/step - loss: 0.1672 - accuracy: 0.7795\n",
      "Epoch 239/1000\n",
      "23/23 [==============================] - 0s 776us/step - loss: 0.1671 - accuracy: 0.7795\n",
      "Epoch 240/1000\n",
      "23/23 [==============================] - 0s 875us/step - loss: 0.1670 - accuracy: 0.7795\n",
      "Epoch 241/1000\n",
      "23/23 [==============================] - 0s 851us/step - loss: 0.1669 - accuracy: 0.7795\n",
      "Epoch 242/1000\n",
      "23/23 [==============================] - 0s 807us/step - loss: 0.1669 - accuracy: 0.7795\n",
      "Epoch 243/1000\n",
      "23/23 [==============================] - 0s 865us/step - loss: 0.1668 - accuracy: 0.7795\n",
      "Epoch 244/1000\n",
      "23/23 [==============================] - 0s 843us/step - loss: 0.1667 - accuracy: 0.7795\n",
      "Epoch 245/1000\n",
      "23/23 [==============================] - 0s 825us/step - loss: 0.1666 - accuracy: 0.7795\n",
      "Epoch 246/1000\n",
      "23/23 [==============================] - 0s 838us/step - loss: 0.1666 - accuracy: 0.7795\n",
      "Epoch 247/1000\n",
      "23/23 [==============================] - 0s 832us/step - loss: 0.1665 - accuracy: 0.7795\n",
      "Epoch 248/1000\n",
      "23/23 [==============================] - 0s 844us/step - loss: 0.1664 - accuracy: 0.7795\n",
      "Epoch 249/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1663 - accuracy: 0.7795\n",
      "Epoch 250/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1662 - accuracy: 0.7795\n",
      "Epoch 251/1000\n",
      "23/23 [==============================] - 0s 811us/step - loss: 0.1662 - accuracy: 0.7795\n",
      "Epoch 252/1000\n",
      "23/23 [==============================] - 0s 862us/step - loss: 0.1661 - accuracy: 0.7795\n",
      "Epoch 253/1000\n",
      "23/23 [==============================] - 0s 885us/step - loss: 0.1660 - accuracy: 0.7795\n",
      "Epoch 254/1000\n",
      "23/23 [==============================] - 0s 869us/step - loss: 0.1660 - accuracy: 0.7795\n",
      "Epoch 255/1000\n",
      "23/23 [==============================] - 0s 846us/step - loss: 0.1659 - accuracy: 0.7795\n",
      "Epoch 256/1000\n",
      "23/23 [==============================] - 0s 908us/step - loss: 0.1658 - accuracy: 0.7795\n",
      "Epoch 257/1000\n",
      "23/23 [==============================] - 0s 944us/step - loss: 0.1657 - accuracy: 0.7795\n",
      "Epoch 258/1000\n",
      "23/23 [==============================] - 0s 867us/step - loss: 0.1657 - accuracy: 0.7795\n",
      "Epoch 259/1000\n",
      "23/23 [==============================] - 0s 860us/step - loss: 0.1656 - accuracy: 0.7795\n",
      "Epoch 260/1000\n",
      "23/23 [==============================] - 0s 837us/step - loss: 0.1655 - accuracy: 0.7795\n",
      "Epoch 261/1000\n",
      "23/23 [==============================] - 0s 863us/step - loss: 0.1655 - accuracy: 0.7795\n",
      "Epoch 262/1000\n",
      "23/23 [==============================] - 0s 835us/step - loss: 0.1654 - accuracy: 0.7795\n",
      "Epoch 263/1000\n",
      "23/23 [==============================] - 0s 785us/step - loss: 0.1653 - accuracy: 0.7795\n",
      "Epoch 264/1000\n",
      "23/23 [==============================] - 0s 877us/step - loss: 0.1653 - accuracy: 0.7795\n",
      "Epoch 265/1000\n",
      "23/23 [==============================] - 0s 879us/step - loss: 0.1652 - accuracy: 0.7795\n",
      "Epoch 266/1000\n",
      "23/23 [==============================] - 0s 829us/step - loss: 0.1651 - accuracy: 0.7795\n",
      "Epoch 267/1000\n",
      "23/23 [==============================] - 0s 933us/step - loss: 0.1651 - accuracy: 0.7795\n",
      "Epoch 268/1000\n",
      "23/23 [==============================] - 0s 926us/step - loss: 0.1650 - accuracy: 0.7795\n",
      "Epoch 269/1000\n",
      "23/23 [==============================] - 0s 826us/step - loss: 0.1649 - accuracy: 0.7795\n",
      "Epoch 270/1000\n",
      "23/23 [==============================] - 0s 891us/step - loss: 0.1649 - accuracy: 0.7795\n",
      "Epoch 271/1000\n",
      "23/23 [==============================] - 0s 829us/step - loss: 0.1648 - accuracy: 0.7795\n",
      "Epoch 272/1000\n",
      "23/23 [==============================] - 0s 845us/step - loss: 0.1647 - accuracy: 0.7795\n",
      "Epoch 273/1000\n",
      "23/23 [==============================] - 0s 845us/step - loss: 0.1647 - accuracy: 0.7795\n",
      "Epoch 274/1000\n",
      "23/23 [==============================] - 0s 873us/step - loss: 0.1646 - accuracy: 0.7795\n",
      "Epoch 275/1000\n",
      "23/23 [==============================] - 0s 899us/step - loss: 0.1645 - accuracy: 0.7795\n",
      "Epoch 276/1000\n",
      "23/23 [==============================] - 0s 871us/step - loss: 0.1645 - accuracy: 0.7795\n",
      "Epoch 277/1000\n",
      "23/23 [==============================] - 0s 825us/step - loss: 0.1644 - accuracy: 0.7795\n",
      "Epoch 278/1000\n",
      "23/23 [==============================] - 0s 845us/step - loss: 0.1643 - accuracy: 0.7795\n",
      "Epoch 279/1000\n",
      "23/23 [==============================] - 0s 825us/step - loss: 0.1643 - accuracy: 0.7795\n",
      "Epoch 280/1000\n",
      "23/23 [==============================] - 0s 805us/step - loss: 0.1642 - accuracy: 0.7795\n",
      "Epoch 281/1000\n",
      "23/23 [==============================] - 0s 770us/step - loss: 0.1642 - accuracy: 0.7795\n",
      "Epoch 282/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1641 - accuracy: 0.7795\n",
      "Epoch 283/1000\n",
      "23/23 [==============================] - 0s 948us/step - loss: 0.1641 - accuracy: 0.7795\n",
      "Epoch 284/1000\n",
      "23/23 [==============================] - 0s 867us/step - loss: 0.1640 - accuracy: 0.7795\n",
      "Epoch 285/1000\n",
      "23/23 [==============================] - 0s 883us/step - loss: 0.1639 - accuracy: 0.7795\n",
      "Epoch 286/1000\n",
      "23/23 [==============================] - 0s 877us/step - loss: 0.1639 - accuracy: 0.7795\n",
      "Epoch 287/1000\n",
      "23/23 [==============================] - 0s 875us/step - loss: 0.1638 - accuracy: 0.7795\n",
      "Epoch 288/1000\n",
      "23/23 [==============================] - 0s 911us/step - loss: 0.1638 - accuracy: 0.7795\n",
      "Epoch 289/1000\n",
      "23/23 [==============================] - 0s 918us/step - loss: 0.1637 - accuracy: 0.7795\n",
      "Epoch 290/1000\n",
      "23/23 [==============================] - 0s 917us/step - loss: 0.1636 - accuracy: 0.7795\n",
      "Epoch 291/1000\n",
      "23/23 [==============================] - 0s 898us/step - loss: 0.1636 - accuracy: 0.7795\n",
      "Epoch 292/1000\n",
      "23/23 [==============================] - 0s 884us/step - loss: 0.1635 - accuracy: 0.7795\n",
      "Epoch 293/1000\n",
      "23/23 [==============================] - 0s 841us/step - loss: 0.1635 - accuracy: 0.7795\n",
      "Epoch 294/1000\n",
      "23/23 [==============================] - 0s 784us/step - loss: 0.1634 - accuracy: 0.7795\n",
      "Epoch 295/1000\n",
      "23/23 [==============================] - 0s 858us/step - loss: 0.1634 - accuracy: 0.7795\n",
      "Epoch 296/1000\n",
      "23/23 [==============================] - 0s 838us/step - loss: 0.1633 - accuracy: 0.7795\n",
      "Epoch 297/1000\n",
      "23/23 [==============================] - 0s 856us/step - loss: 0.1632 - accuracy: 0.7795\n",
      "Epoch 298/1000\n",
      "23/23 [==============================] - 0s 834us/step - loss: 0.1632 - accuracy: 0.7795\n",
      "Epoch 299/1000\n",
      "23/23 [==============================] - 0s 876us/step - loss: 0.1631 - accuracy: 0.7795\n",
      "Epoch 300/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1631 - accuracy: 0.7795\n",
      "Epoch 301/1000\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.1630 - accuracy: 0.7795\n",
      "Epoch 302/1000\n",
      "23/23 [==============================] - 0s 867us/step - loss: 0.1630 - accuracy: 0.7795\n",
      "Epoch 303/1000\n",
      "23/23 [==============================] - 0s 880us/step - loss: 0.1629 - accuracy: 0.7795\n",
      "Epoch 304/1000\n",
      "23/23 [==============================] - 0s 856us/step - loss: 0.1629 - accuracy: 0.7795\n",
      "Epoch 305/1000\n",
      "23/23 [==============================] - 0s 889us/step - loss: 0.1628 - accuracy: 0.7795\n",
      "Epoch 306/1000\n",
      "23/23 [==============================] - 0s 855us/step - loss: 0.1628 - accuracy: 0.7795\n",
      "Epoch 307/1000\n",
      "23/23 [==============================] - 0s 905us/step - loss: 0.1627 - accuracy: 0.7795\n",
      "Epoch 308/1000\n",
      "23/23 [==============================] - 0s 882us/step - loss: 0.1627 - accuracy: 0.7795\n",
      "Epoch 309/1000\n",
      "23/23 [==============================] - 0s 879us/step - loss: 0.1626 - accuracy: 0.7795\n",
      "Epoch 310/1000\n",
      "23/23 [==============================] - 0s 848us/step - loss: 0.1626 - accuracy: 0.7795\n",
      "Epoch 311/1000\n",
      "23/23 [==============================] - 0s 895us/step - loss: 0.1625 - accuracy: 0.7795\n",
      "Epoch 312/1000\n",
      "23/23 [==============================] - 0s 809us/step - loss: 0.1625 - accuracy: 0.7795\n",
      "Epoch 313/1000\n",
      "23/23 [==============================] - 0s 850us/step - loss: 0.1624 - accuracy: 0.7795\n",
      "Epoch 314/1000\n",
      "23/23 [==============================] - 0s 884us/step - loss: 0.1624 - accuracy: 0.7795\n",
      "Epoch 315/1000\n",
      "23/23 [==============================] - 0s 833us/step - loss: 0.1623 - accuracy: 0.7795\n",
      "Epoch 316/1000\n",
      "23/23 [==============================] - 0s 841us/step - loss: 0.1623 - accuracy: 0.7795\n",
      "Epoch 317/1000\n",
      "23/23 [==============================] - 0s 850us/step - loss: 0.1622 - accuracy: 0.7795\n",
      "Epoch 318/1000\n",
      "23/23 [==============================] - 0s 826us/step - loss: 0.1622 - accuracy: 0.7795\n",
      "Epoch 319/1000\n",
      "23/23 [==============================] - 0s 824us/step - loss: 0.1621 - accuracy: 0.7795\n",
      "Epoch 320/1000\n",
      "23/23 [==============================] - 0s 793us/step - loss: 0.1621 - accuracy: 0.7795\n",
      "Epoch 321/1000\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.1620 - accuracy: 0.7795\n",
      "Epoch 322/1000\n",
      "23/23 [==============================] - 0s 810us/step - loss: 0.1620 - accuracy: 0.7795\n",
      "Epoch 323/1000\n",
      "23/23 [==============================] - 0s 810us/step - loss: 0.1619 - accuracy: 0.7795\n",
      "Epoch 324/1000\n",
      "23/23 [==============================] - 0s 830us/step - loss: 0.1619 - accuracy: 0.7795\n",
      "Epoch 325/1000\n",
      "23/23 [==============================] - 0s 785us/step - loss: 0.1618 - accuracy: 0.7795\n",
      "Epoch 326/1000\n",
      "23/23 [==============================] - 0s 867us/step - loss: 0.1618 - accuracy: 0.7795\n",
      "Epoch 327/1000\n",
      "23/23 [==============================] - 0s 836us/step - loss: 0.1617 - accuracy: 0.7795\n",
      "Epoch 328/1000\n",
      "23/23 [==============================] - 0s 834us/step - loss: 0.1617 - accuracy: 0.7795\n",
      "Epoch 329/1000\n",
      "23/23 [==============================] - 0s 872us/step - loss: 0.1617 - accuracy: 0.7795\n",
      "Epoch 330/1000\n",
      "23/23 [==============================] - 0s 848us/step - loss: 0.1616 - accuracy: 0.7795\n",
      "Epoch 331/1000\n",
      "23/23 [==============================] - 0s 841us/step - loss: 0.1616 - accuracy: 0.7795\n",
      "Epoch 332/1000\n",
      "23/23 [==============================] - 0s 821us/step - loss: 0.1615 - accuracy: 0.7795\n",
      "Epoch 333/1000\n",
      "23/23 [==============================] - 0s 825us/step - loss: 0.1615 - accuracy: 0.7795\n",
      "Epoch 334/1000\n",
      "23/23 [==============================] - 0s 814us/step - loss: 0.1614 - accuracy: 0.7795\n",
      "Epoch 335/1000\n",
      "23/23 [==============================] - 0s 826us/step - loss: 0.1614 - accuracy: 0.7795\n",
      "Epoch 336/1000\n",
      "23/23 [==============================] - 0s 832us/step - loss: 0.1614 - accuracy: 0.7795\n",
      "Epoch 337/1000\n",
      "23/23 [==============================] - 0s 880us/step - loss: 0.1613 - accuracy: 0.7795\n",
      "Epoch 338/1000\n",
      "23/23 [==============================] - 0s 873us/step - loss: 0.1613 - accuracy: 0.7795\n",
      "Epoch 339/1000\n",
      "23/23 [==============================] - 0s 880us/step - loss: 0.1612 - accuracy: 0.7795\n",
      "Epoch 340/1000\n",
      "23/23 [==============================] - 0s 862us/step - loss: 0.1612 - accuracy: 0.7795\n",
      "Epoch 341/1000\n",
      "23/23 [==============================] - 0s 804us/step - loss: 0.1611 - accuracy: 0.7795\n",
      "Epoch 342/1000\n",
      "23/23 [==============================] - 0s 868us/step - loss: 0.1611 - accuracy: 0.7795\n",
      "Epoch 343/1000\n",
      "23/23 [==============================] - 0s 868us/step - loss: 0.1611 - accuracy: 0.7795\n",
      "Epoch 344/1000\n",
      "23/23 [==============================] - 0s 848us/step - loss: 0.1610 - accuracy: 0.7795\n",
      "Epoch 345/1000\n",
      "23/23 [==============================] - 0s 834us/step - loss: 0.1610 - accuracy: 0.7795\n",
      "Epoch 346/1000\n",
      "23/23 [==============================] - 0s 845us/step - loss: 0.1609 - accuracy: 0.7795\n",
      "Epoch 347/1000\n",
      "23/23 [==============================] - 0s 803us/step - loss: 0.1609 - accuracy: 0.7795\n",
      "Epoch 348/1000\n",
      "23/23 [==============================] - 0s 827us/step - loss: 0.1608 - accuracy: 0.7795\n",
      "Epoch 349/1000\n",
      "23/23 [==============================] - 0s 849us/step - loss: 0.1608 - accuracy: 0.7795\n",
      "Epoch 350/1000\n",
      "23/23 [==============================] - 0s 824us/step - loss: 0.1608 - accuracy: 0.7795\n",
      "Epoch 351/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1607 - accuracy: 0.7795\n",
      "Epoch 352/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1607 - accuracy: 0.7795\n",
      "Epoch 353/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1606 - accuracy: 0.7795\n",
      "Epoch 354/1000\n",
      "23/23 [==============================] - 0s 842us/step - loss: 0.1606 - accuracy: 0.7795\n",
      "Epoch 355/1000\n",
      "23/23 [==============================] - 0s 827us/step - loss: 0.1606 - accuracy: 0.7795\n",
      "Epoch 356/1000\n",
      "23/23 [==============================] - 0s 776us/step - loss: 0.1605 - accuracy: 0.7795\n",
      "Epoch 357/1000\n",
      "23/23 [==============================] - 0s 847us/step - loss: 0.1605 - accuracy: 0.7795\n",
      "Epoch 358/1000\n",
      "23/23 [==============================] - 0s 801us/step - loss: 0.1605 - accuracy: 0.7795\n",
      "Epoch 359/1000\n",
      "23/23 [==============================] - 0s 812us/step - loss: 0.1604 - accuracy: 0.7795\n",
      "Epoch 360/1000\n",
      "23/23 [==============================] - 0s 863us/step - loss: 0.1604 - accuracy: 0.7795\n",
      "Epoch 361/1000\n",
      "23/23 [==============================] - 0s 912us/step - loss: 0.1603 - accuracy: 0.7795\n",
      "Epoch 362/1000\n",
      "23/23 [==============================] - 0s 857us/step - loss: 0.1603 - accuracy: 0.7795\n",
      "Epoch 363/1000\n",
      "23/23 [==============================] - 0s 839us/step - loss: 0.1603 - accuracy: 0.7795\n",
      "Epoch 364/1000\n",
      "23/23 [==============================] - 0s 777us/step - loss: 0.1602 - accuracy: 0.7795\n",
      "Epoch 365/1000\n",
      "23/23 [==============================] - 0s 847us/step - loss: 0.1602 - accuracy: 0.7795\n",
      "Epoch 366/1000\n",
      "23/23 [==============================] - 0s 802us/step - loss: 0.1602 - accuracy: 0.7795\n",
      "Epoch 367/1000\n",
      "23/23 [==============================] - 0s 792us/step - loss: 0.1601 - accuracy: 0.7795\n",
      "Epoch 368/1000\n",
      "23/23 [==============================] - 0s 844us/step - loss: 0.1601 - accuracy: 0.7795\n",
      "Epoch 369/1000\n",
      "23/23 [==============================] - 0s 816us/step - loss: 0.1600 - accuracy: 0.7795\n",
      "Epoch 370/1000\n",
      "23/23 [==============================] - 0s 885us/step - loss: 0.1600 - accuracy: 0.7795\n",
      "Epoch 371/1000\n",
      "23/23 [==============================] - 0s 911us/step - loss: 0.1600 - accuracy: 0.7795\n",
      "Epoch 372/1000\n",
      "23/23 [==============================] - 0s 852us/step - loss: 0.1599 - accuracy: 0.7795\n",
      "Epoch 373/1000\n",
      "23/23 [==============================] - 0s 839us/step - loss: 0.1599 - accuracy: 0.7795\n",
      "Epoch 374/1000\n",
      "23/23 [==============================] - 0s 832us/step - loss: 0.1599 - accuracy: 0.7795\n",
      "Epoch 375/1000\n",
      "23/23 [==============================] - 0s 875us/step - loss: 0.1598 - accuracy: 0.7795\n",
      "Epoch 376/1000\n",
      "23/23 [==============================] - 0s 872us/step - loss: 0.1598 - accuracy: 0.7795\n",
      "Epoch 377/1000\n",
      "23/23 [==============================] - 0s 871us/step - loss: 0.1598 - accuracy: 0.7795\n",
      "Epoch 378/1000\n",
      "23/23 [==============================] - 0s 873us/step - loss: 0.1597 - accuracy: 0.7795\n",
      "Epoch 379/1000\n",
      "23/23 [==============================] - 0s 829us/step - loss: 0.1597 - accuracy: 0.7795\n",
      "Epoch 380/1000\n",
      "23/23 [==============================] - 0s 797us/step - loss: 0.1596 - accuracy: 0.7795\n",
      "Epoch 381/1000\n",
      "23/23 [==============================] - 0s 914us/step - loss: 0.1596 - accuracy: 0.7795\n",
      "Epoch 382/1000\n",
      "23/23 [==============================] - 0s 933us/step - loss: 0.1596 - accuracy: 0.7795\n",
      "Epoch 383/1000\n",
      "23/23 [==============================] - 0s 917us/step - loss: 0.1595 - accuracy: 0.7795\n",
      "Epoch 384/1000\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.1595 - accuracy: 0.7795\n",
      "Epoch 385/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1595 - accuracy: 0.7795\n",
      "Epoch 386/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1594 - accuracy: 0.7795\n",
      "Epoch 387/1000\n",
      "23/23 [==============================] - 0s 850us/step - loss: 0.1594 - accuracy: 0.7795\n",
      "Epoch 388/1000\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.1594 - accuracy: 0.7795\n",
      "Epoch 389/1000\n",
      "23/23 [==============================] - 0s 900us/step - loss: 0.1593 - accuracy: 0.7795\n",
      "Epoch 390/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1593 - accuracy: 0.7795\n",
      "Epoch 391/1000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1593 - accuracy: 0.7795\n",
      "Epoch 392/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1592 - accuracy: 0.7795\n",
      "Epoch 393/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1592 - accuracy: 0.7795\n",
      "Epoch 394/1000\n",
      "23/23 [==============================] - 0s 853us/step - loss: 0.1592 - accuracy: 0.7795\n",
      "Epoch 395/1000\n",
      "23/23 [==============================] - 0s 885us/step - loss: 0.1591 - accuracy: 0.7795\n",
      "Epoch 396/1000\n",
      "23/23 [==============================] - 0s 962us/step - loss: 0.1591 - accuracy: 0.7795\n",
      "Epoch 397/1000\n",
      "23/23 [==============================] - 0s 946us/step - loss: 0.1591 - accuracy: 0.7795\n",
      "Epoch 398/1000\n",
      "23/23 [==============================] - 0s 978us/step - loss: 0.1590 - accuracy: 0.7795\n",
      "Epoch 399/1000\n",
      "23/23 [==============================] - 0s 914us/step - loss: 0.1590 - accuracy: 0.7795\n",
      "Epoch 400/1000\n",
      "23/23 [==============================] - 0s 852us/step - loss: 0.1590 - accuracy: 0.7795\n",
      "Epoch 401/1000\n",
      "23/23 [==============================] - 0s 831us/step - loss: 0.1589 - accuracy: 0.7795\n",
      "Epoch 402/1000\n",
      "23/23 [==============================] - 0s 857us/step - loss: 0.1589 - accuracy: 0.7795\n",
      "Epoch 403/1000\n",
      "23/23 [==============================] - 0s 812us/step - loss: 0.1589 - accuracy: 0.7795\n",
      "Epoch 404/1000\n",
      "23/23 [==============================] - 0s 827us/step - loss: 0.1589 - accuracy: 0.7795\n",
      "Epoch 405/1000\n",
      "23/23 [==============================] - 0s 799us/step - loss: 0.1588 - accuracy: 0.7795\n",
      "Epoch 406/1000\n",
      "23/23 [==============================] - 0s 788us/step - loss: 0.1588 - accuracy: 0.7795\n",
      "Epoch 407/1000\n",
      "23/23 [==============================] - 0s 832us/step - loss: 0.1588 - accuracy: 0.7795\n",
      "Epoch 408/1000\n",
      "23/23 [==============================] - 0s 853us/step - loss: 0.1587 - accuracy: 0.7795\n",
      "Epoch 409/1000\n",
      "23/23 [==============================] - 0s 905us/step - loss: 0.1587 - accuracy: 0.7795\n",
      "Epoch 410/1000\n",
      "23/23 [==============================] - 0s 929us/step - loss: 0.1587 - accuracy: 0.7795\n",
      "Epoch 411/1000\n",
      "23/23 [==============================] - 0s 873us/step - loss: 0.1586 - accuracy: 0.7795\n",
      "Epoch 412/1000\n",
      "23/23 [==============================] - 0s 954us/step - loss: 0.1586 - accuracy: 0.7795\n",
      "Epoch 413/1000\n",
      "23/23 [==============================] - 0s 838us/step - loss: 0.1586 - accuracy: 0.7795\n",
      "Epoch 414/1000\n",
      "23/23 [==============================] - 0s 821us/step - loss: 0.1586 - accuracy: 0.7795\n",
      "Epoch 415/1000\n",
      "23/23 [==============================] - 0s 888us/step - loss: 0.1585 - accuracy: 0.7795\n",
      "Epoch 416/1000\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.1585 - accuracy: 0.7795\n",
      "Epoch 417/1000\n",
      "23/23 [==============================] - 0s 792us/step - loss: 0.1585 - accuracy: 0.7795\n",
      "Epoch 418/1000\n",
      "23/23 [==============================] - 0s 833us/step - loss: 0.1584 - accuracy: 0.7795\n",
      "Epoch 419/1000\n",
      "23/23 [==============================] - 0s 809us/step - loss: 0.1584 - accuracy: 0.7795\n",
      "Epoch 420/1000\n",
      "23/23 [==============================] - 0s 833us/step - loss: 0.1584 - accuracy: 0.7795\n",
      "Epoch 421/1000\n",
      "23/23 [==============================] - 0s 817us/step - loss: 0.1583 - accuracy: 0.7795\n",
      "Epoch 422/1000\n",
      "23/23 [==============================] - 0s 835us/step - loss: 0.1583 - accuracy: 0.7795\n",
      "Epoch 423/1000\n",
      "23/23 [==============================] - 0s 888us/step - loss: 0.1583 - accuracy: 0.7795\n",
      "Epoch 424/1000\n",
      "23/23 [==============================] - 0s 844us/step - loss: 0.1583 - accuracy: 0.7795\n",
      "Epoch 425/1000\n",
      "23/23 [==============================] - 0s 816us/step - loss: 0.1582 - accuracy: 0.7795\n",
      "Epoch 426/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1582 - accuracy: 0.7795\n",
      "Epoch 427/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1582 - accuracy: 0.7795\n",
      "Epoch 428/1000\n",
      "23/23 [==============================] - 0s 930us/step - loss: 0.1582 - accuracy: 0.7795\n",
      "Epoch 429/1000\n",
      "23/23 [==============================] - 0s 834us/step - loss: 0.1581 - accuracy: 0.7795\n",
      "Epoch 430/1000\n",
      "23/23 [==============================] - 0s 829us/step - loss: 0.1581 - accuracy: 0.7795\n",
      "Epoch 431/1000\n",
      "23/23 [==============================] - 0s 813us/step - loss: 0.1581 - accuracy: 0.7795\n",
      "Epoch 432/1000\n",
      "23/23 [==============================] - 0s 780us/step - loss: 0.1581 - accuracy: 0.7795\n",
      "Epoch 433/1000\n",
      "23/23 [==============================] - 0s 781us/step - loss: 0.1580 - accuracy: 0.7795\n",
      "Epoch 434/1000\n",
      "23/23 [==============================] - 0s 759us/step - loss: 0.1580 - accuracy: 0.7795\n",
      "Epoch 435/1000\n",
      "23/23 [==============================] - 0s 805us/step - loss: 0.1580 - accuracy: 0.7795\n",
      "Epoch 436/1000\n",
      "23/23 [==============================] - 0s 834us/step - loss: 0.1579 - accuracy: 0.7795\n",
      "Epoch 437/1000\n",
      "23/23 [==============================] - 0s 899us/step - loss: 0.1579 - accuracy: 0.7795\n",
      "Epoch 438/1000\n",
      "23/23 [==============================] - 0s 855us/step - loss: 0.1579 - accuracy: 0.7795\n",
      "Epoch 439/1000\n",
      "23/23 [==============================] - 0s 850us/step - loss: 0.1579 - accuracy: 0.7795\n",
      "Epoch 440/1000\n",
      "23/23 [==============================] - 0s 845us/step - loss: 0.1578 - accuracy: 0.7795\n",
      "Epoch 441/1000\n",
      "23/23 [==============================] - 0s 872us/step - loss: 0.1578 - accuracy: 0.7795\n",
      "Epoch 442/1000\n",
      "23/23 [==============================] - 0s 883us/step - loss: 0.1578 - accuracy: 0.7795\n",
      "Epoch 443/1000\n",
      "23/23 [==============================] - 0s 810us/step - loss: 0.1578 - accuracy: 0.7795\n",
      "Epoch 444/1000\n",
      "23/23 [==============================] - 0s 834us/step - loss: 0.1577 - accuracy: 0.7795\n",
      "Epoch 445/1000\n",
      "23/23 [==============================] - 0s 810us/step - loss: 0.1577 - accuracy: 0.7795\n",
      "Epoch 446/1000\n",
      "23/23 [==============================] - 0s 832us/step - loss: 0.1577 - accuracy: 0.7795\n",
      "Epoch 447/1000\n",
      "23/23 [==============================] - 0s 799us/step - loss: 0.1577 - accuracy: 0.7795\n",
      "Epoch 448/1000\n",
      "23/23 [==============================] - 0s 816us/step - loss: 0.1576 - accuracy: 0.7795\n",
      "Epoch 449/1000\n",
      "23/23 [==============================] - 0s 908us/step - loss: 0.1576 - accuracy: 0.7795\n",
      "Epoch 450/1000\n",
      "23/23 [==============================] - 0s 865us/step - loss: 0.1576 - accuracy: 0.7795\n",
      "Epoch 451/1000\n",
      "23/23 [==============================] - 0s 820us/step - loss: 0.1576 - accuracy: 0.7795\n",
      "Epoch 452/1000\n",
      "23/23 [==============================] - 0s 814us/step - loss: 0.1575 - accuracy: 0.7795\n",
      "Epoch 453/1000\n",
      "23/23 [==============================] - 0s 761us/step - loss: 0.1575 - accuracy: 0.7795\n",
      "Epoch 454/1000\n",
      "23/23 [==============================] - 0s 794us/step - loss: 0.1575 - accuracy: 0.7795\n",
      "Epoch 455/1000\n",
      "23/23 [==============================] - 0s 853us/step - loss: 0.1575 - accuracy: 0.7795\n",
      "Epoch 456/1000\n",
      "23/23 [==============================] - 0s 890us/step - loss: 0.1574 - accuracy: 0.7795\n",
      "Epoch 457/1000\n",
      "23/23 [==============================] - 0s 861us/step - loss: 0.1574 - accuracy: 0.7795\n",
      "Epoch 458/1000\n",
      "23/23 [==============================] - 0s 892us/step - loss: 0.1574 - accuracy: 0.7795\n",
      "Epoch 459/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1574 - accuracy: 0.7795\n",
      "Epoch 460/1000\n",
      "23/23 [==============================] - 0s 899us/step - loss: 0.1573 - accuracy: 0.7795\n",
      "Epoch 461/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1573 - accuracy: 0.7795\n",
      "Epoch 462/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1573 - accuracy: 0.7795\n",
      "Epoch 463/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1573 - accuracy: 0.7795\n",
      "Epoch 464/1000\n",
      "23/23 [==============================] - 0s 841us/step - loss: 0.1572 - accuracy: 0.7795\n",
      "Epoch 465/1000\n",
      "23/23 [==============================] - 0s 894us/step - loss: 0.1572 - accuracy: 0.7795\n",
      "Epoch 466/1000\n",
      "23/23 [==============================] - 0s 904us/step - loss: 0.1572 - accuracy: 0.7795\n",
      "Epoch 467/1000\n",
      "23/23 [==============================] - 0s 817us/step - loss: 0.1572 - accuracy: 0.7795\n",
      "Epoch 468/1000\n",
      "23/23 [==============================] - 0s 899us/step - loss: 0.1572 - accuracy: 0.7795\n",
      "Epoch 469/1000\n",
      "23/23 [==============================] - 0s 982us/step - loss: 0.1571 - accuracy: 0.7795\n",
      "Epoch 470/1000\n",
      "23/23 [==============================] - 0s 960us/step - loss: 0.1571 - accuracy: 0.7795\n",
      "Epoch 471/1000\n",
      "23/23 [==============================] - 0s 935us/step - loss: 0.1571 - accuracy: 0.7795\n",
      "Epoch 472/1000\n",
      "23/23 [==============================] - 0s 876us/step - loss: 0.1571 - accuracy: 0.7795\n",
      "Epoch 473/1000\n",
      "23/23 [==============================] - 0s 835us/step - loss: 0.1570 - accuracy: 0.7795\n",
      "Epoch 474/1000\n",
      "23/23 [==============================] - 0s 819us/step - loss: 0.1570 - accuracy: 0.7795\n",
      "Epoch 475/1000\n",
      "23/23 [==============================] - 0s 812us/step - loss: 0.1570 - accuracy: 0.7795\n",
      "Epoch 476/1000\n",
      "23/23 [==============================] - 0s 837us/step - loss: 0.1570 - accuracy: 0.7795\n",
      "Epoch 477/1000\n",
      "23/23 [==============================] - 0s 833us/step - loss: 0.1569 - accuracy: 0.7795\n",
      "Epoch 478/1000\n",
      "23/23 [==============================] - 0s 889us/step - loss: 0.1569 - accuracy: 0.7795\n",
      "Epoch 479/1000\n",
      "23/23 [==============================] - 0s 812us/step - loss: 0.1569 - accuracy: 0.7795\n",
      "Epoch 480/1000\n",
      "23/23 [==============================] - 0s 800us/step - loss: 0.1569 - accuracy: 0.7795\n",
      "Epoch 481/1000\n",
      "23/23 [==============================] - 0s 920us/step - loss: 0.1568 - accuracy: 0.7795\n",
      "Epoch 482/1000\n",
      "23/23 [==============================] - 0s 884us/step - loss: 0.1568 - accuracy: 0.7795\n",
      "Epoch 483/1000\n",
      "23/23 [==============================] - 0s 947us/step - loss: 0.1568 - accuracy: 0.7795\n",
      "Epoch 484/1000\n",
      "23/23 [==============================] - 0s 829us/step - loss: 0.1568 - accuracy: 0.7795\n",
      "Epoch 485/1000\n",
      "23/23 [==============================] - 0s 908us/step - loss: 0.1568 - accuracy: 0.7795\n",
      "Epoch 486/1000\n",
      "23/23 [==============================] - 0s 918us/step - loss: 0.1567 - accuracy: 0.7795\n",
      "Epoch 487/1000\n",
      "23/23 [==============================] - 0s 916us/step - loss: 0.1567 - accuracy: 0.7795\n",
      "Epoch 488/1000\n",
      "23/23 [==============================] - 0s 892us/step - loss: 0.1567 - accuracy: 0.7795\n",
      "Epoch 489/1000\n",
      "23/23 [==============================] - 0s 921us/step - loss: 0.1567 - accuracy: 0.7795\n",
      "Epoch 490/1000\n",
      "23/23 [==============================] - 0s 830us/step - loss: 0.1567 - accuracy: 0.7795\n",
      "Epoch 491/1000\n",
      "23/23 [==============================] - 0s 852us/step - loss: 0.1566 - accuracy: 0.7795\n",
      "Epoch 492/1000\n",
      "23/23 [==============================] - 0s 885us/step - loss: 0.1566 - accuracy: 0.7795\n",
      "Epoch 493/1000\n",
      "23/23 [==============================] - 0s 863us/step - loss: 0.1566 - accuracy: 0.7795\n",
      "Epoch 494/1000\n",
      "23/23 [==============================] - 0s 841us/step - loss: 0.1566 - accuracy: 0.7795\n",
      "Epoch 495/1000\n",
      "23/23 [==============================] - 0s 803us/step - loss: 0.1565 - accuracy: 0.7795\n",
      "Epoch 496/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1565 - accuracy: 0.7795\n",
      "Epoch 497/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1565 - accuracy: 0.7795\n",
      "Epoch 498/1000\n",
      "23/23 [==============================] - 0s 998us/step - loss: 0.1565 - accuracy: 0.7795\n",
      "Epoch 499/1000\n",
      "23/23 [==============================] - 0s 816us/step - loss: 0.1565 - accuracy: 0.7795\n",
      "Epoch 500/1000\n",
      "23/23 [==============================] - 0s 799us/step - loss: 0.1564 - accuracy: 0.7795\n",
      "Epoch 501/1000\n",
      "23/23 [==============================] - 0s 827us/step - loss: 0.1564 - accuracy: 0.7795\n",
      "Epoch 502/1000\n",
      "23/23 [==============================] - 0s 836us/step - loss: 0.1564 - accuracy: 0.7795\n",
      "Epoch 503/1000\n",
      "23/23 [==============================] - 0s 899us/step - loss: 0.1564 - accuracy: 0.7795\n",
      "Epoch 504/1000\n",
      "23/23 [==============================] - 0s 785us/step - loss: 0.1564 - accuracy: 0.7795\n",
      "Epoch 505/1000\n",
      "23/23 [==============================] - 0s 889us/step - loss: 0.1563 - accuracy: 0.7795\n",
      "Epoch 506/1000\n",
      "23/23 [==============================] - 0s 841us/step - loss: 0.1563 - accuracy: 0.7795\n",
      "Epoch 507/1000\n",
      "23/23 [==============================] - 0s 769us/step - loss: 0.1563 - accuracy: 0.7795\n",
      "Epoch 508/1000\n",
      "23/23 [==============================] - 0s 781us/step - loss: 0.1563 - accuracy: 0.7795\n",
      "Epoch 509/1000\n",
      "23/23 [==============================] - 0s 780us/step - loss: 0.1563 - accuracy: 0.7795\n",
      "Epoch 510/1000\n",
      "23/23 [==============================] - 0s 822us/step - loss: 0.1562 - accuracy: 0.7795\n",
      "Epoch 511/1000\n",
      "23/23 [==============================] - 0s 827us/step - loss: 0.1562 - accuracy: 0.7795\n",
      "Epoch 512/1000\n",
      "23/23 [==============================] - 0s 850us/step - loss: 0.1562 - accuracy: 0.7795\n",
      "Epoch 513/1000\n",
      "23/23 [==============================] - 0s 809us/step - loss: 0.1562 - accuracy: 0.7795\n",
      "Epoch 514/1000\n",
      "23/23 [==============================] - 0s 812us/step - loss: 0.1562 - accuracy: 0.7795\n",
      "Epoch 515/1000\n",
      "23/23 [==============================] - 0s 800us/step - loss: 0.1561 - accuracy: 0.7795\n",
      "Epoch 516/1000\n",
      "23/23 [==============================] - 0s 859us/step - loss: 0.1561 - accuracy: 0.7795\n",
      "Epoch 517/1000\n",
      "23/23 [==============================] - 0s 861us/step - loss: 0.1561 - accuracy: 0.7795\n",
      "Epoch 518/1000\n",
      "23/23 [==============================] - 0s 820us/step - loss: 0.1561 - accuracy: 0.7795\n",
      "Epoch 519/1000\n",
      "23/23 [==============================] - 0s 784us/step - loss: 0.1561 - accuracy: 0.7795\n",
      "Epoch 520/1000\n",
      "23/23 [==============================] - 0s 803us/step - loss: 0.1560 - accuracy: 0.7795\n",
      "Epoch 521/1000\n",
      "23/23 [==============================] - 0s 841us/step - loss: 0.1560 - accuracy: 0.7795\n",
      "Epoch 522/1000\n",
      "23/23 [==============================] - 0s 833us/step - loss: 0.1560 - accuracy: 0.7795\n",
      "Epoch 523/1000\n",
      "23/23 [==============================] - 0s 880us/step - loss: 0.1560 - accuracy: 0.7795\n",
      "Epoch 524/1000\n",
      "23/23 [==============================] - 0s 780us/step - loss: 0.1560 - accuracy: 0.7795\n",
      "Epoch 525/1000\n",
      "23/23 [==============================] - 0s 851us/step - loss: 0.1559 - accuracy: 0.7795\n",
      "Epoch 526/1000\n",
      "23/23 [==============================] - 0s 848us/step - loss: 0.1559 - accuracy: 0.7795\n",
      "Epoch 527/1000\n",
      "23/23 [==============================] - 0s 816us/step - loss: 0.1559 - accuracy: 0.7795\n",
      "Epoch 528/1000\n",
      "23/23 [==============================] - 0s 817us/step - loss: 0.1559 - accuracy: 0.7795\n",
      "Epoch 529/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1559 - accuracy: 0.7795\n",
      "Epoch 530/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1559 - accuracy: 0.7795\n",
      "Epoch 531/1000\n",
      "23/23 [==============================] - 0s 898us/step - loss: 0.1558 - accuracy: 0.7795\n",
      "Epoch 532/1000\n",
      "23/23 [==============================] - 0s 871us/step - loss: 0.1558 - accuracy: 0.7795\n",
      "Epoch 533/1000\n",
      "23/23 [==============================] - 0s 846us/step - loss: 0.1558 - accuracy: 0.7795\n",
      "Epoch 534/1000\n",
      "23/23 [==============================] - 0s 816us/step - loss: 0.1558 - accuracy: 0.7795\n",
      "Epoch 535/1000\n",
      "23/23 [==============================] - 0s 877us/step - loss: 0.1558 - accuracy: 0.7795\n",
      "Epoch 536/1000\n",
      "23/23 [==============================] - 0s 805us/step - loss: 0.1557 - accuracy: 0.7795\n",
      "Epoch 537/1000\n",
      "23/23 [==============================] - 0s 835us/step - loss: 0.1557 - accuracy: 0.7795\n",
      "Epoch 538/1000\n",
      "23/23 [==============================] - 0s 814us/step - loss: 0.1557 - accuracy: 0.7795\n",
      "Epoch 539/1000\n",
      "23/23 [==============================] - 0s 815us/step - loss: 0.1557 - accuracy: 0.7795\n",
      "Epoch 540/1000\n",
      "23/23 [==============================] - 0s 845us/step - loss: 0.1557 - accuracy: 0.7795\n",
      "Epoch 541/1000\n",
      "23/23 [==============================] - 0s 833us/step - loss: 0.1556 - accuracy: 0.7795\n",
      "Epoch 542/1000\n",
      "23/23 [==============================] - 0s 743us/step - loss: 0.1556 - accuracy: 0.7795\n",
      "Epoch 543/1000\n",
      "23/23 [==============================] - 0s 770us/step - loss: 0.1556 - accuracy: 0.7795\n",
      "Epoch 544/1000\n",
      "23/23 [==============================] - 0s 787us/step - loss: 0.1556 - accuracy: 0.7795\n",
      "Epoch 545/1000\n",
      "23/23 [==============================] - 0s 779us/step - loss: 0.1556 - accuracy: 0.7795\n",
      "Epoch 546/1000\n",
      "23/23 [==============================] - 0s 824us/step - loss: 0.1556 - accuracy: 0.7795\n",
      "Epoch 547/1000\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.1555 - accuracy: 0.7795\n",
      "Epoch 548/1000\n",
      "23/23 [==============================] - 0s 808us/step - loss: 0.1555 - accuracy: 0.7795\n",
      "Epoch 549/1000\n",
      "23/23 [==============================] - 0s 804us/step - loss: 0.1555 - accuracy: 0.7795\n",
      "Epoch 550/1000\n",
      "23/23 [==============================] - 0s 812us/step - loss: 0.1555 - accuracy: 0.7795\n",
      "Epoch 551/1000\n",
      "23/23 [==============================] - 0s 817us/step - loss: 0.1555 - accuracy: 0.7795\n",
      "Epoch 552/1000\n",
      "23/23 [==============================] - 0s 800us/step - loss: 0.1555 - accuracy: 0.7795\n",
      "Epoch 553/1000\n",
      "23/23 [==============================] - 0s 823us/step - loss: 0.1554 - accuracy: 0.7795\n",
      "Epoch 554/1000\n",
      "23/23 [==============================] - 0s 769us/step - loss: 0.1554 - accuracy: 0.7795\n",
      "Epoch 555/1000\n",
      "23/23 [==============================] - 0s 819us/step - loss: 0.1554 - accuracy: 0.7795\n",
      "Epoch 556/1000\n",
      "23/23 [==============================] - 0s 835us/step - loss: 0.1554 - accuracy: 0.7795\n",
      "Epoch 557/1000\n",
      "23/23 [==============================] - 0s 809us/step - loss: 0.1554 - accuracy: 0.7795\n",
      "Epoch 558/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1554 - accuracy: 0.7795\n",
      "Epoch 559/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1553 - accuracy: 0.7795\n",
      "Epoch 560/1000\n",
      "23/23 [==============================] - 0s 990us/step - loss: 0.1553 - accuracy: 0.7795\n",
      "Epoch 561/1000\n",
      "23/23 [==============================] - 0s 782us/step - loss: 0.1553 - accuracy: 0.7795\n",
      "Epoch 562/1000\n",
      "23/23 [==============================] - 0s 811us/step - loss: 0.1553 - accuracy: 0.7795\n",
      "Epoch 563/1000\n",
      "23/23 [==============================] - 0s 865us/step - loss: 0.1553 - accuracy: 0.7795\n",
      "Epoch 564/1000\n",
      "23/23 [==============================] - 0s 813us/step - loss: 0.1553 - accuracy: 0.7795\n",
      "Epoch 565/1000\n",
      "23/23 [==============================] - 0s 855us/step - loss: 0.1552 - accuracy: 0.7795\n",
      "Epoch 566/1000\n",
      "23/23 [==============================] - 0s 782us/step - loss: 0.1552 - accuracy: 0.7795\n",
      "Epoch 567/1000\n",
      "23/23 [==============================] - 0s 817us/step - loss: 0.1552 - accuracy: 0.7795\n",
      "Epoch 568/1000\n",
      "23/23 [==============================] - 0s 857us/step - loss: 0.1552 - accuracy: 0.7795\n",
      "Epoch 569/1000\n",
      "23/23 [==============================] - 0s 856us/step - loss: 0.1552 - accuracy: 0.7795\n",
      "Epoch 570/1000\n",
      "23/23 [==============================] - 0s 858us/step - loss: 0.1552 - accuracy: 0.7795\n",
      "Epoch 571/1000\n",
      "23/23 [==============================] - 0s 769us/step - loss: 0.1551 - accuracy: 0.7795\n",
      "Epoch 572/1000\n",
      "23/23 [==============================] - 0s 826us/step - loss: 0.1551 - accuracy: 0.7795\n",
      "Epoch 573/1000\n",
      "23/23 [==============================] - 0s 802us/step - loss: 0.1551 - accuracy: 0.7795\n",
      "Epoch 574/1000\n",
      "23/23 [==============================] - 0s 791us/step - loss: 0.1551 - accuracy: 0.7795\n",
      "Epoch 575/1000\n",
      "23/23 [==============================] - 0s 855us/step - loss: 0.1551 - accuracy: 0.7795\n",
      "Epoch 576/1000\n",
      "23/23 [==============================] - 0s 804us/step - loss: 0.1551 - accuracy: 0.7795\n",
      "Epoch 577/1000\n",
      "23/23 [==============================] - 0s 831us/step - loss: 0.1550 - accuracy: 0.7795\n",
      "Epoch 578/1000\n",
      "23/23 [==============================] - 0s 786us/step - loss: 0.1550 - accuracy: 0.7795\n",
      "Epoch 579/1000\n",
      "23/23 [==============================] - 0s 858us/step - loss: 0.1550 - accuracy: 0.7795\n",
      "Epoch 580/1000\n",
      "23/23 [==============================] - 0s 806us/step - loss: 0.1550 - accuracy: 0.7795\n",
      "Epoch 581/1000\n",
      "23/23 [==============================] - 0s 785us/step - loss: 0.1550 - accuracy: 0.7795\n",
      "Epoch 582/1000\n",
      "23/23 [==============================] - 0s 822us/step - loss: 0.1550 - accuracy: 0.7795\n",
      "Epoch 583/1000\n",
      "23/23 [==============================] - 0s 829us/step - loss: 0.1549 - accuracy: 0.7795\n",
      "Epoch 584/1000\n",
      "23/23 [==============================] - 0s 836us/step - loss: 0.1549 - accuracy: 0.7795\n",
      "Epoch 585/1000\n",
      "23/23 [==============================] - 0s 830us/step - loss: 0.1549 - accuracy: 0.7795\n",
      "Epoch 586/1000\n",
      "23/23 [==============================] - 0s 881us/step - loss: 0.1549 - accuracy: 0.7795\n",
      "Epoch 587/1000\n",
      "23/23 [==============================] - 0s 810us/step - loss: 0.1549 - accuracy: 0.7795\n",
      "Epoch 588/1000\n",
      "23/23 [==============================] - 0s 827us/step - loss: 0.1549 - accuracy: 0.7795\n",
      "Epoch 589/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1548 - accuracy: 0.7795\n",
      "Epoch 590/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1548 - accuracy: 0.7795\n",
      "Epoch 591/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1548 - accuracy: 0.7795\n",
      "Epoch 592/1000\n",
      "23/23 [==============================] - 0s 915us/step - loss: 0.1548 - accuracy: 0.7795\n",
      "Epoch 593/1000\n",
      "23/23 [==============================] - 0s 912us/step - loss: 0.1548 - accuracy: 0.7795\n",
      "Epoch 594/1000\n",
      "23/23 [==============================] - 0s 928us/step - loss: 0.1548 - accuracy: 0.7795\n",
      "Epoch 595/1000\n",
      "23/23 [==============================] - 0s 904us/step - loss: 0.1548 - accuracy: 0.7795\n",
      "Epoch 596/1000\n",
      "23/23 [==============================] - 0s 906us/step - loss: 0.1547 - accuracy: 0.7795\n",
      "Epoch 597/1000\n",
      "23/23 [==============================] - 0s 847us/step - loss: 0.1547 - accuracy: 0.7795\n",
      "Epoch 598/1000\n",
      "23/23 [==============================] - 0s 819us/step - loss: 0.1547 - accuracy: 0.7795\n",
      "Epoch 599/1000\n",
      "23/23 [==============================] - 0s 876us/step - loss: 0.1547 - accuracy: 0.7795\n",
      "Epoch 600/1000\n",
      "23/23 [==============================] - 0s 812us/step - loss: 0.1547 - accuracy: 0.7795\n",
      "Epoch 601/1000\n",
      "23/23 [==============================] - 0s 829us/step - loss: 0.1547 - accuracy: 0.7795\n",
      "Epoch 602/1000\n",
      "23/23 [==============================] - 0s 854us/step - loss: 0.1547 - accuracy: 0.7795\n",
      "Epoch 603/1000\n",
      "23/23 [==============================] - 0s 808us/step - loss: 0.1546 - accuracy: 0.7795\n",
      "Epoch 604/1000\n",
      "23/23 [==============================] - 0s 827us/step - loss: 0.1546 - accuracy: 0.7795\n",
      "Epoch 605/1000\n",
      "23/23 [==============================] - 0s 782us/step - loss: 0.1546 - accuracy: 0.7795\n",
      "Epoch 606/1000\n",
      "23/23 [==============================] - 0s 881us/step - loss: 0.1546 - accuracy: 0.7795\n",
      "Epoch 607/1000\n",
      "23/23 [==============================] - 0s 812us/step - loss: 0.1546 - accuracy: 0.7795\n",
      "Epoch 608/1000\n",
      "23/23 [==============================] - 0s 829us/step - loss: 0.1546 - accuracy: 0.7795\n",
      "Epoch 609/1000\n",
      "23/23 [==============================] - 0s 867us/step - loss: 0.1545 - accuracy: 0.7795\n",
      "Epoch 610/1000\n",
      "23/23 [==============================] - 0s 797us/step - loss: 0.1545 - accuracy: 0.7795\n",
      "Epoch 611/1000\n",
      "23/23 [==============================] - 0s 862us/step - loss: 0.1545 - accuracy: 0.7795\n",
      "Epoch 612/1000\n",
      "23/23 [==============================] - 0s 866us/step - loss: 0.1545 - accuracy: 0.7795\n",
      "Epoch 613/1000\n",
      "23/23 [==============================] - 0s 793us/step - loss: 0.1545 - accuracy: 0.7795\n",
      "Epoch 614/1000\n",
      "23/23 [==============================] - 0s 832us/step - loss: 0.1545 - accuracy: 0.7795\n",
      "Epoch 615/1000\n",
      "23/23 [==============================] - 0s 761us/step - loss: 0.1545 - accuracy: 0.7795\n",
      "Epoch 616/1000\n",
      "23/23 [==============================] - 0s 821us/step - loss: 0.1544 - accuracy: 0.7795\n",
      "Epoch 617/1000\n",
      "23/23 [==============================] - 0s 827us/step - loss: 0.1544 - accuracy: 0.7795\n",
      "Epoch 618/1000\n",
      "23/23 [==============================] - 0s 783us/step - loss: 0.1544 - accuracy: 0.7795\n",
      "Epoch 619/1000\n",
      "23/23 [==============================] - 0s 785us/step - loss: 0.1544 - accuracy: 0.7795\n",
      "Epoch 620/1000\n",
      "23/23 [==============================] - 0s 817us/step - loss: 0.1544 - accuracy: 0.7795\n",
      "Epoch 621/1000\n",
      "23/23 [==============================] - 0s 849us/step - loss: 0.1544 - accuracy: 0.7795\n",
      "Epoch 622/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1544 - accuracy: 0.7795\n",
      "Epoch 623/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1543 - accuracy: 0.7795\n",
      "Epoch 624/1000\n",
      "23/23 [==============================] - 0s 845us/step - loss: 0.1543 - accuracy: 0.7795\n",
      "Epoch 625/1000\n",
      "23/23 [==============================] - 0s 748us/step - loss: 0.1543 - accuracy: 0.7795\n",
      "Epoch 626/1000\n",
      "23/23 [==============================] - 0s 842us/step - loss: 0.1543 - accuracy: 0.7795\n",
      "Epoch 627/1000\n",
      "23/23 [==============================] - 0s 856us/step - loss: 0.1543 - accuracy: 0.7795\n",
      "Epoch 628/1000\n",
      "23/23 [==============================] - 0s 850us/step - loss: 0.1543 - accuracy: 0.7795\n",
      "Epoch 629/1000\n",
      "23/23 [==============================] - 0s 813us/step - loss: 0.1543 - accuracy: 0.7795\n",
      "Epoch 630/1000\n",
      "23/23 [==============================] - 0s 810us/step - loss: 0.1542 - accuracy: 0.7795\n",
      "Epoch 631/1000\n",
      "23/23 [==============================] - 0s 854us/step - loss: 0.1542 - accuracy: 0.7795\n",
      "Epoch 632/1000\n",
      "23/23 [==============================] - 0s 775us/step - loss: 0.1542 - accuracy: 0.7795\n",
      "Epoch 633/1000\n",
      "23/23 [==============================] - 0s 785us/step - loss: 0.1542 - accuracy: 0.7795\n",
      "Epoch 634/1000\n",
      "23/23 [==============================] - 0s 825us/step - loss: 0.1542 - accuracy: 0.7795\n",
      "Epoch 635/1000\n",
      "23/23 [==============================] - 0s 782us/step - loss: 0.1542 - accuracy: 0.7795\n",
      "Epoch 636/1000\n",
      "23/23 [==============================] - 0s 842us/step - loss: 0.1542 - accuracy: 0.7795\n",
      "Epoch 637/1000\n",
      "23/23 [==============================] - 0s 830us/step - loss: 0.1541 - accuracy: 0.7795\n",
      "Epoch 638/1000\n",
      "23/23 [==============================] - 0s 838us/step - loss: 0.1541 - accuracy: 0.7795\n",
      "Epoch 639/1000\n",
      "23/23 [==============================] - 0s 824us/step - loss: 0.1541 - accuracy: 0.7795\n",
      "Epoch 640/1000\n",
      "23/23 [==============================] - 0s 786us/step - loss: 0.1541 - accuracy: 0.7795\n",
      "Epoch 641/1000\n",
      "23/23 [==============================] - 0s 752us/step - loss: 0.1541 - accuracy: 0.7795\n",
      "Epoch 642/1000\n",
      "23/23 [==============================] - 0s 777us/step - loss: 0.1541 - accuracy: 0.7795\n",
      "Epoch 643/1000\n",
      "23/23 [==============================] - 0s 845us/step - loss: 0.1541 - accuracy: 0.7795\n",
      "Epoch 644/1000\n",
      "23/23 [==============================] - 0s 842us/step - loss: 0.1540 - accuracy: 0.7795\n",
      "Epoch 645/1000\n",
      "23/23 [==============================] - 0s 842us/step - loss: 0.1540 - accuracy: 0.7795\n",
      "Epoch 646/1000\n",
      "23/23 [==============================] - 0s 844us/step - loss: 0.1540 - accuracy: 0.7795\n",
      "Epoch 647/1000\n",
      "23/23 [==============================] - 0s 795us/step - loss: 0.1540 - accuracy: 0.7795\n",
      "Epoch 648/1000\n",
      "23/23 [==============================] - 0s 848us/step - loss: 0.1540 - accuracy: 0.7795\n",
      "Epoch 649/1000\n",
      "23/23 [==============================] - 0s 754us/step - loss: 0.1540 - accuracy: 0.7795\n",
      "Epoch 650/1000\n",
      "23/23 [==============================] - 0s 914us/step - loss: 0.1540 - accuracy: 0.7795\n",
      "Epoch 651/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1540 - accuracy: 0.7795\n",
      "Epoch 652/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1539 - accuracy: 0.7795\n",
      "Epoch 653/1000\n",
      "23/23 [==============================] - 0s 858us/step - loss: 0.1539 - accuracy: 0.7795\n",
      "Epoch 654/1000\n",
      "23/23 [==============================] - 0s 937us/step - loss: 0.1539 - accuracy: 0.7795\n",
      "Epoch 655/1000\n",
      "23/23 [==============================] - 0s 894us/step - loss: 0.1539 - accuracy: 0.7795\n",
      "Epoch 656/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1539 - accuracy: 0.7795\n",
      "Epoch 657/1000\n",
      "23/23 [==============================] - 0s 812us/step - loss: 0.1539 - accuracy: 0.7795\n",
      "Epoch 658/1000\n",
      "23/23 [==============================] - 0s 799us/step - loss: 0.1539 - accuracy: 0.7795\n",
      "Epoch 659/1000\n",
      "23/23 [==============================] - 0s 844us/step - loss: 0.1539 - accuracy: 0.7795\n",
      "Epoch 660/1000\n",
      "23/23 [==============================] - 0s 853us/step - loss: 0.1538 - accuracy: 0.7795\n",
      "Epoch 661/1000\n",
      "23/23 [==============================] - 0s 934us/step - loss: 0.1538 - accuracy: 0.7795\n",
      "Epoch 662/1000\n",
      "23/23 [==============================] - 0s 930us/step - loss: 0.1538 - accuracy: 0.7795\n",
      "Epoch 663/1000\n",
      "23/23 [==============================] - 0s 863us/step - loss: 0.1538 - accuracy: 0.7795\n",
      "Epoch 664/1000\n",
      "23/23 [==============================] - 0s 805us/step - loss: 0.1538 - accuracy: 0.7795\n",
      "Epoch 665/1000\n",
      "23/23 [==============================] - 0s 806us/step - loss: 0.1538 - accuracy: 0.7795\n",
      "Epoch 666/1000\n",
      "23/23 [==============================] - 0s 840us/step - loss: 0.1538 - accuracy: 0.7795\n",
      "Epoch 667/1000\n",
      "23/23 [==============================] - 0s 856us/step - loss: 0.1537 - accuracy: 0.7795\n",
      "Epoch 668/1000\n",
      "23/23 [==============================] - 0s 809us/step - loss: 0.1537 - accuracy: 0.7795\n",
      "Epoch 669/1000\n",
      "23/23 [==============================] - 0s 852us/step - loss: 0.1537 - accuracy: 0.7795\n",
      "Epoch 670/1000\n",
      "23/23 [==============================] - 0s 812us/step - loss: 0.1537 - accuracy: 0.7795\n",
      "Epoch 671/1000\n",
      "23/23 [==============================] - 0s 843us/step - loss: 0.1537 - accuracy: 0.7795\n",
      "Epoch 672/1000\n",
      "23/23 [==============================] - 0s 810us/step - loss: 0.1537 - accuracy: 0.7795\n",
      "Epoch 673/1000\n",
      "23/23 [==============================] - 0s 834us/step - loss: 0.1537 - accuracy: 0.7795\n",
      "Epoch 674/1000\n",
      "23/23 [==============================] - 0s 881us/step - loss: 0.1537 - accuracy: 0.7795\n",
      "Epoch 675/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1536 - accuracy: 0.7795\n",
      "Epoch 676/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1536 - accuracy: 0.7795\n",
      "Epoch 677/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1536 - accuracy: 0.7795\n",
      "Epoch 678/1000\n",
      "23/23 [==============================] - 0s 917us/step - loss: 0.1536 - accuracy: 0.7795\n",
      "Epoch 679/1000\n",
      "23/23 [==============================] - 0s 852us/step - loss: 0.1536 - accuracy: 0.7795\n",
      "Epoch 680/1000\n",
      "23/23 [==============================] - 0s 783us/step - loss: 0.1536 - accuracy: 0.7795\n",
      "Epoch 681/1000\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.1536 - accuracy: 0.7795\n",
      "Epoch 682/1000\n",
      "23/23 [==============================] - 0s 825us/step - loss: 0.1536 - accuracy: 0.7795\n",
      "Epoch 683/1000\n",
      "23/23 [==============================] - 0s 775us/step - loss: 0.1536 - accuracy: 0.7795\n",
      "Epoch 684/1000\n",
      "23/23 [==============================] - 0s 880us/step - loss: 0.1535 - accuracy: 0.7795\n",
      "Epoch 685/1000\n",
      "23/23 [==============================] - 0s 814us/step - loss: 0.1535 - accuracy: 0.7795\n",
      "Epoch 686/1000\n",
      "23/23 [==============================] - 0s 791us/step - loss: 0.1535 - accuracy: 0.7795\n",
      "Epoch 687/1000\n",
      "23/23 [==============================] - 0s 856us/step - loss: 0.1535 - accuracy: 0.7795\n",
      "Epoch 688/1000\n",
      "23/23 [==============================] - 0s 879us/step - loss: 0.1535 - accuracy: 0.7795\n",
      "Epoch 689/1000\n",
      "23/23 [==============================] - 0s 930us/step - loss: 0.1535 - accuracy: 0.7795\n",
      "Epoch 690/1000\n",
      "23/23 [==============================] - 0s 960us/step - loss: 0.1535 - accuracy: 0.7795\n",
      "Epoch 691/1000\n",
      "23/23 [==============================] - 0s 914us/step - loss: 0.1535 - accuracy: 0.7795\n",
      "Epoch 692/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1534 - accuracy: 0.7795\n",
      "Epoch 693/1000\n",
      "23/23 [==============================] - 0s 966us/step - loss: 0.1534 - accuracy: 0.7795\n",
      "Epoch 694/1000\n",
      "23/23 [==============================] - 0s 865us/step - loss: 0.1534 - accuracy: 0.7795\n",
      "Epoch 695/1000\n",
      "23/23 [==============================] - 0s 910us/step - loss: 0.1534 - accuracy: 0.7795\n",
      "Epoch 696/1000\n",
      "23/23 [==============================] - 0s 832us/step - loss: 0.1534 - accuracy: 0.7795\n",
      "Epoch 697/1000\n",
      "23/23 [==============================] - 0s 890us/step - loss: 0.1534 - accuracy: 0.7795\n",
      "Epoch 698/1000\n",
      "23/23 [==============================] - 0s 790us/step - loss: 0.1534 - accuracy: 0.7795\n",
      "Epoch 699/1000\n",
      "23/23 [==============================] - 0s 853us/step - loss: 0.1534 - accuracy: 0.7795\n",
      "Epoch 700/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1533 - accuracy: 0.7795\n",
      "Epoch 701/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1533 - accuracy: 0.7795\n",
      "Epoch 702/1000\n",
      "23/23 [==============================] - 0s 887us/step - loss: 0.1533 - accuracy: 0.7795\n",
      "Epoch 703/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1533 - accuracy: 0.7795\n",
      "Epoch 704/1000\n",
      "23/23 [==============================] - 0s 935us/step - loss: 0.1533 - accuracy: 0.7795\n",
      "Epoch 705/1000\n",
      "23/23 [==============================] - 0s 891us/step - loss: 0.1533 - accuracy: 0.7795\n",
      "Epoch 706/1000\n",
      "23/23 [==============================] - 0s 820us/step - loss: 0.1533 - accuracy: 0.7795\n",
      "Epoch 707/1000\n",
      "23/23 [==============================] - 0s 802us/step - loss: 0.1533 - accuracy: 0.7795\n",
      "Epoch 708/1000\n",
      "23/23 [==============================] - 0s 857us/step - loss: 0.1533 - accuracy: 0.7795\n",
      "Epoch 709/1000\n",
      "23/23 [==============================] - 0s 792us/step - loss: 0.1532 - accuracy: 0.7795\n",
      "Epoch 710/1000\n",
      "23/23 [==============================] - 0s 810us/step - loss: 0.1532 - accuracy: 0.7809\n",
      "Epoch 711/1000\n",
      "23/23 [==============================] - 0s 858us/step - loss: 0.1532 - accuracy: 0.7809\n",
      "Epoch 712/1000\n",
      "23/23 [==============================] - 0s 795us/step - loss: 0.1532 - accuracy: 0.7809\n",
      "Epoch 713/1000\n",
      "23/23 [==============================] - 0s 878us/step - loss: 0.1532 - accuracy: 0.7809\n",
      "Epoch 714/1000\n",
      "23/23 [==============================] - 0s 814us/step - loss: 0.1532 - accuracy: 0.7809\n",
      "Epoch 715/1000\n",
      "23/23 [==============================] - 0s 862us/step - loss: 0.1532 - accuracy: 0.7809\n",
      "Epoch 716/1000\n",
      "23/23 [==============================] - 0s 875us/step - loss: 0.1532 - accuracy: 0.7809\n",
      "Epoch 717/1000\n",
      "23/23 [==============================] - 0s 837us/step - loss: 0.1532 - accuracy: 0.7809\n",
      "Epoch 718/1000\n",
      "23/23 [==============================] - 0s 811us/step - loss: 0.1532 - accuracy: 0.7809\n",
      "Epoch 719/1000\n",
      "23/23 [==============================] - 0s 820us/step - loss: 0.1531 - accuracy: 0.7809\n",
      "Epoch 720/1000\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.1531 - accuracy: 0.7809\n",
      "Epoch 721/1000\n",
      "23/23 [==============================] - 0s 875us/step - loss: 0.1531 - accuracy: 0.7809\n",
      "Epoch 722/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1531 - accuracy: 0.7809\n",
      "Epoch 723/1000\n",
      "23/23 [==============================] - 0s 992us/step - loss: 0.1531 - accuracy: 0.7809\n",
      "Epoch 724/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1531 - accuracy: 0.7809\n",
      "Epoch 725/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1531 - accuracy: 0.7809\n",
      "Epoch 726/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1531 - accuracy: 0.7809\n",
      "Epoch 727/1000\n",
      "23/23 [==============================] - 0s 998us/step - loss: 0.1530 - accuracy: 0.7809\n",
      "Epoch 728/1000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1530 - accuracy: 0.7823\n",
      "Epoch 729/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1530 - accuracy: 0.7823\n",
      "Epoch 730/1000\n",
      "23/23 [==============================] - 0s 901us/step - loss: 0.1530 - accuracy: 0.7823\n",
      "Epoch 731/1000\n",
      "23/23 [==============================] - 0s 846us/step - loss: 0.1530 - accuracy: 0.7823\n",
      "Epoch 732/1000\n",
      "23/23 [==============================] - 0s 860us/step - loss: 0.1530 - accuracy: 0.7823\n",
      "Epoch 733/1000\n",
      "23/23 [==============================] - 0s 919us/step - loss: 0.1530 - accuracy: 0.7823\n",
      "Epoch 734/1000\n",
      "23/23 [==============================] - 0s 994us/step - loss: 0.1530 - accuracy: 0.7823\n",
      "Epoch 735/1000\n",
      "23/23 [==============================] - 0s 917us/step - loss: 0.1530 - accuracy: 0.7823\n",
      "Epoch 736/1000\n",
      "23/23 [==============================] - 0s 989us/step - loss: 0.1530 - accuracy: 0.7823\n",
      "Epoch 737/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1529 - accuracy: 0.7823\n",
      "Epoch 738/1000\n",
      "23/23 [==============================] - 0s 836us/step - loss: 0.1529 - accuracy: 0.7823\n",
      "Epoch 739/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1529 - accuracy: 0.7823\n",
      "Epoch 740/1000\n",
      "23/23 [==============================] - 0s 979us/step - loss: 0.1529 - accuracy: 0.7823\n",
      "Epoch 741/1000\n",
      "23/23 [==============================] - 0s 879us/step - loss: 0.1529 - accuracy: 0.7823\n",
      "Epoch 742/1000\n",
      "23/23 [==============================] - 0s 871us/step - loss: 0.1529 - accuracy: 0.7823\n",
      "Epoch 743/1000\n",
      "23/23 [==============================] - 0s 869us/step - loss: 0.1529 - accuracy: 0.7823\n",
      "Epoch 744/1000\n",
      "23/23 [==============================] - 0s 906us/step - loss: 0.1529 - accuracy: 0.7823\n",
      "Epoch 745/1000\n",
      "23/23 [==============================] - 0s 831us/step - loss: 0.1529 - accuracy: 0.7823\n",
      "Epoch 746/1000\n",
      "23/23 [==============================] - 0s 831us/step - loss: 0.1528 - accuracy: 0.7823\n",
      "Epoch 747/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1528 - accuracy: 0.7823\n",
      "Epoch 748/1000\n",
      "23/23 [==============================] - 0s 866us/step - loss: 0.1528 - accuracy: 0.7823\n",
      "Epoch 749/1000\n",
      "23/23 [==============================] - 0s 968us/step - loss: 0.1528 - accuracy: 0.7823\n",
      "Epoch 750/1000\n",
      "23/23 [==============================] - 0s 932us/step - loss: 0.1528 - accuracy: 0.7823\n",
      "Epoch 751/1000\n",
      "23/23 [==============================] - 0s 934us/step - loss: 0.1528 - accuracy: 0.7823\n",
      "Epoch 752/1000\n",
      "23/23 [==============================] - 0s 924us/step - loss: 0.1528 - accuracy: 0.7823\n",
      "Epoch 753/1000\n",
      "23/23 [==============================] - 0s 978us/step - loss: 0.1528 - accuracy: 0.7823\n",
      "Epoch 754/1000\n",
      "23/23 [==============================] - 0s 943us/step - loss: 0.1528 - accuracy: 0.7823\n",
      "Epoch 755/1000\n",
      "23/23 [==============================] - 0s 912us/step - loss: 0.1528 - accuracy: 0.7823\n",
      "Epoch 756/1000\n",
      "23/23 [==============================] - 0s 870us/step - loss: 0.1527 - accuracy: 0.7823\n",
      "Epoch 757/1000\n",
      "23/23 [==============================] - 0s 970us/step - loss: 0.1527 - accuracy: 0.7823\n",
      "Epoch 758/1000\n",
      "23/23 [==============================] - 0s 873us/step - loss: 0.1527 - accuracy: 0.7823\n",
      "Epoch 759/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1527 - accuracy: 0.7823\n",
      "Epoch 760/1000\n",
      "23/23 [==============================] - 0s 991us/step - loss: 0.1527 - accuracy: 0.7823\n",
      "Epoch 761/1000\n",
      "23/23 [==============================] - 0s 957us/step - loss: 0.1527 - accuracy: 0.7823\n",
      "Epoch 762/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1527 - accuracy: 0.7823\n",
      "Epoch 763/1000\n",
      "23/23 [==============================] - 0s 849us/step - loss: 0.1527 - accuracy: 0.7823\n",
      "Epoch 764/1000\n",
      "23/23 [==============================] - 0s 925us/step - loss: 0.1527 - accuracy: 0.7823\n",
      "Epoch 765/1000\n",
      "23/23 [==============================] - 0s 878us/step - loss: 0.1527 - accuracy: 0.7823\n",
      "Epoch 766/1000\n",
      "23/23 [==============================] - 0s 930us/step - loss: 0.1526 - accuracy: 0.7823\n",
      "Epoch 767/1000\n",
      "23/23 [==============================] - 0s 787us/step - loss: 0.1526 - accuracy: 0.7823\n",
      "Epoch 768/1000\n",
      "23/23 [==============================] - 0s 860us/step - loss: 0.1526 - accuracy: 0.7823\n",
      "Epoch 769/1000\n",
      "23/23 [==============================] - 0s 849us/step - loss: 0.1526 - accuracy: 0.7823\n",
      "Epoch 770/1000\n",
      "23/23 [==============================] - 0s 930us/step - loss: 0.1526 - accuracy: 0.7823\n",
      "Epoch 771/1000\n",
      "23/23 [==============================] - 0s 886us/step - loss: 0.1526 - accuracy: 0.7823\n",
      "Epoch 772/1000\n",
      "23/23 [==============================] - 0s 849us/step - loss: 0.1526 - accuracy: 0.7823\n",
      "Epoch 773/1000\n",
      "23/23 [==============================] - 0s 845us/step - loss: 0.1526 - accuracy: 0.7823\n",
      "Epoch 774/1000\n",
      "23/23 [==============================] - 0s 866us/step - loss: 0.1526 - accuracy: 0.7823\n",
      "Epoch 775/1000\n",
      "23/23 [==============================] - 0s 918us/step - loss: 0.1526 - accuracy: 0.7823\n",
      "Epoch 776/1000\n",
      "23/23 [==============================] - 0s 856us/step - loss: 0.1525 - accuracy: 0.7823\n",
      "Epoch 777/1000\n",
      "23/23 [==============================] - 0s 849us/step - loss: 0.1525 - accuracy: 0.7823\n",
      "Epoch 778/1000\n",
      "23/23 [==============================] - 0s 881us/step - loss: 0.1525 - accuracy: 0.7823\n",
      "Epoch 779/1000\n",
      "23/23 [==============================] - 0s 844us/step - loss: 0.1525 - accuracy: 0.7823\n",
      "Epoch 780/1000\n",
      "23/23 [==============================] - 0s 904us/step - loss: 0.1525 - accuracy: 0.7823\n",
      "Epoch 781/1000\n",
      "23/23 [==============================] - 0s 872us/step - loss: 0.1525 - accuracy: 0.7823\n",
      "Epoch 782/1000\n",
      "23/23 [==============================] - 0s 820us/step - loss: 0.1525 - accuracy: 0.7823\n",
      "Epoch 783/1000\n",
      "23/23 [==============================] - 0s 893us/step - loss: 0.1525 - accuracy: 0.7823\n",
      "Epoch 784/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1525 - accuracy: 0.7823\n",
      "Epoch 785/1000\n",
      "23/23 [==============================] - 0s 871us/step - loss: 0.1525 - accuracy: 0.7823\n",
      "Epoch 786/1000\n",
      "23/23 [==============================] - 0s 927us/step - loss: 0.1524 - accuracy: 0.7823\n",
      "Epoch 787/1000\n",
      "23/23 [==============================] - 0s 894us/step - loss: 0.1524 - accuracy: 0.7823\n",
      "Epoch 788/1000\n",
      "23/23 [==============================] - 0s 921us/step - loss: 0.1524 - accuracy: 0.7823\n",
      "Epoch 789/1000\n",
      "23/23 [==============================] - 0s 930us/step - loss: 0.1524 - accuracy: 0.7823\n",
      "Epoch 790/1000\n",
      "23/23 [==============================] - 0s 828us/step - loss: 0.1524 - accuracy: 0.7823\n",
      "Epoch 791/1000\n",
      "23/23 [==============================] - 0s 847us/step - loss: 0.1524 - accuracy: 0.7823\n",
      "Epoch 792/1000\n",
      "23/23 [==============================] - 0s 856us/step - loss: 0.1524 - accuracy: 0.7823\n",
      "Epoch 793/1000\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.1524 - accuracy: 0.7823\n",
      "Epoch 794/1000\n",
      "23/23 [==============================] - 0s 893us/step - loss: 0.1524 - accuracy: 0.7823\n",
      "Epoch 795/1000\n",
      "23/23 [==============================] - 0s 840us/step - loss: 0.1524 - accuracy: 0.7823\n",
      "Epoch 796/1000\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.1523 - accuracy: 0.7823\n",
      "Epoch 797/1000\n",
      "23/23 [==============================] - 0s 854us/step - loss: 0.1523 - accuracy: 0.7823\n",
      "Epoch 798/1000\n",
      "23/23 [==============================] - 0s 834us/step - loss: 0.1523 - accuracy: 0.7823\n",
      "Epoch 799/1000\n",
      "23/23 [==============================] - 0s 825us/step - loss: 0.1523 - accuracy: 0.7823\n",
      "Epoch 800/1000\n",
      "23/23 [==============================] - 0s 855us/step - loss: 0.1523 - accuracy: 0.7823\n",
      "Epoch 801/1000\n",
      "23/23 [==============================] - 0s 913us/step - loss: 0.1523 - accuracy: 0.7823\n",
      "Epoch 802/1000\n",
      "23/23 [==============================] - 0s 911us/step - loss: 0.1523 - accuracy: 0.7823\n",
      "Epoch 803/1000\n",
      "23/23 [==============================] - 0s 984us/step - loss: 0.1523 - accuracy: 0.7823\n",
      "Epoch 804/1000\n",
      "23/23 [==============================] - 0s 811us/step - loss: 0.1523 - accuracy: 0.7823\n",
      "Epoch 805/1000\n",
      "23/23 [==============================] - 0s 897us/step - loss: 0.1523 - accuracy: 0.7823\n",
      "Epoch 806/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1523 - accuracy: 0.7823\n",
      "Epoch 807/1000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1522 - accuracy: 0.7823\n",
      "Epoch 808/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1522 - accuracy: 0.7823\n",
      "Epoch 809/1000\n",
      "23/23 [==============================] - 0s 847us/step - loss: 0.1522 - accuracy: 0.7823\n",
      "Epoch 810/1000\n",
      "23/23 [==============================] - 0s 833us/step - loss: 0.1522 - accuracy: 0.7823\n",
      "Epoch 811/1000\n",
      "23/23 [==============================] - 0s 838us/step - loss: 0.1522 - accuracy: 0.7823\n",
      "Epoch 812/1000\n",
      "23/23 [==============================] - 0s 862us/step - loss: 0.1522 - accuracy: 0.7823\n",
      "Epoch 813/1000\n",
      "23/23 [==============================] - 0s 876us/step - loss: 0.1522 - accuracy: 0.7823\n",
      "Epoch 814/1000\n",
      "23/23 [==============================] - 0s 866us/step - loss: 0.1522 - accuracy: 0.7823\n",
      "Epoch 815/1000\n",
      "23/23 [==============================] - 0s 854us/step - loss: 0.1522 - accuracy: 0.7823\n",
      "Epoch 816/1000\n",
      "23/23 [==============================] - 0s 889us/step - loss: 0.1522 - accuracy: 0.7823\n",
      "Epoch 817/1000\n",
      "23/23 [==============================] - 0s 785us/step - loss: 0.1522 - accuracy: 0.7823\n",
      "Epoch 818/1000\n",
      "23/23 [==============================] - 0s 865us/step - loss: 0.1521 - accuracy: 0.7823\n",
      "Epoch 819/1000\n",
      "23/23 [==============================] - 0s 835us/step - loss: 0.1521 - accuracy: 0.7823\n",
      "Epoch 820/1000\n",
      "23/23 [==============================] - 0s 821us/step - loss: 0.1521 - accuracy: 0.7823\n",
      "Epoch 821/1000\n",
      "23/23 [==============================] - 0s 825us/step - loss: 0.1521 - accuracy: 0.7823\n",
      "Epoch 822/1000\n",
      "23/23 [==============================] - 0s 747us/step - loss: 0.1521 - accuracy: 0.7823\n",
      "Epoch 823/1000\n",
      "23/23 [==============================] - 0s 848us/step - loss: 0.1521 - accuracy: 0.7823\n",
      "Epoch 824/1000\n",
      "23/23 [==============================] - 0s 810us/step - loss: 0.1521 - accuracy: 0.7823\n",
      "Epoch 825/1000\n",
      "23/23 [==============================] - 0s 888us/step - loss: 0.1521 - accuracy: 0.7823\n",
      "Epoch 826/1000\n",
      "23/23 [==============================] - 0s 922us/step - loss: 0.1521 - accuracy: 0.7823\n",
      "Epoch 827/1000\n",
      "23/23 [==============================] - 0s 847us/step - loss: 0.1521 - accuracy: 0.7823\n",
      "Epoch 828/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1521 - accuracy: 0.7823\n",
      "Epoch 829/1000\n",
      "23/23 [==============================] - 0s 975us/step - loss: 0.1521 - accuracy: 0.7823\n",
      "Epoch 830/1000\n",
      "23/23 [==============================] - 0s 809us/step - loss: 0.1520 - accuracy: 0.7823\n",
      "Epoch 831/1000\n",
      "23/23 [==============================] - 0s 863us/step - loss: 0.1520 - accuracy: 0.7823\n",
      "Epoch 832/1000\n",
      "23/23 [==============================] - 0s 924us/step - loss: 0.1520 - accuracy: 0.7823\n",
      "Epoch 833/1000\n",
      "23/23 [==============================] - 0s 794us/step - loss: 0.1520 - accuracy: 0.7823\n",
      "Epoch 834/1000\n",
      "23/23 [==============================] - 0s 849us/step - loss: 0.1520 - accuracy: 0.7823\n",
      "Epoch 835/1000\n",
      "23/23 [==============================] - 0s 863us/step - loss: 0.1520 - accuracy: 0.7823\n",
      "Epoch 836/1000\n",
      "23/23 [==============================] - 0s 799us/step - loss: 0.1520 - accuracy: 0.7823\n",
      "Epoch 837/1000\n",
      "23/23 [==============================] - 0s 895us/step - loss: 0.1520 - accuracy: 0.7823\n",
      "Epoch 838/1000\n",
      "23/23 [==============================] - 0s 795us/step - loss: 0.1520 - accuracy: 0.7823\n",
      "Epoch 839/1000\n",
      "23/23 [==============================] - 0s 859us/step - loss: 0.1520 - accuracy: 0.7823\n",
      "Epoch 840/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1520 - accuracy: 0.7823\n",
      "Epoch 841/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1519 - accuracy: 0.7823\n",
      "Epoch 842/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1519 - accuracy: 0.7823\n",
      "Epoch 843/1000\n",
      "23/23 [==============================] - 0s 839us/step - loss: 0.1519 - accuracy: 0.7823\n",
      "Epoch 844/1000\n",
      "23/23 [==============================] - 0s 795us/step - loss: 0.1519 - accuracy: 0.7823\n",
      "Epoch 845/1000\n",
      "23/23 [==============================] - 0s 840us/step - loss: 0.1519 - accuracy: 0.7823\n",
      "Epoch 846/1000\n",
      "23/23 [==============================] - 0s 822us/step - loss: 0.1519 - accuracy: 0.7823\n",
      "Epoch 847/1000\n",
      "23/23 [==============================] - 0s 868us/step - loss: 0.1519 - accuracy: 0.7823\n",
      "Epoch 848/1000\n",
      "23/23 [==============================] - 0s 869us/step - loss: 0.1519 - accuracy: 0.7823\n",
      "Epoch 849/1000\n",
      "23/23 [==============================] - 0s 853us/step - loss: 0.1519 - accuracy: 0.7823\n",
      "Epoch 850/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1519 - accuracy: 0.7823\n",
      "Epoch 851/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1519 - accuracy: 0.7823\n",
      "Epoch 852/1000\n",
      "23/23 [==============================] - 0s 841us/step - loss: 0.1518 - accuracy: 0.7823\n",
      "Epoch 853/1000\n",
      "23/23 [==============================] - 0s 794us/step - loss: 0.1518 - accuracy: 0.7823\n",
      "Epoch 854/1000\n",
      "23/23 [==============================] - 0s 830us/step - loss: 0.1518 - accuracy: 0.7823\n",
      "Epoch 855/1000\n",
      "23/23 [==============================] - 0s 815us/step - loss: 0.1518 - accuracy: 0.7823\n",
      "Epoch 856/1000\n",
      "23/23 [==============================] - 0s 816us/step - loss: 0.1518 - accuracy: 0.7823\n",
      "Epoch 857/1000\n",
      "23/23 [==============================] - 0s 877us/step - loss: 0.1518 - accuracy: 0.7823\n",
      "Epoch 858/1000\n",
      "23/23 [==============================] - 0s 862us/step - loss: 0.1518 - accuracy: 0.7823\n",
      "Epoch 859/1000\n",
      "23/23 [==============================] - 0s 838us/step - loss: 0.1518 - accuracy: 0.7823\n",
      "Epoch 860/1000\n",
      "23/23 [==============================] - 0s 920us/step - loss: 0.1518 - accuracy: 0.7823\n",
      "Epoch 861/1000\n",
      "23/23 [==============================] - 0s 789us/step - loss: 0.1518 - accuracy: 0.7823\n",
      "Epoch 862/1000\n",
      "23/23 [==============================] - 0s 861us/step - loss: 0.1518 - accuracy: 0.7823\n",
      "Epoch 863/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1518 - accuracy: 0.7823\n",
      "Epoch 864/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1517 - accuracy: 0.7823\n",
      "Epoch 865/1000\n",
      "23/23 [==============================] - 0s 933us/step - loss: 0.1517 - accuracy: 0.7823\n",
      "Epoch 866/1000\n",
      "23/23 [==============================] - 0s 879us/step - loss: 0.1517 - accuracy: 0.7823\n",
      "Epoch 867/1000\n",
      "23/23 [==============================] - 0s 891us/step - loss: 0.1517 - accuracy: 0.7823\n",
      "Epoch 868/1000\n",
      "23/23 [==============================] - 0s 856us/step - loss: 0.1517 - accuracy: 0.7823\n",
      "Epoch 869/1000\n",
      "23/23 [==============================] - 0s 906us/step - loss: 0.1517 - accuracy: 0.7823\n",
      "Epoch 870/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1517 - accuracy: 0.7823\n",
      "Epoch 871/1000\n",
      "23/23 [==============================] - 0s 928us/step - loss: 0.1517 - accuracy: 0.7823\n",
      "Epoch 872/1000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1517 - accuracy: 0.7823\n",
      "Epoch 873/1000\n",
      "23/23 [==============================] - 0s 982us/step - loss: 0.1517 - accuracy: 0.7823\n",
      "Epoch 874/1000\n",
      "23/23 [==============================] - 0s 921us/step - loss: 0.1517 - accuracy: 0.7823\n",
      "Epoch 875/1000\n",
      "23/23 [==============================] - 0s 903us/step - loss: 0.1517 - accuracy: 0.7823\n",
      "Epoch 876/1000\n",
      "23/23 [==============================] - 0s 806us/step - loss: 0.1516 - accuracy: 0.7823\n",
      "Epoch 877/1000\n",
      "23/23 [==============================] - 0s 873us/step - loss: 0.1516 - accuracy: 0.7823\n",
      "Epoch 878/1000\n",
      "23/23 [==============================] - 0s 830us/step - loss: 0.1516 - accuracy: 0.7823\n",
      "Epoch 879/1000\n",
      "23/23 [==============================] - 0s 971us/step - loss: 0.1516 - accuracy: 0.7823\n",
      "Epoch 880/1000\n",
      "23/23 [==============================] - 0s 893us/step - loss: 0.1516 - accuracy: 0.7823\n",
      "Epoch 881/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1516 - accuracy: 0.7823\n",
      "Epoch 882/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1516 - accuracy: 0.7823\n",
      "Epoch 883/1000\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.1516 - accuracy: 0.7823\n",
      "Epoch 884/1000\n",
      "23/23 [==============================] - 0s 835us/step - loss: 0.1516 - accuracy: 0.7823\n",
      "Epoch 885/1000\n",
      "23/23 [==============================] - 0s 803us/step - loss: 0.1516 - accuracy: 0.7823\n",
      "Epoch 886/1000\n",
      "23/23 [==============================] - 0s 795us/step - loss: 0.1516 - accuracy: 0.7823\n",
      "Epoch 887/1000\n",
      "23/23 [==============================] - 0s 844us/step - loss: 0.1516 - accuracy: 0.7823\n",
      "Epoch 888/1000\n",
      "23/23 [==============================] - 0s 863us/step - loss: 0.1515 - accuracy: 0.7823\n",
      "Epoch 889/1000\n",
      "23/23 [==============================] - 0s 822us/step - loss: 0.1515 - accuracy: 0.7823\n",
      "Epoch 890/1000\n",
      "23/23 [==============================] - 0s 862us/step - loss: 0.1515 - accuracy: 0.7823\n",
      "Epoch 891/1000\n",
      "23/23 [==============================] - 0s 865us/step - loss: 0.1515 - accuracy: 0.7823\n",
      "Epoch 892/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1515 - accuracy: 0.7823\n",
      "Epoch 893/1000\n",
      "23/23 [==============================] - 0s 917us/step - loss: 0.1515 - accuracy: 0.7823\n",
      "Epoch 894/1000\n",
      "23/23 [==============================] - 0s 904us/step - loss: 0.1515 - accuracy: 0.7823\n",
      "Epoch 895/1000\n",
      "23/23 [==============================] - 0s 914us/step - loss: 0.1515 - accuracy: 0.7823\n",
      "Epoch 896/1000\n",
      "23/23 [==============================] - 0s 841us/step - loss: 0.1515 - accuracy: 0.7823\n",
      "Epoch 897/1000\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.1515 - accuracy: 0.7823\n",
      "Epoch 898/1000\n",
      "23/23 [==============================] - 0s 884us/step - loss: 0.1515 - accuracy: 0.7823\n",
      "Epoch 899/1000\n",
      "23/23 [==============================] - 0s 838us/step - loss: 0.1515 - accuracy: 0.7823\n",
      "Epoch 900/1000\n",
      "23/23 [==============================] - 0s 868us/step - loss: 0.1515 - accuracy: 0.7823\n",
      "Epoch 901/1000\n",
      "23/23 [==============================] - 0s 861us/step - loss: 0.1514 - accuracy: 0.7823\n",
      "Epoch 902/1000\n",
      "23/23 [==============================] - 0s 933us/step - loss: 0.1514 - accuracy: 0.7823\n",
      "Epoch 903/1000\n",
      "23/23 [==============================] - 0s 835us/step - loss: 0.1514 - accuracy: 0.7823\n",
      "Epoch 904/1000\n",
      "23/23 [==============================] - 0s 884us/step - loss: 0.1514 - accuracy: 0.7823\n",
      "Epoch 905/1000\n",
      "23/23 [==============================] - 0s 898us/step - loss: 0.1514 - accuracy: 0.7823\n",
      "Epoch 906/1000\n",
      "23/23 [==============================] - 0s 800us/step - loss: 0.1514 - accuracy: 0.7823\n",
      "Epoch 907/1000\n",
      "23/23 [==============================] - 0s 885us/step - loss: 0.1514 - accuracy: 0.7823\n",
      "Epoch 908/1000\n",
      "23/23 [==============================] - 0s 886us/step - loss: 0.1514 - accuracy: 0.7823\n",
      "Epoch 909/1000\n",
      "23/23 [==============================] - 0s 848us/step - loss: 0.1514 - accuracy: 0.7823\n",
      "Epoch 910/1000\n",
      "23/23 [==============================] - 0s 868us/step - loss: 0.1514 - accuracy: 0.7823\n",
      "Epoch 911/1000\n",
      "23/23 [==============================] - 0s 876us/step - loss: 0.1514 - accuracy: 0.7823\n",
      "Epoch 912/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1514 - accuracy: 0.7823\n",
      "Epoch 913/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1513 - accuracy: 0.7823\n",
      "Epoch 914/1000\n",
      "23/23 [==============================] - 0s 828us/step - loss: 0.1513 - accuracy: 0.7823\n",
      "Epoch 915/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1513 - accuracy: 0.7823\n",
      "Epoch 916/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1513 - accuracy: 0.7823\n",
      "Epoch 917/1000\n",
      "23/23 [==============================] - 0s 892us/step - loss: 0.1513 - accuracy: 0.7823\n",
      "Epoch 918/1000\n",
      "23/23 [==============================] - 0s 929us/step - loss: 0.1513 - accuracy: 0.7823\n",
      "Epoch 919/1000\n",
      "23/23 [==============================] - 0s 845us/step - loss: 0.1513 - accuracy: 0.7823\n",
      "Epoch 920/1000\n",
      "23/23 [==============================] - 0s 856us/step - loss: 0.1513 - accuracy: 0.7823\n",
      "Epoch 921/1000\n",
      "23/23 [==============================] - 0s 958us/step - loss: 0.1513 - accuracy: 0.7823\n",
      "Epoch 922/1000\n",
      "23/23 [==============================] - 0s 848us/step - loss: 0.1513 - accuracy: 0.7823\n",
      "Epoch 923/1000\n",
      "23/23 [==============================] - 0s 868us/step - loss: 0.1513 - accuracy: 0.7823\n",
      "Epoch 924/1000\n",
      "23/23 [==============================] - 0s 892us/step - loss: 0.1513 - accuracy: 0.7823\n",
      "Epoch 925/1000\n",
      "23/23 [==============================] - 0s 867us/step - loss: 0.1513 - accuracy: 0.7823\n",
      "Epoch 926/1000\n",
      "23/23 [==============================] - 0s 839us/step - loss: 0.1513 - accuracy: 0.7823\n",
      "Epoch 927/1000\n",
      "23/23 [==============================] - 0s 893us/step - loss: 0.1512 - accuracy: 0.7823\n",
      "Epoch 928/1000\n",
      "23/23 [==============================] - 0s 957us/step - loss: 0.1512 - accuracy: 0.7823\n",
      "Epoch 929/1000\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.1512 - accuracy: 0.7823\n",
      "Epoch 930/1000\n",
      "23/23 [==============================] - 0s 857us/step - loss: 0.1512 - accuracy: 0.7823\n",
      "Epoch 931/1000\n",
      "23/23 [==============================] - 0s 843us/step - loss: 0.1512 - accuracy: 0.7823\n",
      "Epoch 932/1000\n",
      "23/23 [==============================] - 0s 933us/step - loss: 0.1512 - accuracy: 0.7823\n",
      "Epoch 933/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1512 - accuracy: 0.7823\n",
      "Epoch 934/1000\n",
      "23/23 [==============================] - 0s 839us/step - loss: 0.1512 - accuracy: 0.7823\n",
      "Epoch 935/1000\n",
      "23/23 [==============================] - 0s 906us/step - loss: 0.1512 - accuracy: 0.7823\n",
      "Epoch 936/1000\n",
      "23/23 [==============================] - 0s 796us/step - loss: 0.1512 - accuracy: 0.7823\n",
      "Epoch 937/1000\n",
      "23/23 [==============================] - 0s 832us/step - loss: 0.1512 - accuracy: 0.7823\n",
      "Epoch 938/1000\n",
      "23/23 [==============================] - 0s 817us/step - loss: 0.1512 - accuracy: 0.7823\n",
      "Epoch 939/1000\n",
      "23/23 [==============================] - 0s 780us/step - loss: 0.1512 - accuracy: 0.7823\n",
      "Epoch 940/1000\n",
      "23/23 [==============================] - 0s 886us/step - loss: 0.1511 - accuracy: 0.7823\n",
      "Epoch 941/1000\n",
      "23/23 [==============================] - 0s 860us/step - loss: 0.1511 - accuracy: 0.7823\n",
      "Epoch 942/1000\n",
      "23/23 [==============================] - 0s 794us/step - loss: 0.1511 - accuracy: 0.7823\n",
      "Epoch 943/1000\n",
      "23/23 [==============================] - 0s 822us/step - loss: 0.1511 - accuracy: 0.7823\n",
      "Epoch 944/1000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1511 - accuracy: 0.7823\n",
      "Epoch 945/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1511 - accuracy: 0.7823\n",
      "Epoch 946/1000\n",
      "23/23 [==============================] - 0s 903us/step - loss: 0.1511 - accuracy: 0.7823\n",
      "Epoch 947/1000\n",
      "23/23 [==============================] - 0s 919us/step - loss: 0.1511 - accuracy: 0.7823\n",
      "Epoch 948/1000\n",
      "23/23 [==============================] - 0s 882us/step - loss: 0.1511 - accuracy: 0.7823\n",
      "Epoch 949/1000\n",
      "23/23 [==============================] - 0s 865us/step - loss: 0.1511 - accuracy: 0.7823\n",
      "Epoch 950/1000\n",
      "23/23 [==============================] - 0s 899us/step - loss: 0.1511 - accuracy: 0.7823\n",
      "Epoch 951/1000\n",
      "23/23 [==============================] - 0s 922us/step - loss: 0.1511 - accuracy: 0.7823\n",
      "Epoch 952/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1511 - accuracy: 0.7823\n",
      "Epoch 953/1000\n",
      "23/23 [==============================] - 0s 870us/step - loss: 0.1510 - accuracy: 0.7823\n",
      "Epoch 954/1000\n",
      "23/23 [==============================] - 0s 853us/step - loss: 0.1510 - accuracy: 0.7823\n",
      "Epoch 955/1000\n",
      "23/23 [==============================] - 0s 950us/step - loss: 0.1510 - accuracy: 0.7823\n",
      "Epoch 956/1000\n",
      "23/23 [==============================] - 0s 834us/step - loss: 0.1510 - accuracy: 0.7823\n",
      "Epoch 957/1000\n",
      "23/23 [==============================] - 0s 943us/step - loss: 0.1510 - accuracy: 0.7823\n",
      "Epoch 958/1000\n",
      "23/23 [==============================] - 0s 817us/step - loss: 0.1510 - accuracy: 0.7823\n",
      "Epoch 959/1000\n",
      "23/23 [==============================] - 0s 908us/step - loss: 0.1510 - accuracy: 0.7823\n",
      "Epoch 960/1000\n",
      "23/23 [==============================] - 0s 903us/step - loss: 0.1510 - accuracy: 0.7823\n",
      "Epoch 961/1000\n",
      "23/23 [==============================] - 0s 815us/step - loss: 0.1510 - accuracy: 0.7823\n",
      "Epoch 962/1000\n",
      "23/23 [==============================] - 0s 933us/step - loss: 0.1510 - accuracy: 0.7823\n",
      "Epoch 963/1000\n",
      "23/23 [==============================] - 0s 851us/step - loss: 0.1510 - accuracy: 0.7823\n",
      "Epoch 964/1000\n",
      "23/23 [==============================] - 0s 943us/step - loss: 0.1510 - accuracy: 0.7823\n",
      "Epoch 965/1000\n",
      "23/23 [==============================] - 0s 826us/step - loss: 0.1510 - accuracy: 0.7823\n",
      "Epoch 966/1000\n",
      "23/23 [==============================] - 0s 916us/step - loss: 0.1510 - accuracy: 0.7823\n",
      "Epoch 967/1000\n",
      "23/23 [==============================] - 0s 865us/step - loss: 0.1510 - accuracy: 0.7823\n",
      "Epoch 968/1000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1509 - accuracy: 0.7823\n",
      "Epoch 969/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1509 - accuracy: 0.7823\n",
      "Epoch 970/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1509 - accuracy: 0.7823\n",
      "Epoch 971/1000\n",
      "23/23 [==============================] - 0s 882us/step - loss: 0.1509 - accuracy: 0.7823\n",
      "Epoch 972/1000\n",
      "23/23 [==============================] - 0s 934us/step - loss: 0.1509 - accuracy: 0.7823\n",
      "Epoch 973/1000\n",
      "23/23 [==============================] - 0s 879us/step - loss: 0.1509 - accuracy: 0.7823\n",
      "Epoch 974/1000\n",
      "23/23 [==============================] - 0s 856us/step - loss: 0.1509 - accuracy: 0.7823\n",
      "Epoch 975/1000\n",
      "23/23 [==============================] - 0s 794us/step - loss: 0.1509 - accuracy: 0.7823\n",
      "Epoch 976/1000\n",
      "23/23 [==============================] - 0s 833us/step - loss: 0.1509 - accuracy: 0.7823\n",
      "Epoch 977/1000\n",
      "23/23 [==============================] - 0s 821us/step - loss: 0.1509 - accuracy: 0.7823\n",
      "Epoch 978/1000\n",
      "23/23 [==============================] - 0s 822us/step - loss: 0.1509 - accuracy: 0.7823\n",
      "Epoch 979/1000\n",
      "23/23 [==============================] - 0s 899us/step - loss: 0.1509 - accuracy: 0.7823\n",
      "Epoch 980/1000\n",
      "23/23 [==============================] - 0s 847us/step - loss: 0.1509 - accuracy: 0.7823\n",
      "Epoch 981/1000\n",
      "23/23 [==============================] - 0s 890us/step - loss: 0.1509 - accuracy: 0.7823\n",
      "Epoch 982/1000\n",
      "23/23 [==============================] - 0s 867us/step - loss: 0.1508 - accuracy: 0.7823\n",
      "Epoch 983/1000\n",
      "23/23 [==============================] - 0s 840us/step - loss: 0.1508 - accuracy: 0.7823\n",
      "Epoch 984/1000\n",
      "23/23 [==============================] - 0s 804us/step - loss: 0.1508 - accuracy: 0.7823\n",
      "Epoch 985/1000\n",
      "23/23 [==============================] - 0s 849us/step - loss: 0.1508 - accuracy: 0.7823\n",
      "Epoch 986/1000\n",
      "23/23 [==============================] - 0s 924us/step - loss: 0.1508 - accuracy: 0.7823\n",
      "Epoch 987/1000\n",
      "23/23 [==============================] - 0s 826us/step - loss: 0.1508 - accuracy: 0.7823\n",
      "Epoch 988/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1508 - accuracy: 0.7823\n",
      "Epoch 989/1000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1508 - accuracy: 0.7823\n",
      "Epoch 990/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1508 - accuracy: 0.7823\n",
      "Epoch 991/1000\n",
      "23/23 [==============================] - 0s 909us/step - loss: 0.1508 - accuracy: 0.7823\n",
      "Epoch 992/1000\n",
      "23/23 [==============================] - 0s 928us/step - loss: 0.1508 - accuracy: 0.7823\n",
      "Epoch 993/1000\n",
      "23/23 [==============================] - 0s 800us/step - loss: 0.1508 - accuracy: 0.7823\n",
      "Epoch 994/1000\n",
      "23/23 [==============================] - 0s 893us/step - loss: 0.1508 - accuracy: 0.7823\n",
      "Epoch 995/1000\n",
      "23/23 [==============================] - 0s 873us/step - loss: 0.1508 - accuracy: 0.7823\n",
      "Epoch 996/1000\n",
      "23/23 [==============================] - 0s 888us/step - loss: 0.1507 - accuracy: 0.7823\n",
      "Epoch 997/1000\n",
      "23/23 [==============================] - 0s 903us/step - loss: 0.1507 - accuracy: 0.7823\n",
      "Epoch 998/1000\n",
      "23/23 [==============================] - 0s 917us/step - loss: 0.1507 - accuracy: 0.7823\n",
      "Epoch 999/1000\n",
      "23/23 [==============================] - 0s 879us/step - loss: 0.1507 - accuracy: 0.7823\n",
      "Epoch 1000/1000\n",
      "23/23 [==============================] - 0s 826us/step - loss: 0.1507 - accuracy: 0.7823\n",
      "11/11 [==============================] - 0s 789us/step\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train_normalize, y_train_normalize, epochs=epochs)\n",
    "\n",
    "predict = model.predict(x_test_normalize)\n",
    "predict = np.round(predict)\n",
    "predict = predict.astype(int)\n",
    "predict = predict.reshape(1, -1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model1_cf1 = confusion_matrix(y_test_normalize, predict)\n",
    "plot_cm(model1_cf1, 'Model 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Not Survived</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>204.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>127.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>331.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>331.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision  recall  f1-score  support\n",
       "Not Survived        1.0     1.0       1.0    204.0\n",
       "Survived            1.0     1.0       1.0    127.0\n",
       "accuracy            1.0     1.0       1.0      1.0\n",
       "macro avg           1.0     1.0       1.0    331.0\n",
       "weighted avg        1.0     1.0       1.0    331.0"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_report = classification_report(y_test_normalize, predict, output_dict=True, target_names=['Not Survived',\"Survived\"])\n",
    "pd.DataFrame(model1_report).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLayerPerceptron:\n",
    "    def __init__(self, input_size, learning_rate, epochs):\n",
    "        np.random.seed(42)\n",
    "        self.weights = np.random.uniform(-0.3,0.3, size=input_size+1)  # Additional weight for bias\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def _activation_function(self, x):\n",
    "        # Using a simple step function as the activation function\n",
    "        return 1 if x >= 0 else 0\n",
    "\n",
    "    def _predict(self, inputs):\n",
    "        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
    "        return self._activation_function(summation)\n",
    "\n",
    "    def _calculate_accuracy(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        return accuracy * 100\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        for epoch in range(self.epochs):\n",
    "            for inputs, label in zip(X_train, y_train):\n",
    "                prediction = self._predict(inputs)\n",
    "\n",
    "                # Update weights\n",
    "                error = label - prediction\n",
    "                self.weights[1:] += self.learning_rate * error * inputs\n",
    "                self.weights[0] += self.learning_rate * error\n",
    "\n",
    "            # Calculate training accuracy at each epoch\n",
    "            training_accuracy = self._calculate_accuracy(X_train, y_train)\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}: Training Accuracy = {training_accuracy:.2f}%\")\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predictions = [self._predict(inputs) for inputs in X_test]\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: Training Accuracy = 72.19%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000: Training Accuracy = 78.23%\n",
      "Epoch 3/1000: Training Accuracy = 73.74%\n",
      "Epoch 4/1000: Training Accuracy = 78.09%\n",
      "Epoch 5/1000: Training Accuracy = 76.12%\n",
      "Epoch 6/1000: Training Accuracy = 78.23%\n",
      "Epoch 7/1000: Training Accuracy = 76.97%\n",
      "Epoch 8/1000: Training Accuracy = 79.35%\n",
      "Epoch 9/1000: Training Accuracy = 79.21%\n",
      "Epoch 10/1000: Training Accuracy = 74.72%\n",
      "Epoch 11/1000: Training Accuracy = 75.84%\n",
      "Epoch 12/1000: Training Accuracy = 76.54%\n",
      "Epoch 13/1000: Training Accuracy = 75.98%\n",
      "Epoch 14/1000: Training Accuracy = 80.48%\n",
      "Epoch 15/1000: Training Accuracy = 79.63%\n",
      "Epoch 16/1000: Training Accuracy = 79.07%\n",
      "Epoch 17/1000: Training Accuracy = 76.83%\n",
      "Epoch 18/1000: Training Accuracy = 79.07%\n",
      "Epoch 19/1000: Training Accuracy = 77.11%\n",
      "Epoch 20/1000: Training Accuracy = 80.34%\n",
      "Epoch 21/1000: Training Accuracy = 78.93%\n",
      "Epoch 22/1000: Training Accuracy = 74.72%\n",
      "Epoch 23/1000: Training Accuracy = 76.69%\n",
      "Epoch 24/1000: Training Accuracy = 78.37%\n",
      "Epoch 25/1000: Training Accuracy = 78.37%\n",
      "Epoch 26/1000: Training Accuracy = 80.06%\n",
      "Epoch 27/1000: Training Accuracy = 78.51%\n",
      "Epoch 28/1000: Training Accuracy = 78.23%\n",
      "Epoch 29/1000: Training Accuracy = 77.53%\n",
      "Epoch 30/1000: Training Accuracy = 78.79%\n",
      "Epoch 31/1000: Training Accuracy = 79.78%\n",
      "Epoch 32/1000: Training Accuracy = 79.07%\n",
      "Epoch 33/1000: Training Accuracy = 79.92%\n",
      "Epoch 34/1000: Training Accuracy = 75.14%\n",
      "Epoch 35/1000: Training Accuracy = 80.06%\n",
      "Epoch 36/1000: Training Accuracy = 76.26%\n",
      "Epoch 37/1000: Training Accuracy = 75.98%\n",
      "Epoch 38/1000: Training Accuracy = 78.37%\n",
      "Epoch 39/1000: Training Accuracy = 79.49%\n",
      "Epoch 40/1000: Training Accuracy = 77.95%\n",
      "Epoch 41/1000: Training Accuracy = 79.21%\n",
      "Epoch 42/1000: Training Accuracy = 76.83%\n",
      "Epoch 43/1000: Training Accuracy = 74.30%\n",
      "Epoch 44/1000: Training Accuracy = 76.97%\n",
      "Epoch 45/1000: Training Accuracy = 78.79%\n",
      "Epoch 46/1000: Training Accuracy = 76.26%\n",
      "Epoch 47/1000: Training Accuracy = 79.63%\n",
      "Epoch 48/1000: Training Accuracy = 79.49%\n",
      "Epoch 49/1000: Training Accuracy = 77.81%\n",
      "Epoch 50/1000: Training Accuracy = 79.21%\n",
      "Epoch 51/1000: Training Accuracy = 75.42%\n",
      "Epoch 52/1000: Training Accuracy = 80.06%\n",
      "Epoch 53/1000: Training Accuracy = 77.81%\n",
      "Epoch 54/1000: Training Accuracy = 79.07%\n",
      "Epoch 55/1000: Training Accuracy = 77.95%\n",
      "Epoch 56/1000: Training Accuracy = 77.95%\n",
      "Epoch 57/1000: Training Accuracy = 76.26%\n",
      "Epoch 58/1000: Training Accuracy = 71.77%\n",
      "Epoch 59/1000: Training Accuracy = 76.69%\n",
      "Epoch 60/1000: Training Accuracy = 75.00%\n",
      "Epoch 61/1000: Training Accuracy = 75.56%\n",
      "Epoch 62/1000: Training Accuracy = 79.21%\n",
      "Epoch 63/1000: Training Accuracy = 76.83%\n",
      "Epoch 64/1000: Training Accuracy = 78.51%\n",
      "Epoch 65/1000: Training Accuracy = 80.20%\n",
      "Epoch 66/1000: Training Accuracy = 78.37%\n",
      "Epoch 67/1000: Training Accuracy = 75.00%\n",
      "Epoch 68/1000: Training Accuracy = 77.95%\n",
      "Epoch 69/1000: Training Accuracy = 74.30%\n",
      "Epoch 70/1000: Training Accuracy = 74.16%\n",
      "Epoch 71/1000: Training Accuracy = 72.89%\n",
      "Epoch 72/1000: Training Accuracy = 79.21%\n",
      "Epoch 73/1000: Training Accuracy = 79.35%\n",
      "Epoch 74/1000: Training Accuracy = 79.92%\n",
      "Epoch 75/1000: Training Accuracy = 78.23%\n",
      "Epoch 76/1000: Training Accuracy = 74.44%\n",
      "Epoch 77/1000: Training Accuracy = 75.00%\n",
      "Epoch 78/1000: Training Accuracy = 76.97%\n",
      "Epoch 79/1000: Training Accuracy = 80.76%\n",
      "Epoch 80/1000: Training Accuracy = 78.79%\n",
      "Epoch 81/1000: Training Accuracy = 79.07%\n",
      "Epoch 82/1000: Training Accuracy = 80.76%\n",
      "Epoch 83/1000: Training Accuracy = 78.93%\n",
      "Epoch 84/1000: Training Accuracy = 78.51%\n",
      "Epoch 85/1000: Training Accuracy = 74.30%\n",
      "Epoch 86/1000: Training Accuracy = 73.74%\n",
      "Epoch 87/1000: Training Accuracy = 76.12%\n",
      "Epoch 88/1000: Training Accuracy = 80.34%\n",
      "Epoch 89/1000: Training Accuracy = 78.37%\n",
      "Epoch 90/1000: Training Accuracy = 77.81%\n",
      "Epoch 91/1000: Training Accuracy = 73.60%\n",
      "Epoch 92/1000: Training Accuracy = 75.98%\n",
      "Epoch 93/1000: Training Accuracy = 80.06%\n",
      "Epoch 94/1000: Training Accuracy = 72.89%\n",
      "Epoch 95/1000: Training Accuracy = 75.56%\n",
      "Epoch 96/1000: Training Accuracy = 78.51%\n",
      "Epoch 97/1000: Training Accuracy = 76.26%\n",
      "Epoch 98/1000: Training Accuracy = 76.40%\n",
      "Epoch 99/1000: Training Accuracy = 79.21%\n",
      "Epoch 100/1000: Training Accuracy = 79.35%\n",
      "Epoch 101/1000: Training Accuracy = 80.62%\n",
      "Epoch 102/1000: Training Accuracy = 78.79%\n",
      "Epoch 103/1000: Training Accuracy = 75.84%\n",
      "Epoch 104/1000: Training Accuracy = 75.28%\n",
      "Epoch 105/1000: Training Accuracy = 78.79%\n",
      "Epoch 106/1000: Training Accuracy = 79.07%\n",
      "Epoch 107/1000: Training Accuracy = 77.81%\n",
      "Epoch 108/1000: Training Accuracy = 79.21%\n",
      "Epoch 109/1000: Training Accuracy = 79.63%\n",
      "Epoch 110/1000: Training Accuracy = 78.65%\n",
      "Epoch 111/1000: Training Accuracy = 79.21%\n",
      "Epoch 112/1000: Training Accuracy = 78.65%\n",
      "Epoch 113/1000: Training Accuracy = 74.86%\n",
      "Epoch 114/1000: Training Accuracy = 79.49%\n",
      "Epoch 115/1000: Training Accuracy = 71.91%\n",
      "Epoch 116/1000: Training Accuracy = 77.95%\n",
      "Epoch 117/1000: Training Accuracy = 76.26%\n",
      "Epoch 118/1000: Training Accuracy = 76.40%\n",
      "Epoch 119/1000: Training Accuracy = 76.69%\n",
      "Epoch 120/1000: Training Accuracy = 78.93%\n",
      "Epoch 121/1000: Training Accuracy = 79.92%\n",
      "Epoch 122/1000: Training Accuracy = 76.54%\n",
      "Epoch 123/1000: Training Accuracy = 73.74%\n",
      "Epoch 124/1000: Training Accuracy = 78.23%\n",
      "Epoch 125/1000: Training Accuracy = 81.18%\n",
      "Epoch 126/1000: Training Accuracy = 80.06%\n",
      "Epoch 127/1000: Training Accuracy = 78.37%\n",
      "Epoch 128/1000: Training Accuracy = 79.07%\n",
      "Epoch 129/1000: Training Accuracy = 74.58%\n",
      "Epoch 130/1000: Training Accuracy = 79.21%\n",
      "Epoch 131/1000: Training Accuracy = 78.79%\n",
      "Epoch 132/1000: Training Accuracy = 79.49%\n",
      "Epoch 133/1000: Training Accuracy = 76.12%\n",
      "Epoch 134/1000: Training Accuracy = 80.48%\n",
      "Epoch 135/1000: Training Accuracy = 79.07%\n",
      "Epoch 136/1000: Training Accuracy = 79.63%\n",
      "Epoch 137/1000: Training Accuracy = 78.65%\n",
      "Epoch 138/1000: Training Accuracy = 73.88%\n",
      "Epoch 139/1000: Training Accuracy = 75.14%\n",
      "Epoch 140/1000: Training Accuracy = 74.72%\n",
      "Epoch 141/1000: Training Accuracy = 79.35%\n",
      "Epoch 142/1000: Training Accuracy = 79.21%\n",
      "Epoch 143/1000: Training Accuracy = 80.62%\n",
      "Epoch 144/1000: Training Accuracy = 78.65%\n",
      "Epoch 145/1000: Training Accuracy = 79.63%\n",
      "Epoch 146/1000: Training Accuracy = 76.40%\n",
      "Epoch 147/1000: Training Accuracy = 79.21%\n",
      "Epoch 148/1000: Training Accuracy = 77.67%\n",
      "Epoch 149/1000: Training Accuracy = 80.90%\n",
      "Epoch 150/1000: Training Accuracy = 77.81%\n",
      "Epoch 151/1000: Training Accuracy = 76.40%\n",
      "Epoch 152/1000: Training Accuracy = 79.21%\n",
      "Epoch 153/1000: Training Accuracy = 75.84%\n",
      "Epoch 154/1000: Training Accuracy = 78.93%\n",
      "Epoch 155/1000: Training Accuracy = 78.23%\n",
      "Epoch 156/1000: Training Accuracy = 74.86%\n",
      "Epoch 157/1000: Training Accuracy = 79.92%\n",
      "Epoch 158/1000: Training Accuracy = 79.21%\n",
      "Epoch 159/1000: Training Accuracy = 79.07%\n",
      "Epoch 160/1000: Training Accuracy = 78.93%\n",
      "Epoch 161/1000: Training Accuracy = 79.35%\n",
      "Epoch 162/1000: Training Accuracy = 76.69%\n",
      "Epoch 163/1000: Training Accuracy = 79.35%\n",
      "Epoch 164/1000: Training Accuracy = 78.51%\n",
      "Epoch 165/1000: Training Accuracy = 78.93%\n",
      "Epoch 166/1000: Training Accuracy = 80.76%\n",
      "Epoch 167/1000: Training Accuracy = 78.65%\n",
      "Epoch 168/1000: Training Accuracy = 78.93%\n",
      "Epoch 169/1000: Training Accuracy = 77.39%\n",
      "Epoch 170/1000: Training Accuracy = 77.25%\n",
      "Epoch 171/1000: Training Accuracy = 80.20%\n",
      "Epoch 172/1000: Training Accuracy = 75.14%\n",
      "Epoch 173/1000: Training Accuracy = 78.51%\n",
      "Epoch 174/1000: Training Accuracy = 79.21%\n",
      "Epoch 175/1000: Training Accuracy = 80.48%\n",
      "Epoch 176/1000: Training Accuracy = 81.04%\n",
      "Epoch 177/1000: Training Accuracy = 79.92%\n",
      "Epoch 178/1000: Training Accuracy = 80.06%\n",
      "Epoch 179/1000: Training Accuracy = 76.83%\n",
      "Epoch 180/1000: Training Accuracy = 73.74%\n",
      "Epoch 181/1000: Training Accuracy = 78.79%\n",
      "Epoch 182/1000: Training Accuracy = 79.07%\n",
      "Epoch 183/1000: Training Accuracy = 80.20%\n",
      "Epoch 184/1000: Training Accuracy = 74.44%\n",
      "Epoch 185/1000: Training Accuracy = 79.21%\n",
      "Epoch 186/1000: Training Accuracy = 80.90%\n",
      "Epoch 187/1000: Training Accuracy = 79.21%\n",
      "Epoch 188/1000: Training Accuracy = 75.28%\n",
      "Epoch 189/1000: Training Accuracy = 80.06%\n",
      "Epoch 190/1000: Training Accuracy = 73.60%\n",
      "Epoch 191/1000: Training Accuracy = 73.46%\n",
      "Epoch 192/1000: Training Accuracy = 79.63%\n",
      "Epoch 193/1000: Training Accuracy = 76.83%\n",
      "Epoch 194/1000: Training Accuracy = 71.63%\n",
      "Epoch 195/1000: Training Accuracy = 73.60%\n",
      "Epoch 196/1000: Training Accuracy = 74.86%\n",
      "Epoch 197/1000: Training Accuracy = 72.61%\n",
      "Epoch 198/1000: Training Accuracy = 79.35%\n",
      "Epoch 199/1000: Training Accuracy = 77.53%\n",
      "Epoch 200/1000: Training Accuracy = 75.42%\n",
      "Epoch 201/1000: Training Accuracy = 77.95%\n",
      "Epoch 202/1000: Training Accuracy = 76.54%\n",
      "Epoch 203/1000: Training Accuracy = 79.07%\n",
      "Epoch 204/1000: Training Accuracy = 79.07%\n",
      "Epoch 205/1000: Training Accuracy = 79.78%\n",
      "Epoch 206/1000: Training Accuracy = 75.14%\n",
      "Epoch 207/1000: Training Accuracy = 79.07%\n",
      "Epoch 208/1000: Training Accuracy = 75.28%\n",
      "Epoch 209/1000: Training Accuracy = 77.53%\n",
      "Epoch 210/1000: Training Accuracy = 78.09%\n",
      "Epoch 211/1000: Training Accuracy = 79.78%\n",
      "Epoch 212/1000: Training Accuracy = 74.44%\n",
      "Epoch 213/1000: Training Accuracy = 79.92%\n",
      "Epoch 214/1000: Training Accuracy = 72.47%\n",
      "Epoch 215/1000: Training Accuracy = 75.00%\n",
      "Epoch 216/1000: Training Accuracy = 78.51%\n",
      "Epoch 217/1000: Training Accuracy = 79.35%\n",
      "Epoch 218/1000: Training Accuracy = 75.84%\n",
      "Epoch 219/1000: Training Accuracy = 79.35%\n",
      "Epoch 220/1000: Training Accuracy = 79.21%\n",
      "Epoch 221/1000: Training Accuracy = 78.51%\n",
      "Epoch 222/1000: Training Accuracy = 75.98%\n",
      "Epoch 223/1000: Training Accuracy = 75.56%\n",
      "Epoch 224/1000: Training Accuracy = 75.14%\n",
      "Epoch 225/1000: Training Accuracy = 79.78%\n",
      "Epoch 226/1000: Training Accuracy = 72.19%\n",
      "Epoch 227/1000: Training Accuracy = 76.12%\n",
      "Epoch 228/1000: Training Accuracy = 79.21%\n",
      "Epoch 229/1000: Training Accuracy = 73.88%\n",
      "Epoch 230/1000: Training Accuracy = 78.93%\n",
      "Epoch 231/1000: Training Accuracy = 79.35%\n",
      "Epoch 232/1000: Training Accuracy = 73.74%\n",
      "Epoch 233/1000: Training Accuracy = 76.40%\n",
      "Epoch 234/1000: Training Accuracy = 79.07%\n",
      "Epoch 235/1000: Training Accuracy = 79.63%\n",
      "Epoch 236/1000: Training Accuracy = 78.93%\n",
      "Epoch 237/1000: Training Accuracy = 73.46%\n",
      "Epoch 238/1000: Training Accuracy = 78.09%\n",
      "Epoch 239/1000: Training Accuracy = 76.54%\n",
      "Epoch 240/1000: Training Accuracy = 76.54%\n",
      "Epoch 241/1000: Training Accuracy = 79.63%\n",
      "Epoch 242/1000: Training Accuracy = 80.06%\n",
      "Epoch 243/1000: Training Accuracy = 74.16%\n",
      "Epoch 244/1000: Training Accuracy = 79.07%\n",
      "Epoch 245/1000: Training Accuracy = 74.44%\n",
      "Epoch 246/1000: Training Accuracy = 79.78%\n",
      "Epoch 247/1000: Training Accuracy = 78.79%\n",
      "Epoch 248/1000: Training Accuracy = 75.98%\n",
      "Epoch 249/1000: Training Accuracy = 73.88%\n",
      "Epoch 250/1000: Training Accuracy = 72.33%\n",
      "Epoch 251/1000: Training Accuracy = 80.06%\n",
      "Epoch 252/1000: Training Accuracy = 75.98%\n",
      "Epoch 253/1000: Training Accuracy = 77.25%\n",
      "Epoch 254/1000: Training Accuracy = 80.90%\n",
      "Epoch 255/1000: Training Accuracy = 73.74%\n",
      "Epoch 256/1000: Training Accuracy = 80.34%\n",
      "Epoch 257/1000: Training Accuracy = 74.72%\n",
      "Epoch 258/1000: Training Accuracy = 73.60%\n",
      "Epoch 259/1000: Training Accuracy = 78.65%\n",
      "Epoch 260/1000: Training Accuracy = 78.79%\n",
      "Epoch 261/1000: Training Accuracy = 80.20%\n",
      "Epoch 262/1000: Training Accuracy = 80.20%\n",
      "Epoch 263/1000: Training Accuracy = 74.02%\n",
      "Epoch 264/1000: Training Accuracy = 79.63%\n",
      "Epoch 265/1000: Training Accuracy = 77.53%\n",
      "Epoch 266/1000: Training Accuracy = 76.69%\n",
      "Epoch 267/1000: Training Accuracy = 77.39%\n",
      "Epoch 268/1000: Training Accuracy = 80.34%\n",
      "Epoch 269/1000: Training Accuracy = 74.44%\n",
      "Epoch 270/1000: Training Accuracy = 79.49%\n",
      "Epoch 271/1000: Training Accuracy = 75.42%\n",
      "Epoch 272/1000: Training Accuracy = 76.54%\n",
      "Epoch 273/1000: Training Accuracy = 79.78%\n",
      "Epoch 274/1000: Training Accuracy = 80.06%\n",
      "Epoch 275/1000: Training Accuracy = 72.19%\n",
      "Epoch 276/1000: Training Accuracy = 77.67%\n",
      "Epoch 277/1000: Training Accuracy = 78.09%\n",
      "Epoch 278/1000: Training Accuracy = 75.98%\n",
      "Epoch 279/1000: Training Accuracy = 78.37%\n",
      "Epoch 280/1000: Training Accuracy = 79.63%\n",
      "Epoch 281/1000: Training Accuracy = 79.63%\n",
      "Epoch 282/1000: Training Accuracy = 76.54%\n",
      "Epoch 283/1000: Training Accuracy = 80.06%\n",
      "Epoch 284/1000: Training Accuracy = 75.14%\n",
      "Epoch 285/1000: Training Accuracy = 77.11%\n",
      "Epoch 286/1000: Training Accuracy = 76.26%\n",
      "Epoch 287/1000: Training Accuracy = 75.98%\n",
      "Epoch 288/1000: Training Accuracy = 79.35%\n",
      "Epoch 289/1000: Training Accuracy = 79.49%\n",
      "Epoch 290/1000: Training Accuracy = 73.46%\n",
      "Epoch 291/1000: Training Accuracy = 78.79%\n",
      "Epoch 292/1000: Training Accuracy = 79.21%\n",
      "Epoch 293/1000: Training Accuracy = 78.93%\n",
      "Epoch 294/1000: Training Accuracy = 78.65%\n",
      "Epoch 295/1000: Training Accuracy = 73.88%\n",
      "Epoch 296/1000: Training Accuracy = 77.39%\n",
      "Epoch 297/1000: Training Accuracy = 76.54%\n",
      "Epoch 298/1000: Training Accuracy = 76.12%\n",
      "Epoch 299/1000: Training Accuracy = 79.63%\n",
      "Epoch 300/1000: Training Accuracy = 75.14%\n",
      "Epoch 301/1000: Training Accuracy = 79.21%\n",
      "Epoch 302/1000: Training Accuracy = 80.20%\n",
      "Epoch 303/1000: Training Accuracy = 77.81%\n",
      "Epoch 304/1000: Training Accuracy = 76.69%\n",
      "Epoch 305/1000: Training Accuracy = 79.78%\n",
      "Epoch 306/1000: Training Accuracy = 78.23%\n",
      "Epoch 307/1000: Training Accuracy = 74.58%\n",
      "Epoch 308/1000: Training Accuracy = 78.37%\n",
      "Epoch 309/1000: Training Accuracy = 78.79%\n",
      "Epoch 310/1000: Training Accuracy = 72.33%\n",
      "Epoch 311/1000: Training Accuracy = 74.02%\n",
      "Epoch 312/1000: Training Accuracy = 79.21%\n",
      "Epoch 313/1000: Training Accuracy = 79.92%\n",
      "Epoch 314/1000: Training Accuracy = 75.00%\n",
      "Epoch 315/1000: Training Accuracy = 78.51%\n",
      "Epoch 316/1000: Training Accuracy = 79.21%\n",
      "Epoch 317/1000: Training Accuracy = 75.56%\n",
      "Epoch 318/1000: Training Accuracy = 79.78%\n",
      "Epoch 319/1000: Training Accuracy = 78.93%\n",
      "Epoch 320/1000: Training Accuracy = 75.14%\n",
      "Epoch 321/1000: Training Accuracy = 76.83%\n",
      "Epoch 322/1000: Training Accuracy = 72.61%\n",
      "Epoch 323/1000: Training Accuracy = 78.37%\n",
      "Epoch 324/1000: Training Accuracy = 77.81%\n",
      "Epoch 325/1000: Training Accuracy = 78.51%\n",
      "Epoch 326/1000: Training Accuracy = 79.35%\n",
      "Epoch 327/1000: Training Accuracy = 75.14%\n",
      "Epoch 328/1000: Training Accuracy = 78.37%\n",
      "Epoch 329/1000: Training Accuracy = 79.07%\n",
      "Epoch 330/1000: Training Accuracy = 80.06%\n",
      "Epoch 331/1000: Training Accuracy = 79.35%\n",
      "Epoch 332/1000: Training Accuracy = 78.93%\n",
      "Epoch 333/1000: Training Accuracy = 75.28%\n",
      "Epoch 334/1000: Training Accuracy = 76.26%\n",
      "Epoch 335/1000: Training Accuracy = 73.74%\n",
      "Epoch 336/1000: Training Accuracy = 77.11%\n",
      "Epoch 337/1000: Training Accuracy = 74.30%\n",
      "Epoch 338/1000: Training Accuracy = 75.00%\n",
      "Epoch 339/1000: Training Accuracy = 78.65%\n",
      "Epoch 340/1000: Training Accuracy = 76.12%\n",
      "Epoch 341/1000: Training Accuracy = 74.44%\n",
      "Epoch 342/1000: Training Accuracy = 78.23%\n",
      "Epoch 343/1000: Training Accuracy = 76.26%\n",
      "Epoch 344/1000: Training Accuracy = 78.37%\n",
      "Epoch 345/1000: Training Accuracy = 79.63%\n",
      "Epoch 346/1000: Training Accuracy = 75.70%\n",
      "Epoch 347/1000: Training Accuracy = 79.49%\n",
      "Epoch 348/1000: Training Accuracy = 79.49%\n",
      "Epoch 349/1000: Training Accuracy = 78.65%\n",
      "Epoch 350/1000: Training Accuracy = 77.81%\n",
      "Epoch 351/1000: Training Accuracy = 79.21%\n",
      "Epoch 352/1000: Training Accuracy = 80.76%\n",
      "Epoch 353/1000: Training Accuracy = 80.20%\n",
      "Epoch 354/1000: Training Accuracy = 78.23%\n",
      "Epoch 355/1000: Training Accuracy = 79.21%\n",
      "Epoch 356/1000: Training Accuracy = 80.06%\n",
      "Epoch 357/1000: Training Accuracy = 78.79%\n",
      "Epoch 358/1000: Training Accuracy = 79.35%\n",
      "Epoch 359/1000: Training Accuracy = 74.58%\n",
      "Epoch 360/1000: Training Accuracy = 72.19%\n",
      "Epoch 361/1000: Training Accuracy = 71.21%\n",
      "Epoch 362/1000: Training Accuracy = 75.84%\n",
      "Epoch 363/1000: Training Accuracy = 72.47%\n",
      "Epoch 364/1000: Training Accuracy = 76.40%\n",
      "Epoch 365/1000: Training Accuracy = 78.65%\n",
      "Epoch 366/1000: Training Accuracy = 79.63%\n",
      "Epoch 367/1000: Training Accuracy = 79.21%\n",
      "Epoch 368/1000: Training Accuracy = 77.53%\n",
      "Epoch 369/1000: Training Accuracy = 78.65%\n",
      "Epoch 370/1000: Training Accuracy = 78.93%\n",
      "Epoch 371/1000: Training Accuracy = 76.12%\n",
      "Epoch 372/1000: Training Accuracy = 79.92%\n",
      "Epoch 373/1000: Training Accuracy = 73.17%\n",
      "Epoch 374/1000: Training Accuracy = 78.65%\n",
      "Epoch 375/1000: Training Accuracy = 79.35%\n",
      "Epoch 376/1000: Training Accuracy = 77.39%\n",
      "Epoch 377/1000: Training Accuracy = 74.58%\n",
      "Epoch 378/1000: Training Accuracy = 78.65%\n",
      "Epoch 379/1000: Training Accuracy = 76.26%\n",
      "Epoch 380/1000: Training Accuracy = 80.62%\n",
      "Epoch 381/1000: Training Accuracy = 79.21%\n",
      "Epoch 382/1000: Training Accuracy = 75.70%\n",
      "Epoch 383/1000: Training Accuracy = 78.37%\n",
      "Epoch 384/1000: Training Accuracy = 73.17%\n",
      "Epoch 385/1000: Training Accuracy = 79.63%\n",
      "Epoch 386/1000: Training Accuracy = 80.34%\n",
      "Epoch 387/1000: Training Accuracy = 79.78%\n",
      "Epoch 388/1000: Training Accuracy = 79.21%\n",
      "Epoch 389/1000: Training Accuracy = 78.65%\n",
      "Epoch 390/1000: Training Accuracy = 78.65%\n",
      "Epoch 391/1000: Training Accuracy = 74.86%\n",
      "Epoch 392/1000: Training Accuracy = 78.51%\n",
      "Epoch 393/1000: Training Accuracy = 76.54%\n",
      "Epoch 394/1000: Training Accuracy = 80.20%\n",
      "Epoch 395/1000: Training Accuracy = 78.65%\n",
      "Epoch 396/1000: Training Accuracy = 80.06%\n",
      "Epoch 397/1000: Training Accuracy = 79.07%\n",
      "Epoch 398/1000: Training Accuracy = 79.78%\n",
      "Epoch 399/1000: Training Accuracy = 73.17%\n",
      "Epoch 400/1000: Training Accuracy = 79.35%\n",
      "Epoch 401/1000: Training Accuracy = 79.92%\n",
      "Epoch 402/1000: Training Accuracy = 75.00%\n",
      "Epoch 403/1000: Training Accuracy = 77.95%\n",
      "Epoch 404/1000: Training Accuracy = 79.35%\n",
      "Epoch 405/1000: Training Accuracy = 79.35%\n",
      "Epoch 406/1000: Training Accuracy = 80.34%\n",
      "Epoch 407/1000: Training Accuracy = 78.93%\n",
      "Epoch 408/1000: Training Accuracy = 73.88%\n",
      "Epoch 409/1000: Training Accuracy = 78.37%\n",
      "Epoch 410/1000: Training Accuracy = 78.79%\n",
      "Epoch 411/1000: Training Accuracy = 71.91%\n",
      "Epoch 412/1000: Training Accuracy = 73.88%\n",
      "Epoch 413/1000: Training Accuracy = 72.19%\n",
      "Epoch 414/1000: Training Accuracy = 79.92%\n",
      "Epoch 415/1000: Training Accuracy = 79.21%\n",
      "Epoch 416/1000: Training Accuracy = 75.28%\n",
      "Epoch 417/1000: Training Accuracy = 75.42%\n",
      "Epoch 418/1000: Training Accuracy = 75.14%\n",
      "Epoch 419/1000: Training Accuracy = 79.63%\n",
      "Epoch 420/1000: Training Accuracy = 79.21%\n",
      "Epoch 421/1000: Training Accuracy = 78.93%\n",
      "Epoch 422/1000: Training Accuracy = 77.67%\n",
      "Epoch 423/1000: Training Accuracy = 77.25%\n",
      "Epoch 424/1000: Training Accuracy = 80.20%\n",
      "Epoch 425/1000: Training Accuracy = 76.40%\n",
      "Epoch 426/1000: Training Accuracy = 79.07%\n",
      "Epoch 427/1000: Training Accuracy = 75.56%\n",
      "Epoch 428/1000: Training Accuracy = 79.35%\n",
      "Epoch 429/1000: Training Accuracy = 79.35%\n",
      "Epoch 430/1000: Training Accuracy = 80.20%\n",
      "Epoch 431/1000: Training Accuracy = 78.93%\n",
      "Epoch 432/1000: Training Accuracy = 79.63%\n",
      "Epoch 433/1000: Training Accuracy = 74.02%\n",
      "Epoch 434/1000: Training Accuracy = 79.07%\n",
      "Epoch 435/1000: Training Accuracy = 78.23%\n",
      "Epoch 436/1000: Training Accuracy = 75.42%\n",
      "Epoch 437/1000: Training Accuracy = 77.95%\n",
      "Epoch 438/1000: Training Accuracy = 74.44%\n",
      "Epoch 439/1000: Training Accuracy = 74.86%\n",
      "Epoch 440/1000: Training Accuracy = 78.93%\n",
      "Epoch 441/1000: Training Accuracy = 76.26%\n",
      "Epoch 442/1000: Training Accuracy = 79.63%\n",
      "Epoch 443/1000: Training Accuracy = 75.14%\n",
      "Epoch 444/1000: Training Accuracy = 79.78%\n",
      "Epoch 445/1000: Training Accuracy = 79.63%\n",
      "Epoch 446/1000: Training Accuracy = 74.02%\n",
      "Epoch 447/1000: Training Accuracy = 79.63%\n",
      "Epoch 448/1000: Training Accuracy = 79.07%\n",
      "Epoch 449/1000: Training Accuracy = 74.86%\n",
      "Epoch 450/1000: Training Accuracy = 79.49%\n",
      "Epoch 451/1000: Training Accuracy = 79.63%\n",
      "Epoch 452/1000: Training Accuracy = 79.63%\n",
      "Epoch 453/1000: Training Accuracy = 76.26%\n",
      "Epoch 454/1000: Training Accuracy = 79.78%\n",
      "Epoch 455/1000: Training Accuracy = 79.49%\n",
      "Epoch 456/1000: Training Accuracy = 77.81%\n",
      "Epoch 457/1000: Training Accuracy = 79.07%\n",
      "Epoch 458/1000: Training Accuracy = 78.23%\n",
      "Epoch 459/1000: Training Accuracy = 79.07%\n",
      "Epoch 460/1000: Training Accuracy = 73.74%\n",
      "Epoch 461/1000: Training Accuracy = 79.78%\n",
      "Epoch 462/1000: Training Accuracy = 78.93%\n",
      "Epoch 463/1000: Training Accuracy = 74.44%\n",
      "Epoch 464/1000: Training Accuracy = 75.00%\n",
      "Epoch 465/1000: Training Accuracy = 79.92%\n",
      "Epoch 466/1000: Training Accuracy = 79.07%\n",
      "Epoch 467/1000: Training Accuracy = 75.98%\n",
      "Epoch 468/1000: Training Accuracy = 73.03%\n",
      "Epoch 469/1000: Training Accuracy = 79.07%\n",
      "Epoch 470/1000: Training Accuracy = 77.81%\n",
      "Epoch 471/1000: Training Accuracy = 80.48%\n",
      "Epoch 472/1000: Training Accuracy = 78.23%\n",
      "Epoch 473/1000: Training Accuracy = 74.30%\n",
      "Epoch 474/1000: Training Accuracy = 74.44%\n",
      "Epoch 475/1000: Training Accuracy = 78.65%\n",
      "Epoch 476/1000: Training Accuracy = 77.53%\n",
      "Epoch 477/1000: Training Accuracy = 78.23%\n",
      "Epoch 478/1000: Training Accuracy = 77.95%\n",
      "Epoch 479/1000: Training Accuracy = 78.65%\n",
      "Epoch 480/1000: Training Accuracy = 78.93%\n",
      "Epoch 481/1000: Training Accuracy = 79.07%\n",
      "Epoch 482/1000: Training Accuracy = 73.74%\n",
      "Epoch 483/1000: Training Accuracy = 80.62%\n",
      "Epoch 484/1000: Training Accuracy = 78.37%\n",
      "Epoch 485/1000: Training Accuracy = 78.79%\n",
      "Epoch 486/1000: Training Accuracy = 73.46%\n",
      "Epoch 487/1000: Training Accuracy = 79.35%\n",
      "Epoch 488/1000: Training Accuracy = 76.12%\n",
      "Epoch 489/1000: Training Accuracy = 75.14%\n",
      "Epoch 490/1000: Training Accuracy = 75.28%\n",
      "Epoch 491/1000: Training Accuracy = 79.49%\n",
      "Epoch 492/1000: Training Accuracy = 78.93%\n",
      "Epoch 493/1000: Training Accuracy = 79.07%\n",
      "Epoch 494/1000: Training Accuracy = 79.63%\n",
      "Epoch 495/1000: Training Accuracy = 77.95%\n",
      "Epoch 496/1000: Training Accuracy = 76.69%\n",
      "Epoch 497/1000: Training Accuracy = 74.72%\n",
      "Epoch 498/1000: Training Accuracy = 79.49%\n",
      "Epoch 499/1000: Training Accuracy = 77.11%\n",
      "Epoch 500/1000: Training Accuracy = 80.20%\n",
      "Epoch 501/1000: Training Accuracy = 74.02%\n",
      "Epoch 502/1000: Training Accuracy = 76.26%\n",
      "Epoch 503/1000: Training Accuracy = 73.46%\n",
      "Epoch 504/1000: Training Accuracy = 79.35%\n",
      "Epoch 505/1000: Training Accuracy = 79.35%\n",
      "Epoch 506/1000: Training Accuracy = 80.20%\n",
      "Epoch 507/1000: Training Accuracy = 78.93%\n",
      "Epoch 508/1000: Training Accuracy = 79.63%\n",
      "Epoch 509/1000: Training Accuracy = 74.02%\n",
      "Epoch 510/1000: Training Accuracy = 76.54%\n",
      "Epoch 511/1000: Training Accuracy = 74.44%\n",
      "Epoch 512/1000: Training Accuracy = 78.37%\n",
      "Epoch 513/1000: Training Accuracy = 74.30%\n",
      "Epoch 514/1000: Training Accuracy = 77.25%\n",
      "Epoch 515/1000: Training Accuracy = 76.12%\n",
      "Epoch 516/1000: Training Accuracy = 75.98%\n",
      "Epoch 517/1000: Training Accuracy = 79.07%\n",
      "Epoch 518/1000: Training Accuracy = 78.93%\n",
      "Epoch 519/1000: Training Accuracy = 79.92%\n",
      "Epoch 520/1000: Training Accuracy = 78.65%\n",
      "Epoch 521/1000: Training Accuracy = 72.33%\n",
      "Epoch 522/1000: Training Accuracy = 73.03%\n",
      "Epoch 523/1000: Training Accuracy = 80.20%\n",
      "Epoch 524/1000: Training Accuracy = 72.89%\n",
      "Epoch 525/1000: Training Accuracy = 79.49%\n",
      "Epoch 526/1000: Training Accuracy = 79.78%\n",
      "Epoch 527/1000: Training Accuracy = 79.78%\n",
      "Epoch 528/1000: Training Accuracy = 78.65%\n",
      "Epoch 529/1000: Training Accuracy = 73.74%\n",
      "Epoch 530/1000: Training Accuracy = 78.09%\n",
      "Epoch 531/1000: Training Accuracy = 80.62%\n",
      "Epoch 532/1000: Training Accuracy = 76.83%\n",
      "Epoch 533/1000: Training Accuracy = 79.07%\n",
      "Epoch 534/1000: Training Accuracy = 79.78%\n",
      "Epoch 535/1000: Training Accuracy = 80.06%\n",
      "Epoch 536/1000: Training Accuracy = 77.81%\n",
      "Epoch 537/1000: Training Accuracy = 76.54%\n",
      "Epoch 538/1000: Training Accuracy = 79.78%\n",
      "Epoch 539/1000: Training Accuracy = 78.23%\n",
      "Epoch 540/1000: Training Accuracy = 74.44%\n",
      "Epoch 541/1000: Training Accuracy = 79.21%\n",
      "Epoch 542/1000: Training Accuracy = 78.37%\n",
      "Epoch 543/1000: Training Accuracy = 76.97%\n",
      "Epoch 544/1000: Training Accuracy = 78.93%\n",
      "Epoch 545/1000: Training Accuracy = 78.93%\n",
      "Epoch 546/1000: Training Accuracy = 75.00%\n",
      "Epoch 547/1000: Training Accuracy = 78.79%\n",
      "Epoch 548/1000: Training Accuracy = 79.07%\n",
      "Epoch 549/1000: Training Accuracy = 78.51%\n",
      "Epoch 550/1000: Training Accuracy = 80.62%\n",
      "Epoch 551/1000: Training Accuracy = 73.46%\n",
      "Epoch 552/1000: Training Accuracy = 77.81%\n",
      "Epoch 553/1000: Training Accuracy = 75.84%\n",
      "Epoch 554/1000: Training Accuracy = 74.58%\n",
      "Epoch 555/1000: Training Accuracy = 73.88%\n",
      "Epoch 556/1000: Training Accuracy = 74.02%\n",
      "Epoch 557/1000: Training Accuracy = 74.44%\n",
      "Epoch 558/1000: Training Accuracy = 77.53%\n",
      "Epoch 559/1000: Training Accuracy = 78.37%\n",
      "Epoch 560/1000: Training Accuracy = 79.21%\n",
      "Epoch 561/1000: Training Accuracy = 75.56%\n",
      "Epoch 562/1000: Training Accuracy = 73.46%\n",
      "Epoch 563/1000: Training Accuracy = 79.63%\n",
      "Epoch 564/1000: Training Accuracy = 75.98%\n",
      "Epoch 565/1000: Training Accuracy = 79.07%\n",
      "Epoch 566/1000: Training Accuracy = 79.49%\n",
      "Epoch 567/1000: Training Accuracy = 78.37%\n",
      "Epoch 568/1000: Training Accuracy = 78.51%\n",
      "Epoch 569/1000: Training Accuracy = 75.70%\n",
      "Epoch 570/1000: Training Accuracy = 77.53%\n",
      "Epoch 571/1000: Training Accuracy = 79.63%\n",
      "Epoch 572/1000: Training Accuracy = 79.35%\n",
      "Epoch 573/1000: Training Accuracy = 74.16%\n",
      "Epoch 574/1000: Training Accuracy = 78.93%\n",
      "Epoch 575/1000: Training Accuracy = 79.35%\n",
      "Epoch 576/1000: Training Accuracy = 78.23%\n",
      "Epoch 577/1000: Training Accuracy = 79.78%\n",
      "Epoch 578/1000: Training Accuracy = 80.20%\n",
      "Epoch 579/1000: Training Accuracy = 77.95%\n",
      "Epoch 580/1000: Training Accuracy = 79.21%\n",
      "Epoch 581/1000: Training Accuracy = 80.20%\n",
      "Epoch 582/1000: Training Accuracy = 71.77%\n",
      "Epoch 583/1000: Training Accuracy = 78.37%\n",
      "Epoch 584/1000: Training Accuracy = 79.49%\n",
      "Epoch 585/1000: Training Accuracy = 79.35%\n",
      "Epoch 586/1000: Training Accuracy = 79.63%\n",
      "Epoch 587/1000: Training Accuracy = 79.07%\n",
      "Epoch 588/1000: Training Accuracy = 79.49%\n",
      "Epoch 589/1000: Training Accuracy = 77.95%\n",
      "Epoch 590/1000: Training Accuracy = 74.44%\n",
      "Epoch 591/1000: Training Accuracy = 79.07%\n",
      "Epoch 592/1000: Training Accuracy = 79.63%\n",
      "Epoch 593/1000: Training Accuracy = 79.63%\n",
      "Epoch 594/1000: Training Accuracy = 73.46%\n",
      "Epoch 595/1000: Training Accuracy = 79.35%\n",
      "Epoch 596/1000: Training Accuracy = 79.35%\n",
      "Epoch 597/1000: Training Accuracy = 74.16%\n",
      "Epoch 598/1000: Training Accuracy = 73.46%\n",
      "Epoch 599/1000: Training Accuracy = 76.97%\n",
      "Epoch 600/1000: Training Accuracy = 76.97%\n",
      "Epoch 601/1000: Training Accuracy = 77.95%\n",
      "Epoch 602/1000: Training Accuracy = 78.37%\n",
      "Epoch 603/1000: Training Accuracy = 79.21%\n",
      "Epoch 604/1000: Training Accuracy = 79.78%\n",
      "Epoch 605/1000: Training Accuracy = 79.49%\n",
      "Epoch 606/1000: Training Accuracy = 77.67%\n",
      "Epoch 607/1000: Training Accuracy = 79.35%\n",
      "Epoch 608/1000: Training Accuracy = 72.61%\n",
      "Epoch 609/1000: Training Accuracy = 79.92%\n",
      "Epoch 610/1000: Training Accuracy = 78.51%\n",
      "Epoch 611/1000: Training Accuracy = 78.09%\n",
      "Epoch 612/1000: Training Accuracy = 75.28%\n",
      "Epoch 613/1000: Training Accuracy = 72.75%\n",
      "Epoch 614/1000: Training Accuracy = 79.35%\n",
      "Epoch 615/1000: Training Accuracy = 80.34%\n",
      "Epoch 616/1000: Training Accuracy = 72.75%\n",
      "Epoch 617/1000: Training Accuracy = 80.48%\n",
      "Epoch 618/1000: Training Accuracy = 73.60%\n",
      "Epoch 619/1000: Training Accuracy = 79.92%\n",
      "Epoch 620/1000: Training Accuracy = 79.35%\n",
      "Epoch 621/1000: Training Accuracy = 78.23%\n",
      "Epoch 622/1000: Training Accuracy = 78.93%\n",
      "Epoch 623/1000: Training Accuracy = 73.17%\n",
      "Epoch 624/1000: Training Accuracy = 75.98%\n",
      "Epoch 625/1000: Training Accuracy = 80.20%\n",
      "Epoch 626/1000: Training Accuracy = 80.48%\n",
      "Epoch 627/1000: Training Accuracy = 77.67%\n",
      "Epoch 628/1000: Training Accuracy = 73.46%\n",
      "Epoch 629/1000: Training Accuracy = 76.54%\n",
      "Epoch 630/1000: Training Accuracy = 78.93%\n",
      "Epoch 631/1000: Training Accuracy = 79.21%\n",
      "Epoch 632/1000: Training Accuracy = 72.33%\n",
      "Epoch 633/1000: Training Accuracy = 79.78%\n",
      "Epoch 634/1000: Training Accuracy = 79.07%\n",
      "Epoch 635/1000: Training Accuracy = 78.09%\n",
      "Epoch 636/1000: Training Accuracy = 71.49%\n",
      "Epoch 637/1000: Training Accuracy = 75.00%\n",
      "Epoch 638/1000: Training Accuracy = 79.07%\n",
      "Epoch 639/1000: Training Accuracy = 76.83%\n",
      "Epoch 640/1000: Training Accuracy = 78.93%\n",
      "Epoch 641/1000: Training Accuracy = 74.72%\n",
      "Epoch 642/1000: Training Accuracy = 76.54%\n",
      "Epoch 643/1000: Training Accuracy = 74.44%\n",
      "Epoch 644/1000: Training Accuracy = 71.63%\n",
      "Epoch 645/1000: Training Accuracy = 77.81%\n",
      "Epoch 646/1000: Training Accuracy = 79.49%\n",
      "Epoch 647/1000: Training Accuracy = 74.72%\n",
      "Epoch 648/1000: Training Accuracy = 79.21%\n",
      "Epoch 649/1000: Training Accuracy = 79.35%\n",
      "Epoch 650/1000: Training Accuracy = 80.62%\n",
      "Epoch 651/1000: Training Accuracy = 76.69%\n",
      "Epoch 652/1000: Training Accuracy = 78.51%\n",
      "Epoch 653/1000: Training Accuracy = 77.81%\n",
      "Epoch 654/1000: Training Accuracy = 78.23%\n",
      "Epoch 655/1000: Training Accuracy = 75.00%\n",
      "Epoch 656/1000: Training Accuracy = 76.26%\n",
      "Epoch 657/1000: Training Accuracy = 79.07%\n",
      "Epoch 658/1000: Training Accuracy = 76.12%\n",
      "Epoch 659/1000: Training Accuracy = 76.26%\n",
      "Epoch 660/1000: Training Accuracy = 76.26%\n",
      "Epoch 661/1000: Training Accuracy = 80.34%\n",
      "Epoch 662/1000: Training Accuracy = 78.51%\n",
      "Epoch 663/1000: Training Accuracy = 76.40%\n",
      "Epoch 664/1000: Training Accuracy = 72.47%\n",
      "Epoch 665/1000: Training Accuracy = 79.21%\n",
      "Epoch 666/1000: Training Accuracy = 77.39%\n",
      "Epoch 667/1000: Training Accuracy = 75.14%\n",
      "Epoch 668/1000: Training Accuracy = 78.51%\n",
      "Epoch 669/1000: Training Accuracy = 79.63%\n",
      "Epoch 670/1000: Training Accuracy = 75.00%\n",
      "Epoch 671/1000: Training Accuracy = 76.26%\n",
      "Epoch 672/1000: Training Accuracy = 78.79%\n",
      "Epoch 673/1000: Training Accuracy = 79.49%\n",
      "Epoch 674/1000: Training Accuracy = 73.31%\n",
      "Epoch 675/1000: Training Accuracy = 79.21%\n",
      "Epoch 676/1000: Training Accuracy = 76.12%\n",
      "Epoch 677/1000: Training Accuracy = 74.30%\n",
      "Epoch 678/1000: Training Accuracy = 74.44%\n",
      "Epoch 679/1000: Training Accuracy = 79.78%\n",
      "Epoch 680/1000: Training Accuracy = 76.26%\n",
      "Epoch 681/1000: Training Accuracy = 74.72%\n",
      "Epoch 682/1000: Training Accuracy = 73.60%\n",
      "Epoch 683/1000: Training Accuracy = 76.69%\n",
      "Epoch 684/1000: Training Accuracy = 73.60%\n",
      "Epoch 685/1000: Training Accuracy = 74.58%\n",
      "Epoch 686/1000: Training Accuracy = 79.78%\n",
      "Epoch 687/1000: Training Accuracy = 78.37%\n",
      "Epoch 688/1000: Training Accuracy = 79.07%\n",
      "Epoch 689/1000: Training Accuracy = 75.42%\n",
      "Epoch 690/1000: Training Accuracy = 79.21%\n",
      "Epoch 691/1000: Training Accuracy = 75.84%\n",
      "Epoch 692/1000: Training Accuracy = 79.78%\n",
      "Epoch 693/1000: Training Accuracy = 80.48%\n",
      "Epoch 694/1000: Training Accuracy = 77.67%\n",
      "Epoch 695/1000: Training Accuracy = 79.49%\n",
      "Epoch 696/1000: Training Accuracy = 79.07%\n",
      "Epoch 697/1000: Training Accuracy = 73.46%\n",
      "Epoch 698/1000: Training Accuracy = 78.93%\n",
      "Epoch 699/1000: Training Accuracy = 80.48%\n",
      "Epoch 700/1000: Training Accuracy = 79.35%\n",
      "Epoch 701/1000: Training Accuracy = 78.79%\n",
      "Epoch 702/1000: Training Accuracy = 75.56%\n",
      "Epoch 703/1000: Training Accuracy = 80.34%\n",
      "Epoch 704/1000: Training Accuracy = 79.21%\n",
      "Epoch 705/1000: Training Accuracy = 75.28%\n",
      "Epoch 706/1000: Training Accuracy = 75.98%\n",
      "Epoch 707/1000: Training Accuracy = 72.61%\n",
      "Epoch 708/1000: Training Accuracy = 79.07%\n",
      "Epoch 709/1000: Training Accuracy = 73.60%\n",
      "Epoch 710/1000: Training Accuracy = 79.92%\n",
      "Epoch 711/1000: Training Accuracy = 79.78%\n",
      "Epoch 712/1000: Training Accuracy = 78.23%\n",
      "Epoch 713/1000: Training Accuracy = 80.06%\n",
      "Epoch 714/1000: Training Accuracy = 72.47%\n",
      "Epoch 715/1000: Training Accuracy = 77.39%\n",
      "Epoch 716/1000: Training Accuracy = 74.72%\n",
      "Epoch 717/1000: Training Accuracy = 74.86%\n",
      "Epoch 718/1000: Training Accuracy = 74.16%\n",
      "Epoch 719/1000: Training Accuracy = 74.72%\n",
      "Epoch 720/1000: Training Accuracy = 79.21%\n",
      "Epoch 721/1000: Training Accuracy = 78.51%\n",
      "Epoch 722/1000: Training Accuracy = 76.26%\n",
      "Epoch 723/1000: Training Accuracy = 80.06%\n",
      "Epoch 724/1000: Training Accuracy = 76.12%\n",
      "Epoch 725/1000: Training Accuracy = 79.49%\n",
      "Epoch 726/1000: Training Accuracy = 74.02%\n",
      "Epoch 727/1000: Training Accuracy = 78.93%\n",
      "Epoch 728/1000: Training Accuracy = 75.00%\n",
      "Epoch 729/1000: Training Accuracy = 78.79%\n",
      "Epoch 730/1000: Training Accuracy = 80.20%\n",
      "Epoch 731/1000: Training Accuracy = 75.14%\n",
      "Epoch 732/1000: Training Accuracy = 80.06%\n",
      "Epoch 733/1000: Training Accuracy = 73.60%\n",
      "Epoch 734/1000: Training Accuracy = 74.02%\n",
      "Epoch 735/1000: Training Accuracy = 79.49%\n",
      "Epoch 736/1000: Training Accuracy = 79.35%\n",
      "Epoch 737/1000: Training Accuracy = 75.70%\n",
      "Epoch 738/1000: Training Accuracy = 79.21%\n",
      "Epoch 739/1000: Training Accuracy = 76.40%\n",
      "Epoch 740/1000: Training Accuracy = 79.78%\n",
      "Epoch 741/1000: Training Accuracy = 72.75%\n",
      "Epoch 742/1000: Training Accuracy = 75.42%\n",
      "Epoch 743/1000: Training Accuracy = 72.61%\n",
      "Epoch 744/1000: Training Accuracy = 79.07%\n",
      "Epoch 745/1000: Training Accuracy = 76.97%\n",
      "Epoch 746/1000: Training Accuracy = 79.92%\n",
      "Epoch 747/1000: Training Accuracy = 78.09%\n",
      "Epoch 748/1000: Training Accuracy = 79.63%\n",
      "Epoch 749/1000: Training Accuracy = 80.34%\n",
      "Epoch 750/1000: Training Accuracy = 78.23%\n",
      "Epoch 751/1000: Training Accuracy = 78.23%\n",
      "Epoch 752/1000: Training Accuracy = 76.40%\n",
      "Epoch 753/1000: Training Accuracy = 78.51%\n",
      "Epoch 754/1000: Training Accuracy = 76.40%\n",
      "Epoch 755/1000: Training Accuracy = 79.07%\n",
      "Epoch 756/1000: Training Accuracy = 79.21%\n",
      "Epoch 757/1000: Training Accuracy = 79.78%\n",
      "Epoch 758/1000: Training Accuracy = 78.51%\n",
      "Epoch 759/1000: Training Accuracy = 79.21%\n",
      "Epoch 760/1000: Training Accuracy = 74.02%\n",
      "Epoch 761/1000: Training Accuracy = 80.62%\n",
      "Epoch 762/1000: Training Accuracy = 78.93%\n",
      "Epoch 763/1000: Training Accuracy = 76.26%\n",
      "Epoch 764/1000: Training Accuracy = 79.49%\n",
      "Epoch 765/1000: Training Accuracy = 79.78%\n",
      "Epoch 766/1000: Training Accuracy = 71.63%\n",
      "Epoch 767/1000: Training Accuracy = 81.04%\n",
      "Epoch 768/1000: Training Accuracy = 77.67%\n",
      "Epoch 769/1000: Training Accuracy = 78.23%\n",
      "Epoch 770/1000: Training Accuracy = 79.49%\n",
      "Epoch 771/1000: Training Accuracy = 76.54%\n",
      "Epoch 772/1000: Training Accuracy = 79.49%\n",
      "Epoch 773/1000: Training Accuracy = 80.76%\n",
      "Epoch 774/1000: Training Accuracy = 76.97%\n",
      "Epoch 775/1000: Training Accuracy = 74.16%\n",
      "Epoch 776/1000: Training Accuracy = 76.54%\n",
      "Epoch 777/1000: Training Accuracy = 79.49%\n",
      "Epoch 778/1000: Training Accuracy = 79.07%\n",
      "Epoch 779/1000: Training Accuracy = 78.65%\n",
      "Epoch 780/1000: Training Accuracy = 73.60%\n",
      "Epoch 781/1000: Training Accuracy = 78.51%\n",
      "Epoch 782/1000: Training Accuracy = 76.12%\n",
      "Epoch 783/1000: Training Accuracy = 79.07%\n",
      "Epoch 784/1000: Training Accuracy = 73.46%\n",
      "Epoch 785/1000: Training Accuracy = 78.37%\n",
      "Epoch 786/1000: Training Accuracy = 79.78%\n",
      "Epoch 787/1000: Training Accuracy = 76.97%\n",
      "Epoch 788/1000: Training Accuracy = 80.34%\n",
      "Epoch 789/1000: Training Accuracy = 79.21%\n",
      "Epoch 790/1000: Training Accuracy = 79.92%\n",
      "Epoch 791/1000: Training Accuracy = 79.49%\n",
      "Epoch 792/1000: Training Accuracy = 78.93%\n",
      "Epoch 793/1000: Training Accuracy = 75.14%\n",
      "Epoch 794/1000: Training Accuracy = 76.69%\n",
      "Epoch 795/1000: Training Accuracy = 78.65%\n",
      "Epoch 796/1000: Training Accuracy = 74.86%\n",
      "Epoch 797/1000: Training Accuracy = 79.07%\n",
      "Epoch 798/1000: Training Accuracy = 74.58%\n",
      "Epoch 799/1000: Training Accuracy = 79.92%\n",
      "Epoch 800/1000: Training Accuracy = 79.63%\n",
      "Epoch 801/1000: Training Accuracy = 75.14%\n",
      "Epoch 802/1000: Training Accuracy = 79.35%\n",
      "Epoch 803/1000: Training Accuracy = 75.00%\n",
      "Epoch 804/1000: Training Accuracy = 79.07%\n",
      "Epoch 805/1000: Training Accuracy = 81.18%\n",
      "Epoch 806/1000: Training Accuracy = 74.58%\n",
      "Epoch 807/1000: Training Accuracy = 78.79%\n",
      "Epoch 808/1000: Training Accuracy = 78.65%\n",
      "Epoch 809/1000: Training Accuracy = 79.78%\n",
      "Epoch 810/1000: Training Accuracy = 79.78%\n",
      "Epoch 811/1000: Training Accuracy = 78.23%\n",
      "Epoch 812/1000: Training Accuracy = 78.23%\n",
      "Epoch 813/1000: Training Accuracy = 77.95%\n",
      "Epoch 814/1000: Training Accuracy = 71.91%\n",
      "Epoch 815/1000: Training Accuracy = 78.93%\n",
      "Epoch 816/1000: Training Accuracy = 73.74%\n",
      "Epoch 817/1000: Training Accuracy = 79.78%\n",
      "Epoch 818/1000: Training Accuracy = 80.20%\n",
      "Epoch 819/1000: Training Accuracy = 78.93%\n",
      "Epoch 820/1000: Training Accuracy = 78.93%\n",
      "Epoch 821/1000: Training Accuracy = 78.93%\n",
      "Epoch 822/1000: Training Accuracy = 79.21%\n",
      "Epoch 823/1000: Training Accuracy = 79.63%\n",
      "Epoch 824/1000: Training Accuracy = 76.69%\n",
      "Epoch 825/1000: Training Accuracy = 78.65%\n",
      "Epoch 826/1000: Training Accuracy = 77.81%\n",
      "Epoch 827/1000: Training Accuracy = 77.67%\n",
      "Epoch 828/1000: Training Accuracy = 74.30%\n",
      "Epoch 829/1000: Training Accuracy = 73.88%\n",
      "Epoch 830/1000: Training Accuracy = 73.60%\n",
      "Epoch 831/1000: Training Accuracy = 78.93%\n",
      "Epoch 832/1000: Training Accuracy = 78.93%\n",
      "Epoch 833/1000: Training Accuracy = 79.92%\n",
      "Epoch 834/1000: Training Accuracy = 78.37%\n",
      "Epoch 835/1000: Training Accuracy = 75.00%\n",
      "Epoch 836/1000: Training Accuracy = 79.35%\n",
      "Epoch 837/1000: Training Accuracy = 79.92%\n",
      "Epoch 838/1000: Training Accuracy = 80.34%\n",
      "Epoch 839/1000: Training Accuracy = 78.51%\n",
      "Epoch 840/1000: Training Accuracy = 80.76%\n",
      "Epoch 841/1000: Training Accuracy = 76.40%\n",
      "Epoch 842/1000: Training Accuracy = 79.35%\n",
      "Epoch 843/1000: Training Accuracy = 80.06%\n",
      "Epoch 844/1000: Training Accuracy = 72.19%\n",
      "Epoch 845/1000: Training Accuracy = 78.79%\n",
      "Epoch 846/1000: Training Accuracy = 79.07%\n",
      "Epoch 847/1000: Training Accuracy = 81.32%\n",
      "Epoch 848/1000: Training Accuracy = 79.07%\n",
      "Epoch 849/1000: Training Accuracy = 78.79%\n",
      "Epoch 850/1000: Training Accuracy = 76.40%\n",
      "Epoch 851/1000: Training Accuracy = 79.49%\n",
      "Epoch 852/1000: Training Accuracy = 80.06%\n",
      "Epoch 853/1000: Training Accuracy = 74.58%\n",
      "Epoch 854/1000: Training Accuracy = 80.06%\n",
      "Epoch 855/1000: Training Accuracy = 76.83%\n",
      "Epoch 856/1000: Training Accuracy = 72.75%\n",
      "Epoch 857/1000: Training Accuracy = 78.65%\n",
      "Epoch 858/1000: Training Accuracy = 76.12%\n",
      "Epoch 859/1000: Training Accuracy = 79.92%\n",
      "Epoch 860/1000: Training Accuracy = 79.21%\n",
      "Epoch 861/1000: Training Accuracy = 77.81%\n",
      "Epoch 862/1000: Training Accuracy = 78.37%\n",
      "Epoch 863/1000: Training Accuracy = 76.54%\n",
      "Epoch 864/1000: Training Accuracy = 78.37%\n",
      "Epoch 865/1000: Training Accuracy = 80.34%\n",
      "Epoch 866/1000: Training Accuracy = 79.21%\n",
      "Epoch 867/1000: Training Accuracy = 78.23%\n",
      "Epoch 868/1000: Training Accuracy = 79.21%\n",
      "Epoch 869/1000: Training Accuracy = 80.90%\n",
      "Epoch 870/1000: Training Accuracy = 76.12%\n",
      "Epoch 871/1000: Training Accuracy = 79.21%\n",
      "Epoch 872/1000: Training Accuracy = 79.49%\n",
      "Epoch 873/1000: Training Accuracy = 76.12%\n",
      "Epoch 874/1000: Training Accuracy = 78.09%\n",
      "Epoch 875/1000: Training Accuracy = 73.60%\n",
      "Epoch 876/1000: Training Accuracy = 77.81%\n",
      "Epoch 877/1000: Training Accuracy = 73.74%\n",
      "Epoch 878/1000: Training Accuracy = 79.63%\n",
      "Epoch 879/1000: Training Accuracy = 78.37%\n",
      "Epoch 880/1000: Training Accuracy = 73.46%\n",
      "Epoch 881/1000: Training Accuracy = 75.98%\n",
      "Epoch 882/1000: Training Accuracy = 79.21%\n",
      "Epoch 883/1000: Training Accuracy = 79.92%\n",
      "Epoch 884/1000: Training Accuracy = 78.79%\n",
      "Epoch 885/1000: Training Accuracy = 79.78%\n",
      "Epoch 886/1000: Training Accuracy = 71.91%\n",
      "Epoch 887/1000: Training Accuracy = 72.47%\n",
      "Epoch 888/1000: Training Accuracy = 72.47%\n",
      "Epoch 889/1000: Training Accuracy = 78.51%\n",
      "Epoch 890/1000: Training Accuracy = 77.95%\n",
      "Epoch 891/1000: Training Accuracy = 80.20%\n",
      "Epoch 892/1000: Training Accuracy = 73.17%\n",
      "Epoch 893/1000: Training Accuracy = 79.78%\n",
      "Epoch 894/1000: Training Accuracy = 75.84%\n",
      "Epoch 895/1000: Training Accuracy = 72.75%\n",
      "Epoch 896/1000: Training Accuracy = 72.61%\n",
      "Epoch 897/1000: Training Accuracy = 80.06%\n",
      "Epoch 898/1000: Training Accuracy = 76.40%\n",
      "Epoch 899/1000: Training Accuracy = 79.21%\n",
      "Epoch 900/1000: Training Accuracy = 79.35%\n",
      "Epoch 901/1000: Training Accuracy = 79.07%\n",
      "Epoch 902/1000: Training Accuracy = 79.21%\n",
      "Epoch 903/1000: Training Accuracy = 79.21%\n",
      "Epoch 904/1000: Training Accuracy = 78.65%\n",
      "Epoch 905/1000: Training Accuracy = 80.34%\n",
      "Epoch 906/1000: Training Accuracy = 76.26%\n",
      "Epoch 907/1000: Training Accuracy = 74.30%\n",
      "Epoch 908/1000: Training Accuracy = 72.47%\n",
      "Epoch 909/1000: Training Accuracy = 72.61%\n",
      "Epoch 910/1000: Training Accuracy = 79.63%\n",
      "Epoch 911/1000: Training Accuracy = 75.00%\n",
      "Epoch 912/1000: Training Accuracy = 78.51%\n",
      "Epoch 913/1000: Training Accuracy = 78.37%\n",
      "Epoch 914/1000: Training Accuracy = 78.79%\n",
      "Epoch 915/1000: Training Accuracy = 80.90%\n",
      "Epoch 916/1000: Training Accuracy = 75.42%\n",
      "Epoch 917/1000: Training Accuracy = 78.65%\n",
      "Epoch 918/1000: Training Accuracy = 77.39%\n",
      "Epoch 919/1000: Training Accuracy = 79.21%\n",
      "Epoch 920/1000: Training Accuracy = 78.93%\n",
      "Epoch 921/1000: Training Accuracy = 72.61%\n",
      "Epoch 922/1000: Training Accuracy = 79.07%\n",
      "Epoch 923/1000: Training Accuracy = 79.07%\n",
      "Epoch 924/1000: Training Accuracy = 75.70%\n",
      "Epoch 925/1000: Training Accuracy = 79.07%\n",
      "Epoch 926/1000: Training Accuracy = 76.12%\n",
      "Epoch 927/1000: Training Accuracy = 79.07%\n",
      "Epoch 928/1000: Training Accuracy = 79.49%\n",
      "Epoch 929/1000: Training Accuracy = 78.23%\n",
      "Epoch 930/1000: Training Accuracy = 79.78%\n",
      "Epoch 931/1000: Training Accuracy = 79.21%\n",
      "Epoch 932/1000: Training Accuracy = 78.93%\n",
      "Epoch 933/1000: Training Accuracy = 80.34%\n",
      "Epoch 934/1000: Training Accuracy = 75.56%\n",
      "Epoch 935/1000: Training Accuracy = 80.06%\n",
      "Epoch 936/1000: Training Accuracy = 79.92%\n",
      "Epoch 937/1000: Training Accuracy = 77.39%\n",
      "Epoch 938/1000: Training Accuracy = 72.89%\n",
      "Epoch 939/1000: Training Accuracy = 75.00%\n",
      "Epoch 940/1000: Training Accuracy = 78.65%\n",
      "Epoch 941/1000: Training Accuracy = 77.81%\n",
      "Epoch 942/1000: Training Accuracy = 79.07%\n",
      "Epoch 943/1000: Training Accuracy = 76.12%\n",
      "Epoch 944/1000: Training Accuracy = 76.40%\n",
      "Epoch 945/1000: Training Accuracy = 80.34%\n",
      "Epoch 946/1000: Training Accuracy = 79.07%\n",
      "Epoch 947/1000: Training Accuracy = 72.61%\n",
      "Epoch 948/1000: Training Accuracy = 75.14%\n",
      "Epoch 949/1000: Training Accuracy = 73.03%\n",
      "Epoch 950/1000: Training Accuracy = 73.17%\n",
      "Epoch 951/1000: Training Accuracy = 78.93%\n",
      "Epoch 952/1000: Training Accuracy = 77.39%\n",
      "Epoch 953/1000: Training Accuracy = 80.34%\n",
      "Epoch 954/1000: Training Accuracy = 79.49%\n",
      "Epoch 955/1000: Training Accuracy = 77.25%\n",
      "Epoch 956/1000: Training Accuracy = 75.84%\n",
      "Epoch 957/1000: Training Accuracy = 79.07%\n",
      "Epoch 958/1000: Training Accuracy = 75.42%\n",
      "Epoch 959/1000: Training Accuracy = 71.63%\n",
      "Epoch 960/1000: Training Accuracy = 78.65%\n",
      "Epoch 961/1000: Training Accuracy = 79.07%\n",
      "Epoch 962/1000: Training Accuracy = 77.95%\n",
      "Epoch 963/1000: Training Accuracy = 75.98%\n",
      "Epoch 964/1000: Training Accuracy = 77.53%\n",
      "Epoch 965/1000: Training Accuracy = 79.49%\n",
      "Epoch 966/1000: Training Accuracy = 79.63%\n",
      "Epoch 967/1000: Training Accuracy = 76.26%\n",
      "Epoch 968/1000: Training Accuracy = 79.07%\n",
      "Epoch 969/1000: Training Accuracy = 79.49%\n",
      "Epoch 970/1000: Training Accuracy = 78.79%\n",
      "Epoch 971/1000: Training Accuracy = 77.95%\n",
      "Epoch 972/1000: Training Accuracy = 75.00%\n",
      "Epoch 973/1000: Training Accuracy = 75.14%\n",
      "Epoch 974/1000: Training Accuracy = 75.56%\n",
      "Epoch 975/1000: Training Accuracy = 72.61%\n",
      "Epoch 976/1000: Training Accuracy = 79.21%\n",
      "Epoch 977/1000: Training Accuracy = 73.46%\n",
      "Epoch 978/1000: Training Accuracy = 79.35%\n",
      "Epoch 979/1000: Training Accuracy = 79.21%\n",
      "Epoch 980/1000: Training Accuracy = 76.97%\n",
      "Epoch 981/1000: Training Accuracy = 79.63%\n",
      "Epoch 982/1000: Training Accuracy = 73.74%\n",
      "Epoch 983/1000: Training Accuracy = 73.03%\n",
      "Epoch 984/1000: Training Accuracy = 78.93%\n",
      "Epoch 985/1000: Training Accuracy = 80.48%\n",
      "Epoch 986/1000: Training Accuracy = 75.00%\n",
      "Epoch 987/1000: Training Accuracy = 72.61%\n",
      "Epoch 988/1000: Training Accuracy = 79.63%\n",
      "Epoch 989/1000: Training Accuracy = 76.54%\n",
      "Epoch 990/1000: Training Accuracy = 74.86%\n",
      "Epoch 991/1000: Training Accuracy = 75.28%\n",
      "Epoch 992/1000: Training Accuracy = 74.72%\n",
      "Epoch 993/1000: Training Accuracy = 75.98%\n",
      "Epoch 994/1000: Training Accuracy = 79.21%\n",
      "Epoch 995/1000: Training Accuracy = 79.21%\n",
      "Epoch 996/1000: Training Accuracy = 79.07%\n",
      "Epoch 997/1000: Training Accuracy = 79.07%\n",
      "Epoch 998/1000: Training Accuracy = 78.79%\n",
      "Epoch 999/1000: Training Accuracy = 76.69%\n",
      "Epoch 1000/1000: Training Accuracy = 79.21%\n"
     ]
    }
   ],
   "source": [
    "input_size = x_train.shape[1]\n",
    "perceptron = SingleLayerPerceptron(input_size, learning_rate=lr, epochs=epochs)\n",
    "perceptron.train(x_train_normalize, y_train_normalize)\n",
    "pred = perceptron.predict(x_test_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model2_cf = confusion_matrix(y_test_normalize, pred)\n",
    "plot_cm(model2_cf, 'Model 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Not Survived</th>\n",
       "      <td>0.952607</td>\n",
       "      <td>0.985294</td>\n",
       "      <td>0.968675</td>\n",
       "      <td>204.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.921260</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>127.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.960725</td>\n",
       "      <td>0.960725</td>\n",
       "      <td>0.960725</td>\n",
       "      <td>0.960725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.963803</td>\n",
       "      <td>0.953277</td>\n",
       "      <td>0.958022</td>\n",
       "      <td>331.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.961199</td>\n",
       "      <td>0.960725</td>\n",
       "      <td>0.960500</td>\n",
       "      <td>331.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "Not Survived   0.952607  0.985294  0.968675  204.000000\n",
       "Survived       0.975000  0.921260  0.947368  127.000000\n",
       "accuracy       0.960725  0.960725  0.960725    0.960725\n",
       "macro avg      0.963803  0.953277  0.958022  331.000000\n",
       "weighted avg   0.961199  0.960725  0.960500  331.000000"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2_report = classification_report(y_test_normalize, pred, output_dict=True, target_names=['Not Survived',\"Survived\"])\n",
    "pd.DataFrame(model2_report).transpose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
